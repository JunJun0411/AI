{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 오버플로우 방지\n",
    "        eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "        x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # dx = dout * y * (1-y)\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # dx = dout * (1 - y제곱)\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        # dw, db 결과를 저장한다.\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "            if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                # v를 곱해줌으로써 자연스럽게 하강하도록 한다.\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "            \n",
    "class CustomOptimizer:\n",
    "    \"\"\" Adam : Momentum 과 AdaGrad 를 융합한 방법 \n",
    "    v, h 가 각각 최초 0으로 설정되어 학습 초반에 0으로 biased 되는 문제를 해결하기 위해 고안한 방법 \n",
    "    \"\"\"\n",
    "    def __init__(self, lr = 0.0001):\n",
    "        self.lr = lr                # learningRate\n",
    "        self.B1 = 0.9               # 베타1 0~1사이 값\n",
    "        self.B2 = 0.999             # 베타2 0~1사이 값\n",
    "        self.t = 0                  # Initialize timestep\n",
    "        self.epsilon = 1e-7         # 1e-8 or 1e-7 무관\n",
    "        self.m, self.v = None, None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        # None 인경우 m, v 초기화\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                    \n",
    "                #  Initialize 1st moment vector\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                #  Initialize 2nd moment vector\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.t += 1 # t = t + 1\n",
    "        # 연산속도 높이기 위해 key와 관련없는 값 반복문에서 빼서 미리 계산, epsilon은 작은 값이라 영향X\n",
    "        lr1 = self.lr * np.sqrt(1.0 - self.B2**self.t) / (1.0 - self.B1**self.t)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            # dropOut를 위해 저장된 파라미터는 무관하므로 Pass\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "                \n",
    "            # Update biased first moment estimate\n",
    "            self.m[key] = (self.B1 * self.m[key]) + ((1 - self.B1) * grads[key])\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[key] = (self.B2 * self.v[key]) + (1 - self.B2) * grads[key]**2\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            mt = self.m[key] # / (1.0 - self.B1**self.t) 미리 계산\n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            vt = self.v[key] # / (1.0 - self.B2**self.t) 미리 계산\n",
    "            # Update parameters\n",
    "            params[key] -= lr1 * mt / (np.sqrt(vt) + self.epsilon)\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        # 0.9만 남기고 10%의 노드를 끈다.\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        # flg를 default로 False로 두어 train할때만 Dropout이 실행됨\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(val, type = 'Sigmoid'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / val)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(val)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 64, 64, 64, 64, 6], lr=0.0003):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag #\n",
    "        self.params = {}\n",
    "        self.params['mean'] = mean\n",
    "        self.params['std'] = std\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.W = {} #\n",
    "        self.layer_unit = layer_unit\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = CustomOptimizer(lr)\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            self.layers['Sigmoid{}'.format(i)] = Sigmoid() #Activation Function\n",
    "            \n",
    "            # dropOut\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(1, self.layer_size):\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'Sigmoid') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 확인\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag\n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        x2 = x.copy()\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key:\n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "        self.dropFlag=False\n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        self.__init_layer()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
