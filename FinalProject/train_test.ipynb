{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-ae7e41cf817a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m                     \u001b[1;31m# 트레이너를 사용해 모델 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m                     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m                     \u001b[1;31m# 파라미터 보관\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-ae7e41cf817a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minference_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-ae7e41cf817a>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m             \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_acc_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-ae7e41cf817a>\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0macc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[0minference_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import numpy as np\n",
    "from AReM import *\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class CustomActivation:\n",
    "    \"\"\"sigmoid\"\"\"\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "        xSafe = np.array(np.maximum(x, eMIN))\n",
    "        self.out = (1.0 / 1 + np.exp(-xSafe))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "\n",
    "class CustomOptimizer:\n",
    "    pass\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "def weight_init_std(input, type = 0):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    if type:\n",
    "        return 1.0 / np.sqrt(input)\n",
    "    # he 초깃값 -> Relu\n",
    "    return np.sqrt(2.0 / input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, layer_unit = [6, 10, 10, 10, 10, 6], lr=0.01, dr = False):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        self.dr = dr\n",
    "        self.params = {}\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layer_size = len(layer_unit) # 히든 레이어 수\n",
    "        self.__init_weight(layer_unit)\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = SGD(lr)\n",
    "\n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(self.layer_size - 2):\n",
    "            self.layers['Affine{}'.format(i+1)] = \\\n",
    "                Affine(self.params['W{}'.format(i+1)], self.params['b{}'.format(i+1)])\n",
    "            if self.dr:\n",
    "                self.layers['Dropout{}'.format(i+1)] = Dropout()\n",
    "            self.layers['Relu{}'.format(i+1)] = Relu()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        if self.dr:\n",
    "            self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self, unit):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(self.layer_size - 1):\n",
    "            self.params['W{}'.format(i + 1)] = np.random.randn(unit[i], unit[i + 1]) * weight_init_std(unit[i])\n",
    "            self.params['b{}'.format(i + 1)] = np.zeros(unit[i + 1])\n",
    "        \n",
    "    \n",
    "    def update(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(self.layer_size - 1):\n",
    "            grads['W{}'.format(i+1)] = self.layers['Affine{}'.format(i+1)].dw\n",
    "            grads['b{}'.format(i+1)] = self.layers['Affine{}'.format(i+1)].db\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        pass\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = [6, 12, 12, 12, 12 ,6]):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 12, 12, 12, 12, 6]\n",
    "        self.layer_size = len(layers)\n",
    "\n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "\n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "#             if self.verbose: print(\n",
    "#                 \"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "#                 \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            \"\"\"제출 전 수정\"\"\"\n",
    "            file = open('./Result/SGD/[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].txt'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers), 'w')\n",
    "            file.write(\"=============== Final Test Accuracy ===============\\n\")\n",
    "            file.write(\"test acc: %f,  \" % test_acc)\n",
    "            file.write(\"inference_time: %f\\n\" % inference_time)\n",
    "            file.close()\n",
    "#             print(\"=============== Final Test Accuracy ===============\")\n",
    "#             print(\"test acc:\" + str(test_acc) + \", inference_time:\" + str(inference_time))\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class Tester:\n",
    "    \"\"\"\n",
    "    test 해주는 클래스. 수정불가\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_test, t_test, mini_batch_size=100, verbose=True):\n",
    "        self.network = network\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_test.shape[0]\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"\n",
    "        수정불가\n",
    "        \"\"\"\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time()-start_time)/x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(one_hot_label=False)\n",
    "    \n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "    \n",
    "    # hyperparameter\n",
    "    epochs = [50, 100]\n",
    "    batchs = [10, 20, 50, 100]\n",
    "    learningRate = [0.01, 0.001, 0.0001, 0.00001]\n",
    "    layer_unit_list = [[6, 12, 12, 6],\n",
    "                      [6, 12, 12, 12, 6],\n",
    "                      [6, 12, 12, 12, 12, 6],\n",
    "                      [6, 12, 12, 12, 12, 12, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in batchs:\n",
    "            for lr in learningRate:\n",
    "                for layer_unit in layer_unit_list:\n",
    "                    sf = './Params/SGD/params[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(layer_unit)\n",
    "\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=True, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "\n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf)\n",
    "                #     print(\"Network params Saved \")\n",
    "\n",
    "                    (_, _), (x_test, t_test) = load_AReM(one_hot_label=False)\n",
    "\n",
    "                    network = Model()\n",
    "\n",
    "                    tester = Tester(network, x_test, t_test)\n",
    "                #     args = arg()\n",
    "\n",
    "                    network.load_params(args.sf)\n",
    "\n",
    "\n",
    "                    # 배치사이즈100으로 accuracy test, 다른 배치사이즈로 학습했다면 결과가 달라질 수 있습니다.\n",
    "                    test_acc, inference_time = tester.accuracy(x_test, t_test)\n",
    "                    file = open('./Result/SGD/[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].txt'\\\n",
    "                                        .format(len(layer_unit), epoch, batch, lr, layer_unit), 'a')\n",
    "                    file.write(\"=============== Final Test Accuracy ===============\\n\")\n",
    "                    file.write(\"test acc: %f,  \" % test_acc)\n",
    "                    file.write(\"inference_time: %f\\n\" % inference_time)\n",
    "                    file.close()\n",
    "                #     print(\"=============== Final Test Accuracy ===============\")\n",
    "                #     print(\"test acc:\" + str(test_acc) + \", inference_time:\" + str(inference_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
