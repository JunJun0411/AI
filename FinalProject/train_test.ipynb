{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "        xSafe = np.array(np.maximum(x, eMIN))\n",
    "        self.out = (1.0 / 1 + np.exp(-xSafe))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "class RMSprop:\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n",
    "            \n",
    "class CustomOptimizer:\n",
    "    pass\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "class BatchNormalization:\n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n",
    "\n",
    "        # 시험할 때 사용할 평균과 분산\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        # backward 시에 사용할 중간 데이터\n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "\n",
    "        out = self.__forward(x, train_flg)\n",
    "        \n",
    "        return out.reshape(*self.input_shape)\n",
    "            \n",
    "    def __forward(self, x, train_flg):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "                        \n",
    "        if train_flg:\n",
    "            mu = x.mean(axis=0)\n",
    "            xc = x - mu\n",
    "            var = np.mean(xc**2, axis=0)\n",
    "            std = np.sqrt(var + 10e-7)\n",
    "            xn = xc / std\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        dbeta = dout.sum(axis=0)\n",
    "        dgamma = np.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "        dmu = np.sum(dxc, axis=0)\n",
    "        dx = dxc - dmu / self.batch_size\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "def weight_init_std(input, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(4.0 / input)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    return 1.0 / np.sqrt(input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, layer_unit = [6, 50, 50, 6], lr=0.01, dr = False):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        self.dr = dr\n",
    "        self.params = {}\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.W = {}\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight(layer_unit)\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = Adam(lr)\n",
    "        \n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.normalize = True\n",
    "        self.standardze = True        \n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(self.layer_size - 2):\n",
    "            self.layers['Affine{}'.format(i+1)] = \\\n",
    "                Affine(self.params['W{}'.format(i+1)], self.params['b{}'.format(i+1)])\n",
    "            if self.dr:\n",
    "                self.layers['Dropout{}'.format(i+1)] = Dropout()\n",
    "            self.layers['Relu{}'.format(i+1)] = Relu()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self, unit):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(self.layer_size - 1):\n",
    "            self.params['W{}'.format(i + 1)] = weight_init_std(unit[i], 'Relu') * np.random.randn(unit[i], unit[i + 1]) \n",
    "            self.params['b{}'.format(i + 1)] = np.zeros(unit[i + 1])\n",
    "        for i in range(self.layer_size - 1):\n",
    "            self.W['W{}'.format(i + 1)] = self.params['W{}'.format(i + 1)].copy()\n",
    "            self.W['b{}'.format(i + 1)] = self.params['b{}'.format(i + 1)].copy()\n",
    "        \n",
    "    \n",
    "    def update(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.normalize:\n",
    "            x = x.astype(np.float32)\n",
    "            x = (x - np.min(x, axis=0)) / (np.max(x, axis=0) - np.min(x, axis=0))\n",
    "\n",
    "        if self.standardze:\n",
    "            x = x.astype(np.float32)\n",
    "            x -= np.mean(x, axis=0)\n",
    "            x /= np.std(x, axis=0)\n",
    "            \n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(self.layer_size - 1):\n",
    "            grads['W{}'.format(i+1)] = self.layers['Affine{}'.format(i+1)].dw\n",
    "            grads['b{}'.format(i+1)] = self.layers['Affine{}'.format(i+1)].db\n",
    "        \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        self.__init_layer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch: 1 , iteration: 0 , train acc:0.181 , test acc:0.186 , train loss:12.884 ===\n",
      "=== epoch: 2 , iteration: 250 , train acc:0.671 , test acc:0.662 , train loss:0.613 ===\n",
      "=== epoch: 3 , iteration: 500 , train acc:0.702 , test acc:0.692 , train loss:0.604 ===\n",
      "=== epoch: 4 , iteration: 750 , train acc:0.694 , test acc:0.681 , train loss:0.608 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-9118f45ce76d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                     \u001b[1;31m# 트레이너를 사용해 모델 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m                     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                     \u001b[1;31m# 파라미터 보관\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-9118f45ce76d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minference_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-9118f45ce76d>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# 네트워크 업데이트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-3b4c97161d64>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    387\u001b[0m         \"\"\"\n\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-3b4c97161d64>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mla\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m             \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;31m# 결과 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-3b4c97161d64>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dout)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = [6, 12, 12, 12, 12 ,6]):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 12, 12, 12, 12, 6]\n",
    "        self.layer_size = len(layers)\n",
    "        \n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "\n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose is False: print(\n",
    "                \"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            \"\"\"제출 전 수정\"\"\"\n",
    "            file = open('./Result/SGD/[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].txt'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers), 'w')\n",
    "            file.write(\"=============== Final Test Accuracy ===============\\n\")\n",
    "            file.write(\"test acc: %f,  \" % test_acc)\n",
    "            file.write(\"inference_time: %f\\n\" % inference_time)\n",
    "            file.close()\n",
    "        else:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class Tester:\n",
    "    \"\"\"\n",
    "    test 해주는 클래스. 수정불가\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_test, t_test, mini_batch_size=100, verbose=True):\n",
    "        self.network = network\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_test.shape[0]\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"\n",
    "        수정불가\n",
    "        \"\"\"\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time()-start_time)/x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(one_hot_label=False)\n",
    "    \n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "\n",
    "    testacc=[]\n",
    "    \n",
    "    # hyperparameter\n",
    "    epochs = [100]\n",
    "    batchs = [100]\n",
    "    learningRate = [0.005]\n",
    "    layer_unit_list = [[6, 512, 512, 512, 512, 512, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in batchs:\n",
    "            for lr in learningRate:\n",
    "                for layer_unit in layer_unit_list:\n",
    "                    sf = './Params/Adam/params[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(layer_unit)\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=False, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "                    \n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf) \n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "                    x = np.arange(len(trainer.train_loss_list))\n",
    "                    plt.plot(x, trainer.train_loss_list)\n",
    "                    plt.legend([\"loss\"]) # 각주\n",
    "                    plt.title('layerSize: {}, epoch : {}, batch : {}, lr: {} layer: {}'\\\n",
    "                              .format(len(layer_unit), epoch, batch, lr, layer_unit))\n",
    "                    plt.savefig('./Result/Adam/[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "                                .format(len(layer_unit), epoch, batch, lr, layer_unit), dpi=100)\n",
    "                    plt.show()\n",
    "                    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=1\n",
    "plt.figure(figsize = [10,10])\n",
    "plt.title(\"histogram\")\n",
    "\n",
    "for key,val in network.W.items():\n",
    "      if key[0] == 'W':\n",
    "            plt.subplot(math.ceil((network.layer_size+1)/2),2,i)\n",
    "            plt.title(key)\n",
    "            plt.hist(val,bins=10)\n",
    "            i+=1\n",
    "                #     print(\"Network params Saved \")\n",
    "\n",
    "#                     (_, _), (x_test, t_test) = load_AReM(one_hot_label=False)\n",
    "\n",
    "#                     network = Model(layer_unit)\n",
    "\n",
    "#                     tester = Tester(network, x_test, t_test)\n",
    "#                 #     args = arg()\n",
    "\n",
    "#                     network.load_params(args.sf)\n",
    "\n",
    "\n",
    "#                     # 배치사이즈100으로 accuracy test, 다른 배치사이즈로 학습했다면 결과가 달라질 수 있습니다.\n",
    "#                     test_acc, inference_time = tester.accuracy(x_test, t_test)\n",
    "# #                     file = open('./Result/SGD/[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].txt'\\\n",
    "# #                                         .format(len(layer_unit), epoch, batch, lr, layer_unit), 'a')\n",
    "# #                     file.write(\"=============== Final Test Accuracy ===============\\n\")\n",
    "# #                     file.write(\"test acc: %f,  \" % test_acc)\n",
    "# #                     file.write(\"inference_time: %f\\n\" % inference_time)\n",
    "# #                     file.close()\n",
    "#                     print(\"=============== Final Test Accuracy ===============\")\n",
    "#                     print(\"test acc:\" + str(round(test_acc,3)) + \", inference_time:\" + str(inference_time))\n",
    "#                     print(\"\\n=========================================================\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32.    5.24 21.    0.82 18.33  1.25]\n",
      " [33.    4.95 17.    3.08 14.75  5.85]\n",
      " [53.5   0.5  13.5   0.87 16.75  0.83]\n",
      " ...\n",
      " [26.5   0.87  4.67  2.05  5.5   1.12]\n",
      " [39.67  0.47  9.25  0.43 10.33  1.25]\n",
      " [26.    0.    0.    0.   18.5   0.5 ]]\n",
      "(25055, 6)\n",
      "[4 4 3 ... 3 3 5]\n",
      "(25055, 6)\n",
      "[[21.75  0.83 12.5   2.06 21.75  1.3 ]\n",
      " [37.25  1.3  22.    5.87 20.75  4.44]\n",
      " [41.75  2.59  9.5   5.22 16.5   2.96]\n",
      " ...\n",
      " [36.    0.    8.5   2.69 21.    0.  ]\n",
      " [42.    0.   18.25  0.43 19.25  0.83]\n",
      " [38.5   2.6  13.    2.83 15.    4.97]]\n",
      "(8351, 6)\n",
      "[5 4 0 ... 2 2 0]\n",
      "(8351,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# coding: utf-8\n",
    "try:\n",
    "    import urllib.request\n",
    "except ImportError:\n",
    "    raise ImportError('You should use Python 3.x')\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _change_one_hot_label(X):\n",
    "    T = np.zeros((X.size, 6))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "def load_AReM(normalize=False, standardze=False, one_hot_label=False):\n",
    "    \"\"\"AReM 데이터셋 읽기\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    normalize : 데이터를 0.0~1.0 사이의 값으로 정규화할지 정한다.\n",
    "    standardze : 데이터를 평균을 기준으로 어느정도 떨어지게 만들지 정한다.\n",
    "    one_hot_label :\n",
    "        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.\n",
    "        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (트레인 데이터, 트레인 정답),(발리데이션 데이터, 발리데이션 정답)\n",
    "    \"\"\"\n",
    "    #assert (not (normalize & standardze)), \"Choose one\"\n",
    "    with open('dataset.pkl', 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    if normalize:\n",
    "        for key in ('train_', 'val_'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] = (dataset[key] - np.min(dataset[key], axis=0)) / (\n",
    "                        np.max(dataset[key], axis=0) - np.min(dataset[key], axis=0))\n",
    "\n",
    "    if standardze:\n",
    "        for key in ('train_', 'val_'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] -= np.mean(dataset[key], axis=0)\n",
    "            dataset[key] /= np.std(dataset[key], axis=0)\n",
    "\n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
    "        dataset['val_label'] = _change_one_hot_label(dataset['val_label'])\n",
    "\n",
    "    return (dataset['train_'], dataset['train_label']), (dataset['val_'], dataset['val_label'])\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_AReM(normalize = False, standardze=False, one_hot_label=False)\n",
    "\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "print(t_train)\n",
    "print(x_train.shape)\n",
    "print(x_test)\n",
    "print(x_test.shape)\n",
    "print(t_test)\n",
    "print(t_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
