{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch: 1 , iteration: 0 , train acc:0.172 , test acc:0.172 , train loss:1.778 ===\n",
      "=== epoch: 2 , iteration: 250 , train acc:0.664 , test acc:0.664 , train loss:0.76 ===\n",
      "=== epoch: 3 , iteration: 500 , train acc:0.676 , test acc:0.67 , train loss:0.763 ===\n",
      "=== epoch: 4 , iteration: 750 , train acc:0.689 , test acc:0.69 , train loss:0.594 ===\n",
      "=== epoch: 5 , iteration: 1000 , train acc:0.697 , test acc:0.69 , train loss:0.639 ===\n",
      "=== epoch: 98 , iteration: 24250 , train acc:0.786 , test acc:0.766 , train loss:0.584 ===\n",
      "=== epoch: 99 , iteration: 24500 , train acc:0.79 , test acc:0.766 , train loss:0.522 ===\n",
      "=== epoch: 100 , iteration: 24750 , train acc:0.792 , test acc:0.772 , train loss:0.504 ===\n",
      "=== epoch: 101 , iteration: 25000 , train acc:0.787 , test acc:0.762 , train loss:0.476 ===\n",
      "=== epoch: 102 , iteration: 25250 , train acc:0.789 , test acc:0.765 , train loss:0.515 ===\n",
      "=== epoch: 298 , iteration: 74250 , train acc:0.803 , test acc:0.772 , train loss:0.466 ===\n",
      "=== epoch: 299 , iteration: 74500 , train acc:0.808 , test acc:0.779 , train loss:0.357 ===\n",
      "=== epoch: 300 , iteration: 74750 , train acc:0.807 , test acc:0.779 , train loss:0.589 ===\n",
      "=== epoch: 301 , iteration: 75000 , train acc:0.809 , test acc:0.778 , train loss:0.461 ===\n",
      "=== epoch: 302 , iteration: 75250 , train acc:0.805 , test acc:0.777 , train loss:0.382 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e59507f65c88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m                     \u001b[1;31m# 트레이너를 사용해 모델 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 546\u001b[1;33m                     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    547\u001b[0m                     \u001b[1;31m# 파라미터 보관\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m                     \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e59507f65c88>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minference_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e59507f65c88>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         \u001b[1;31m# 네트워크 업데이트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e59507f65c88>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, x, t, dropFlag)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropFlag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropFlag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e59507f65c88>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    352\u001b[0m         \"\"\"\n\u001b[0;32m    353\u001b[0m         \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e59507f65c88>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \"\"\"\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e59507f65c88>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    328\u001b[0m                 \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropFlag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                 \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropFlag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e59507f65c88>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0meMIN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meMIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "        x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "class RMSprop:\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "class CustomOptimizer:\n",
    "    pass\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(input, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / input)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 32, 32, 6], lr=0.005):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag #\n",
    "        self.params = {}\n",
    "        self.params['mean'] = mean\n",
    "        self.params['std'] = std\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.W = {} #\n",
    "        self.layer_unit = layer_unit\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = Adam(lr)\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            self.layers['Sigmoid{}'.format(i)] = Sigmoid() #Activation Function\n",
    "            \n",
    "            # dropOut\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(1, self.layer_size):\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'Sigmoid') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 확인\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag\n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        x2 = x.copy()\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key:\n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "        self.dropFlag=False\n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        self.__init_layer()\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = []):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 32, 32, 6]\n",
    "        self.layer_size = len(layers)\n",
    "        self.test_acc = None\n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch, True)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            if self.verbose and ((self.current_epoch <= 5) or (self.current_epoch>=98 and\\\n",
    "                                          self.current_epoch<=102) or (self.current_epoch>=298 and\\\n",
    "                                          self.current_epoch<=302) or (self.current_epoch>=498 and\\\n",
    "                                          self.current_epoch<=502) or\\\n",
    "                                          ((self.current_epoch) >= (self.epochs - 5))): \n",
    "                print(\"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "        self.test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(self.test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        y = self.network.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        acc += np.sum(y == t)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(one_hot_label=False)\n",
    "    \n",
    "    mean = np.mean(x_train, axis = 0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "\n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "    # hyperparameter\n",
    "    epochs = [1000]\n",
    "    batchs = [100]\n",
    "    learningRate = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "#     for i in range(10):\n",
    "#         learningRate.append(10 ** np.random.uniform (-6, -2))\n",
    "    layer_unit_list = [[6, 64, 64, 6], [6, 64, 64, 64, 6], [6, 64, 64, 64, 64, 6], \n",
    "                       [6, 128, 128, 6], [6, 128, 128, 128, 6], [6, 128, 128, 128, 128, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in batchs:\n",
    "            for lr in learningRate:\n",
    "                for layer_unit in layer_unit_list:\n",
    "                    sf = './Params/Adam/ff/params[si={}][ep={}][ba={}][lr={}][la={}]22.pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(mean, std, True, layer_unit)\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=True, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf) \n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "                    x = np.arange(len(trainer.train_loss_list))\n",
    "                    plt.plot(x, trainer.train_loss_list)\n",
    "                    plt.legend([\"loss\"]) # 각주\n",
    "                    plt.title('acc: {}, layerSize: {}, epoch : {}\\nbatch : {}, lr: {} layer: {}'\\\n",
    "                              .format(round(trainer.test_acc, 10), len(layer_unit), epoch, batch, \\\n",
    "                                      round(lr, 5), layer_unit))\n",
    "#                     plt.savefig('./Result/Adam/tanh/{}[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "#                                 .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch,\\\n",
    "#                                         round(lr, 3), layer_unit), dpi=100)\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    #final sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:583: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324aba5f4c034d31926b32ed08f92cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='전체', max=1.0, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:584: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478f5a6c4f33485e8b871ae6d1126f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='lr', max=1.0, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:585: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480f08b85b0c42d4a24d615ef9ee1690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='layer_unit', max=1.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch: 1 , iteration: 0 , train acc:0.175 , test acc:0.167 , train loss:1.776 ===\n",
      "=== epoch: 2 , iteration: 250 , train acc:0.609 , test acc:0.615 , train loss:0.746 ===\n",
      "=== epoch: 3 , iteration: 500 , train acc:0.661 , test acc:0.659 , train loss:0.815 ===\n",
      "=== epoch: 4 , iteration: 750 , train acc:0.673 , test acc:0.666 , train loss:0.703 ===\n",
      "=== epoch: 5 , iteration: 1000 , train acc:0.688 , test acc:0.678 , train loss:0.783 ===\n",
      "=== epoch: 98 , iteration: 24250 , train acc:0.798 , test acc:0.773 , train loss:0.47 ===\n",
      "=== epoch: 99 , iteration: 24500 , train acc:0.8 , test acc:0.777 , train loss:0.494 ===\n",
      "=== epoch: 100 , iteration: 24750 , train acc:0.802 , test acc:0.775 , train loss:0.451 ===\n",
      "=== epoch: 101 , iteration: 25000 , train acc:0.797 , test acc:0.767 , train loss:0.376 ===\n",
      "=== epoch: 102 , iteration: 25250 , train acc:0.798 , test acc:0.775 , train loss:0.402 ===\n",
      "=== epoch: 298 , iteration: 74250 , train acc:0.815 , test acc:0.776 , train loss:0.536 ===\n",
      "=== epoch: 299 , iteration: 74500 , train acc:0.818 , test acc:0.78 , train loss:0.353 ===\n",
      "=== epoch: 300 , iteration: 74750 , train acc:0.816 , test acc:0.781 , train loss:0.408 ===\n",
      "=== epoch: 301 , iteration: 75000 , train acc:0.816 , test acc:0.781 , train loss:0.375 ===\n",
      "=== epoch: 302 , iteration: 75250 , train acc:0.817 , test acc:0.778 , train loss:0.355 ===\n",
      "=== epoch: 498 , iteration: 124250 , train acc:0.824 , test acc:0.783 , train loss:0.409 ===\n",
      "=== epoch: 499 , iteration: 124500 , train acc:0.825 , test acc:0.783 , train loss:0.377 ===\n",
      "=== epoch: 500 , iteration: 124750 , train acc:0.824 , test acc:0.784 , train loss:0.444 ===\n",
      "=== epoch: 501 , iteration: 125000 , train acc:0.825 , test acc:0.782 , train loss:0.301 ===\n",
      "=== epoch: 502 , iteration: 125250 , train acc:0.824 , test acc:0.78 , train loss:0.337 ===\n",
      "=== epoch: 995 , iteration: 248500 , train acc:0.831 , test acc:0.778 , train loss:0.387 ===\n",
      "=== epoch: 996 , iteration: 248750 , train acc:0.828 , test acc:0.776 , train loss:0.393 ===\n",
      "=== epoch: 997 , iteration: 249000 , train acc:0.834 , test acc:0.782 , train loss:0.405 ===\n",
      "=== epoch: 998 , iteration: 249250 , train acc:0.832 , test acc:0.776 , train loss:0.438 ===\n",
      "=== epoch: 999 , iteration: 249500 , train acc:0.835 , test acc:0.783 , train loss:0.351 ===\n",
      "=== epoch: 1000 , iteration: 249750 , train acc:0.834 , test acc:0.782 , train loss:0.446 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7758352293138546, inference_time:8.743403985991104e-06\n",
      "[size = 6][epoch = 1000][batch = 100][lr = 0.0003][layer = [6, 64, 64, 64, 64, 6]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:613: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:132: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "        x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "class RMSprop:\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "class CustomOptimizer:\n",
    "    pass\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(input, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / input)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 32, 32, 6], lr=0.005):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag #\n",
    "        self.params = {}\n",
    "        self.params['mean'] = mean\n",
    "        self.params['std'] = std\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.W = {} #\n",
    "        self.layer_unit = layer_unit\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = Adam(lr)\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            self.layers['Sigmoid{}'.format(i)] = Sigmoid() #Activation Function\n",
    "            \n",
    "            # dropOut\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(1, self.layer_size):\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'Sigmoid') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 확인\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag\n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        x2 = x.copy()\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key:\n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "        self.dropFlag=False\n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        self.__init_layer()\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = []):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 32, 32, 6]\n",
    "        self.layer_size = len(layers)\n",
    "        self.test_acc = None\n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch, True)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            if self.verbose is False and ((self.current_epoch <= 5) or (self.current_epoch>=98 and\\\n",
    "                                          self.current_epoch<=102) or (self.current_epoch>=298 and\\\n",
    "                                          self.current_epoch<=302) or (self.current_epoch>=498 and\\\n",
    "                                          self.current_epoch<=502) or\\\n",
    "                                          ((self.current_epoch) >= (self.epochs - 5))): \n",
    "                print(\"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "        self.test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "        if self.verbose:\n",
    "            \"\"\"제출 전 수정\"\"\"\n",
    "            file = open('./Result/SGD/[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].txt'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers), 'w')\n",
    "            file.write(\"=============== Final Test Accuracy ===============\\n\")\n",
    "            file.write(\"test acc: %f,  \" % self.test_acc)\n",
    "            file.write(\"inference_time: %f\\n\" % inference_time)\n",
    "            file.close()\n",
    "        else:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(self.test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            \n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(standardze = False, one_hot_label=False)\n",
    "    \n",
    "    mean = np.mean(x_train, axis = 0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "\n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "    # hyperparameter\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    epochs = [1000]\n",
    "    batchs = [100]\n",
    "    learningRate = [0.0003]\n",
    "#     for i in range(10):\n",
    "#         learningRate.append(10 ** np.random.uniform (-6, -2))\n",
    "    layer_unit_list = [[6, 64, 64, 64, 64, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in tqdm_notebook(batchs, desc = '전체'):\n",
    "            for lr in tqdm_notebook(learningRate, desc = 'lr'):\n",
    "                for layer_unit in tqdm_notebook(layer_unit_list, desc = 'layer_unit'):\n",
    "                    sf = './Params/Adam/sig/params[si={}][ep={}][ba={}][lr={}][la={}]5.pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(mean, std, True, layer_unit)\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=False, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf) \n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "                    x = np.arange(len(trainer.train_loss_list))\n",
    "                    plt.plot(x, trainer.train_loss_list)\n",
    "                    plt.legend([\"loss\"]) # 각주\n",
    "                    plt.title('acc: {}, layerSize: {}, epoch : {}\\nbatch : {}, lr: {} layer: {}'\\\n",
    "                              .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch, \\\n",
    "                                      round(lr, 3), layer_unit))\n",
    "                    plt.savefig('./Result/Adam/tanh/{}[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "                                .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch,\\\n",
    "                                        round(lr, 3), layer_unit), dpi=100)\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    #final sigmoid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
