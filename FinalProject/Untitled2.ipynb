{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:583: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddbb89650954395b6ea1a128ab7ec3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='전체', max=1.0, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:584: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62671f0d05a447d2afb6c7d655aa908b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='lr', max=1.0, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:585: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611e3f29586f4e53859303e032fb71b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='layer_unit', max=1.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch: 1 , iteration: 0 , train acc:0.135 , test acc:0.139 , train loss:1.788 ===\n",
      "=== epoch: 2 , iteration: 250 , train acc:0.634 , test acc:0.638 , train loss:0.807 ===\n",
      "=== epoch: 3 , iteration: 500 , train acc:0.669 , test acc:0.666 , train loss:0.77 ===\n",
      "=== epoch: 4 , iteration: 750 , train acc:0.676 , test acc:0.676 , train loss:0.666 ===\n",
      "=== epoch: 5 , iteration: 1000 , train acc:0.698 , test acc:0.692 , train loss:0.688 ===\n",
      "=== epoch: 98 , iteration: 24250 , train acc:0.795 , test acc:0.771 , train loss:0.466 ===\n",
      "=== epoch: 99 , iteration: 24500 , train acc:0.795 , test acc:0.772 , train loss:0.459 ===\n",
      "=== epoch: 100 , iteration: 24750 , train acc:0.795 , test acc:0.767 , train loss:0.541 ===\n",
      "=== epoch: 101 , iteration: 25000 , train acc:0.794 , test acc:0.771 , train loss:0.334 ===\n",
      "=== epoch: 102 , iteration: 25250 , train acc:0.796 , test acc:0.774 , train loss:0.429 ===\n",
      "=== epoch: 298 , iteration: 74250 , train acc:0.817 , test acc:0.779 , train loss:0.374 ===\n",
      "=== epoch: 299 , iteration: 74500 , train acc:0.816 , test acc:0.778 , train loss:0.423 ===\n",
      "=== epoch: 300 , iteration: 74750 , train acc:0.817 , test acc:0.777 , train loss:0.432 ===\n",
      "=== epoch: 301 , iteration: 75000 , train acc:0.814 , test acc:0.778 , train loss:0.338 ===\n",
      "=== epoch: 302 , iteration: 75250 , train acc:0.817 , test acc:0.778 , train loss:0.466 ===\n",
      "=== epoch: 498 , iteration: 124250 , train acc:0.823 , test acc:0.782 , train loss:0.273 ===\n",
      "=== epoch: 499 , iteration: 124500 , train acc:0.825 , test acc:0.778 , train loss:0.281 ===\n",
      "=== epoch: 500 , iteration: 124750 , train acc:0.822 , test acc:0.778 , train loss:0.33 ===\n",
      "=== epoch: 501 , iteration: 125000 , train acc:0.824 , test acc:0.78 , train loss:0.422 ===\n",
      "=== epoch: 502 , iteration: 125250 , train acc:0.827 , test acc:0.78 , train loss:0.319 ===\n",
      "=== epoch: 995 , iteration: 248500 , train acc:0.836 , test acc:0.782 , train loss:0.33 ===\n",
      "=== epoch: 996 , iteration: 248750 , train acc:0.833 , test acc:0.782 , train loss:0.349 ===\n",
      "=== epoch: 997 , iteration: 249000 , train acc:0.833 , test acc:0.782 , train loss:0.341 ===\n",
      "=== epoch: 998 , iteration: 249250 , train acc:0.836 , test acc:0.783 , train loss:0.389 ===\n",
      "=== epoch: 999 , iteration: 249500 , train acc:0.833 , test acc:0.783 , train loss:0.409 ===\n",
      "=== epoch: 1000 , iteration: 249750 , train acc:0.833 , test acc:0.782 , train loss:0.425 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7837384744341995, inference_time:4.551308181611039e-06\n",
      "[size = 6][epoch = 1000][batch = 100][lr = 0.0003][layer = [6, 64, 64, 64, 64, 6]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:613: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:132: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "        x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "class RMSprop:\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "class CustomOptimizer:\n",
    "    pass\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(input, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / input)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 32, 32, 6], lr=0.005):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag #\n",
    "        self.params = {}\n",
    "        self.params['mean'] = mean\n",
    "        self.params['std'] = std\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.W = {} #\n",
    "        self.layer_unit = layer_unit\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = Adam(lr)\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            self.layers['Sigmoid{}'.format(i)] = Sigmoid() #Activation Function\n",
    "            \n",
    "            # dropOut\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(1, self.layer_size):\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'Sigmoid') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 확인\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag\n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        x2 = x.copy()\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key:\n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "        self.dropFlag=False\n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        self.__init_layer()\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = []):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 32, 32, 6]\n",
    "        self.layer_size = len(layers)\n",
    "        self.test_acc = None\n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch, True)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            if self.verbose is False and ((self.current_epoch <= 5) or (self.current_epoch>=98 and\\\n",
    "                                          self.current_epoch<=102) or (self.current_epoch>=298 and\\\n",
    "                                          self.current_epoch<=302) or (self.current_epoch>=498 and\\\n",
    "                                          self.current_epoch<=502) or\\\n",
    "                                          ((self.current_epoch) >= (self.epochs - 5))): \n",
    "                print(\"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "        self.test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "        if self.verbose:\n",
    "            \"\"\"제출 전 수정\"\"\"\n",
    "            file = open('./Result/SGD/[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].txt'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers), 'w')\n",
    "            file.write(\"=============== Final Test Accuracy ===============\\n\")\n",
    "            file.write(\"test acc: %f,  \" % self.test_acc)\n",
    "            file.write(\"inference_time: %f\\n\" % inference_time)\n",
    "            file.close()\n",
    "        else:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(self.test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            \n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class Tester:\n",
    "    \"\"\"\n",
    "    test 해주는 클래스. 수정불가\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_test, t_test, mini_batch_size=100, verbose=True):\n",
    "        self.network = network\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_test.shape[0]\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"\n",
    "        수정불가\n",
    "        \"\"\"\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            \n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time()-start_time)/x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(standardze = False, one_hot_label=False)\n",
    "    \n",
    "    mean = np.mean(x_train, axis = 0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "\n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "    # hyperparameter\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    epochs = [1000]\n",
    "    batchs = [100]\n",
    "    learningRate = [0.0003]\n",
    "#     for i in range(10):\n",
    "#         learningRate.append(10 ** np.random.uniform (-6, -2))\n",
    "    layer_unit_list = [[6, 64, 64, 64, 64, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in tqdm_notebook(batchs, desc = '전체'):\n",
    "            for lr in tqdm_notebook(learningRate, desc = 'lr'):\n",
    "                for layer_unit in tqdm_notebook(layer_unit_list, desc = 'layer_unit'):\n",
    "                    sf = './Params/Adam/sig/params[si={}][ep={}][ba={}][lr={}][la={}].pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(mean, std, True, layer_unit)\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=False, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf) \n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "                    x = np.arange(len(trainer.train_loss_list))\n",
    "                    plt.plot(x, trainer.train_loss_list)\n",
    "                    plt.legend([\"loss\"]) # 각주\n",
    "                    plt.title('acc: {}, layerSize: {}, epoch : {}\\nbatch : {}, lr: {} layer: {}'\\\n",
    "                              .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch, \\\n",
    "                                      round(lr, 3), layer_unit))\n",
    "                    plt.savefig('./Result/Adam/tanh/{}[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "                                .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch,\\\n",
    "                                        round(lr, 3), layer_unit), dpi=100)\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    #final sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:583: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa2c4f2ffbe431aa3ecfbbeb02860e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='전체', max=1.0, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:584: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0d434e9f4c4ea8be78e6ad7da82041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='lr', max=1.0, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:585: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4956bfb786417a8586eb7bf1b8eab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='layer_unit', max=1.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch: 1 , iteration: 0 , train acc:0.175 , test acc:0.167 , train loss:1.758 ===\n",
      "=== epoch: 2 , iteration: 250 , train acc:0.645 , test acc:0.65 , train loss:0.872 ===\n",
      "=== epoch: 3 , iteration: 500 , train acc:0.671 , test acc:0.669 , train loss:0.754 ===\n",
      "=== epoch: 4 , iteration: 750 , train acc:0.685 , test acc:0.681 , train loss:0.706 ===\n",
      "=== epoch: 5 , iteration: 1000 , train acc:0.701 , test acc:0.699 , train loss:0.678 ===\n",
      "=== epoch: 98 , iteration: 24250 , train acc:0.793 , test acc:0.777 , train loss:0.461 ===\n",
      "=== epoch: 99 , iteration: 24500 , train acc:0.795 , test acc:0.768 , train loss:0.377 ===\n",
      "=== epoch: 100 , iteration: 24750 , train acc:0.794 , test acc:0.766 , train loss:0.346 ===\n",
      "=== epoch: 101 , iteration: 25000 , train acc:0.794 , test acc:0.774 , train loss:0.481 ===\n",
      "=== epoch: 102 , iteration: 25250 , train acc:0.797 , test acc:0.78 , train loss:0.503 ===\n",
      "=== epoch: 298 , iteration: 74250 , train acc:0.816 , test acc:0.781 , train loss:0.311 ===\n",
      "=== epoch: 299 , iteration: 74500 , train acc:0.815 , test acc:0.781 , train loss:0.374 ===\n",
      "=== epoch: 300 , iteration: 74750 , train acc:0.809 , test acc:0.777 , train loss:0.35 ===\n",
      "=== epoch: 301 , iteration: 75000 , train acc:0.812 , test acc:0.777 , train loss:0.43 ===\n",
      "=== epoch: 302 , iteration: 75250 , train acc:0.814 , test acc:0.779 , train loss:0.379 ===\n",
      "=== epoch: 498 , iteration: 124250 , train acc:0.822 , test acc:0.782 , train loss:0.342 ===\n",
      "=== epoch: 499 , iteration: 124500 , train acc:0.824 , test acc:0.782 , train loss:0.459 ===\n",
      "=== epoch: 500 , iteration: 124750 , train acc:0.821 , test acc:0.781 , train loss:0.443 ===\n",
      "=== epoch: 501 , iteration: 125000 , train acc:0.823 , test acc:0.78 , train loss:0.41 ===\n",
      "=== epoch: 502 , iteration: 125250 , train acc:0.821 , test acc:0.782 , train loss:0.471 ===\n",
      "=== epoch: 995 , iteration: 248500 , train acc:0.83 , test acc:0.774 , train loss:0.336 ===\n",
      "=== epoch: 996 , iteration: 248750 , train acc:0.83 , test acc:0.78 , train loss:0.386 ===\n",
      "=== epoch: 997 , iteration: 249000 , train acc:0.834 , test acc:0.778 , train loss:0.353 ===\n",
      "=== epoch: 998 , iteration: 249250 , train acc:0.83 , test acc:0.778 , train loss:0.294 ===\n",
      "=== epoch: 999 , iteration: 249500 , train acc:0.827 , test acc:0.78 , train loss:0.367 ===\n",
      "=== epoch: 1000 , iteration: 249750 , train acc:0.828 , test acc:0.776 , train loss:0.487 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7770326906957251, inference_time:7.066565664239079e-06\n",
      "[size = 6][epoch = 1000][batch = 100][lr = 0.0003][layer = [6, 64, 64, 64, 64, 6]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:613: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:132: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "        x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "class RMSprop:\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "class CustomOptimizer:\n",
    "    pass\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(input, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / input)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 32, 32, 6], lr=0.005):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag #\n",
    "        self.params = {}\n",
    "        self.params['mean'] = mean\n",
    "        self.params['std'] = std\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.W = {} #\n",
    "        self.layer_unit = layer_unit\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = Adam(lr)\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            self.layers['Sigmoid{}'.format(i)] = Sigmoid() #Activation Function\n",
    "            \n",
    "            # dropOut\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(1, self.layer_size):\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'Sigmoid') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 확인\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag\n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        x2 = x.copy()\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key:\n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "        self.dropFlag=False\n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        self.__init_layer()\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = []):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 32, 32, 6]\n",
    "        self.layer_size = len(layers)\n",
    "        self.test_acc = None\n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch, True)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            if self.verbose is False and ((self.current_epoch <= 5) or (self.current_epoch>=98 and\\\n",
    "                                          self.current_epoch<=102) or (self.current_epoch>=298 and\\\n",
    "                                          self.current_epoch<=302) or (self.current_epoch>=498 and\\\n",
    "                                          self.current_epoch<=502) or\\\n",
    "                                          ((self.current_epoch) >= (self.epochs - 5))): \n",
    "                print(\"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "        self.test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "        if self.verbose:\n",
    "            \"\"\"제출 전 수정\"\"\"\n",
    "            file = open('./Result/SGD/[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].txt'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers), 'w')\n",
    "            file.write(\"=============== Final Test Accuracy ===============\\n\")\n",
    "            file.write(\"test acc: %f,  \" % self.test_acc)\n",
    "            file.write(\"inference_time: %f\\n\" % inference_time)\n",
    "            file.close()\n",
    "        else:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(self.test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            \n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class Tester:\n",
    "    \"\"\"\n",
    "    test 해주는 클래스. 수정불가\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_test, t_test, mini_batch_size=100, verbose=True):\n",
    "        self.network = network\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_test.shape[0]\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"\n",
    "        수정불가\n",
    "        \"\"\"\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            \n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time()-start_time)/x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(standardze = False, one_hot_label=False)\n",
    "    \n",
    "    mean = np.mean(x_train, axis = 0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "\n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "    # hyperparameter\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    epochs = [1000]\n",
    "    batchs = [100]\n",
    "    learningRate = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]\n",
    "#     for i in range(10):\n",
    "#         learningRate.append(10 ** np.random.uniform (-6, -2))\n",
    "    layer_unit_list = [[6, 64, 64, 64, 64, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in tqdm_notebook(batchs, desc = '전체'):\n",
    "            for lr in tqdm_notebook(learningRate, desc = 'lr'):\n",
    "                for layer_unit in tqdm_notebook(layer_unit_list, desc = 'layer_unit'):\n",
    "                    sf = './Params/Adam/sig/params[si={}][ep={}][ba={}][lr={}][la={}]11.pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(mean, std, True, layer_unit)\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=False, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf) \n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "                    x = np.arange(len(trainer.train_loss_list))\n",
    "                    plt.plot(x, trainer.train_loss_list)\n",
    "                    plt.legend([\"loss\"]) # 각주\n",
    "                    plt.title('acc: {}, layerSize: {}, epoch : {}\\nbatch : {}, lr: {} layer: {}'\\\n",
    "                              .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch, \\\n",
    "                                      round(lr, 3), layer_unit))\n",
    "#                     plt.savefig('./Result/Adam/tanh/{}[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "#                                 .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch,\\\n",
    "#                                         round(lr, 3), layer_unit), dpi=100)\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    #final sigmoid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
