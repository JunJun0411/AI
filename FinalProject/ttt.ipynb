{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1111\n",
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "#         eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "#         x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "class CustomOptimizer:\n",
    "    pass\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(input, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / input)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 32, 32, 6], lr=0.005):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag #\n",
    "        self.params = {}\n",
    "        self.params['mean'] = mean\n",
    "        self.params['std'] = std\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.W = {} #\n",
    "        self.layer_unit = layer_unit\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = Adam(lr)\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            self.layers['Relu{}'.format(i)] = Relu() #Activation Function\n",
    "            \n",
    "            # dropOut\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(1, self.layer_size):\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'Relu') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 확인\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag\n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        x2 = x.copy()\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key:\n",
    "                \n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "                \n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "        self.dropFlag=False\n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        self.__init_layer()\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = []):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 32, 32, 6]\n",
    "        self.layer_size = len(layers)\n",
    "        self.test_acc = None\n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch, True)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            if self.verbose is False and ((self.current_epoch <= 5) or (self.current_epoch>=98 and\\\n",
    "                                          self.current_epoch<=102) or\\\n",
    "                                          ((self.current_epoch) >= (self.epochs - 5))): \n",
    "                print(\"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "        self.test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "        if self.verbose:\n",
    "            \"\"\"제출 전 수정\"\"\"\n",
    "            file = open('./Result/SGD/[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].txt'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers), 'w')\n",
    "            file.write(\"=============== Final Test Accuracy ===============\\n\")\n",
    "            file.write(\"test acc: %f,  \" % self.test_acc)\n",
    "            file.write(\"inference_time: %f\\n\" % inference_time)\n",
    "            file.close()\n",
    "        else:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(self.test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            \n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class Tester:\n",
    "    \"\"\"\n",
    "    test 해주는 클래스. 수정불가\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_test, t_test, mini_batch_size=100, verbose=True):\n",
    "        self.network = network\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_test.shape[0]\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"\n",
    "        수정불가\n",
    "        \"\"\"\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            \n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time()-start_time)/x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(standardze = False, one_hot_label=False)\n",
    "    \n",
    "    mean = np.mean(x_train, axis = 0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "    # hyperparameter\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    epochs = [500]\n",
    "    batchs = [100]\n",
    "    learningRate = [0.0005]\n",
    "#     for i in range(10):\n",
    "#         learningRate.append(10 ** np.random.uniform (-6, -2))\n",
    "    layer_unit_list = [[6, 64, 64, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in batchs:\n",
    "            for lr in learningRate:\n",
    "                for layer_unit in layer_unit_list:\n",
    "                    sf = './Params/SGD/params[si={}][ep={}][ba={}][lr={}][la={}].pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(mean, std, True, layer_unit)\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=False, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf) \n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "                    x = np.arange(len(trainer.train_loss_list))\n",
    "                    plt.plot(x, trainer.train_loss_list)\n",
    "                    plt.legend([\"loss\"]) # 각주\n",
    "                    plt.title('acc: {}, layerSize: {}, epoch : {}\\nbatch : {}, lr: {} layer: {}'\\\n",
    "                              .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch, \\\n",
    "                                      round(lr, 3), layer_unit))\n",
    "                    plt.savefig('./Result/Adam/{}[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "                                .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch,\\\n",
    "                                        round(lr, 3), layer_unit), dpi=100)\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    #dropON Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch: 1 , iteration: 0 , train acc:0.383 , test acc:0.381 , train loss:1.481 ===\n",
      "=== epoch: 2 , iteration: 250 , train acc:0.715 , test acc:0.707 , train loss:0.765 ===\n",
      "=== epoch: 3 , iteration: 500 , train acc:0.733 , test acc:0.717 , train loss:0.605 ===\n",
      "=== epoch: 4 , iteration: 750 , train acc:0.745 , test acc:0.733 , train loss:0.601 ===\n",
      "=== epoch: 5 , iteration: 1000 , train acc:0.747 , test acc:0.728 , train loss:0.649 ===\n",
      "=== epoch: 98 , iteration: 24250 , train acc:0.839 , test acc:0.771 , train loss:0.438 ===\n",
      "=== epoch: 99 , iteration: 24500 , train acc:0.838 , test acc:0.766 , train loss:0.376 ===\n",
      "=== epoch: 100 , iteration: 24750 , train acc:0.836 , test acc:0.768 , train loss:0.339 ===\n",
      "=== epoch: 101 , iteration: 25000 , train acc:0.839 , test acc:0.767 , train loss:0.381 ===\n",
      "=== epoch: 102 , iteration: 25250 , train acc:0.838 , test acc:0.763 , train loss:0.356 ===\n",
      "=== epoch: 495 , iteration: 123500 , train acc:0.894 , test acc:0.753 , train loss:0.264 ===\n",
      "=== epoch: 496 , iteration: 123750 , train acc:0.894 , test acc:0.751 , train loss:0.382 ===\n",
      "=== epoch: 497 , iteration: 124000 , train acc:0.89 , test acc:0.749 , train loss:0.354 ===\n",
      "=== epoch: 498 , iteration: 124250 , train acc:0.892 , test acc:0.752 , train loss:0.201 ===\n",
      "=== epoch: 499 , iteration: 124500 , train acc:0.887 , test acc:0.745 , train loss:0.247 ===\n",
      "=== epoch: 500 , iteration: 124750 , train acc:0.894 , test acc:0.753 , train loss:0.19 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7435037720033529, inference_time:3.593151561658035e-06\n",
      "[size = 4][epoch = 500][batch = 100][lr = 0.0005][layer = [6, 64, 64, 6]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2222\n",
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "#         eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "#         x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "class CustomOptimizer:\n",
    "    pass\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(input, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / input)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 32, 32, 6], lr=0.005):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag #\n",
    "        self.params = {}\n",
    "        self.params['mean'] = mean\n",
    "        self.params['std'] = std\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.W = {} #\n",
    "        self.layer_unit = layer_unit\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = Adam(lr)\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            self.layers['tanh{}'.format(i)] = tanh() #Activation Function\n",
    "            \n",
    "            # dropOut\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(1, self.layer_size):\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'tanh') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 확인\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag\n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        x2 = x.copy()\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key:\n",
    "                \n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "                \n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "        self.dropFlag=False\n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        self.__init_layer()\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = []):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 32, 32, 6]\n",
    "        self.layer_size = len(layers)\n",
    "        self.test_acc = None\n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch, True)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            if self.verbose is False and ((self.current_epoch <= 5) or (self.current_epoch>=98 and\\\n",
    "                                          self.current_epoch<=102) or\\\n",
    "                                          ((self.current_epoch) >= (self.epochs - 5))): \n",
    "                print(\"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "        self.test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "        if self.verbose:\n",
    "            \"\"\"제출 전 수정\"\"\"\n",
    "            file = open('./Result/SGD/[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].txt'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers), 'w')\n",
    "            file.write(\"=============== Final Test Accuracy ===============\\n\")\n",
    "            file.write(\"test acc: %f,  \" % self.test_acc)\n",
    "            file.write(\"inference_time: %f\\n\" % inference_time)\n",
    "            file.close()\n",
    "        else:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(self.test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            \n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class Tester:\n",
    "    \"\"\"\n",
    "    test 해주는 클래스. 수정불가\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_test, t_test, mini_batch_size=100, verbose=True):\n",
    "        self.network = network\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_test.shape[0]\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"\n",
    "        수정불가\n",
    "        \"\"\"\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            \n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time()-start_time)/x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(standardze = False, one_hot_label=False)\n",
    "    \n",
    "    mean = np.mean(x_train, axis = 0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "    # hyperparameter\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    epochs = [500]\n",
    "    batchs = [100]\n",
    "    learningRate = [0.0005]\n",
    "#     for i in range(10):\n",
    "#         learningRate.append(10 ** np.random.uniform (-6, -2))\n",
    "    layer_unit_list = [[6, 64, 64, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in batchs:\n",
    "            for lr in learningRate:\n",
    "                for layer_unit in layer_unit_list:\n",
    "                    sf = './Params/SGD/params[si={}][ep={}][ba={}][lr={}][la={}].pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(mean, std, False, layer_unit)\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=False, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf) \n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "                    x = np.arange(len(trainer.train_loss_list))\n",
    "                    plt.plot(x, trainer.train_loss_list)\n",
    "                    plt.legend([\"loss\"]) # 각주\n",
    "                    plt.title('acc: {}, layerSize: {}, epoch : {}\\nbatch : {}, lr: {} layer: {}'\\\n",
    "                              .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch, \\\n",
    "                                      round(lr, 3), layer_unit))\n",
    "                    plt.savefig('./Result/Adam/{}[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "                                .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch,\\\n",
    "                                        round(lr, 3), layer_unit), dpi=100)\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    #tanh off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch: 1 , iteration: 0 , train acc:0.421 , test acc:0.418 , train loss:1.628 ===\n",
      "=== epoch: 2 , iteration: 250 , train acc:0.697 , test acc:0.688 , train loss:0.824 ===\n",
      "=== epoch: 3 , iteration: 500 , train acc:0.723 , test acc:0.71 , train loss:0.731 ===\n",
      "=== epoch: 4 , iteration: 750 , train acc:0.722 , test acc:0.711 , train loss:0.637 ===\n",
      "=== epoch: 5 , iteration: 1000 , train acc:0.733 , test acc:0.722 , train loss:0.687 ===\n",
      "=== epoch: 98 , iteration: 24250 , train acc:0.797 , test acc:0.765 , train loss:0.383 ===\n",
      "=== epoch: 99 , iteration: 24500 , train acc:0.797 , test acc:0.759 , train loss:0.461 ===\n",
      "=== epoch: 100 , iteration: 24750 , train acc:0.8 , test acc:0.77 , train loss:0.438 ===\n",
      "=== epoch: 101 , iteration: 25000 , train acc:0.797 , test acc:0.764 , train loss:0.46 ===\n",
      "=== epoch: 102 , iteration: 25250 , train acc:0.792 , test acc:0.758 , train loss:0.492 ===\n",
      "=== epoch: 495 , iteration: 123500 , train acc:0.813 , test acc:0.769 , train loss:0.474 ===\n",
      "=== epoch: 496 , iteration: 123750 , train acc:0.81 , test acc:0.77 , train loss:0.431 ===\n",
      "=== epoch: 497 , iteration: 124000 , train acc:0.813 , test acc:0.772 , train loss:0.286 ===\n",
      "=== epoch: 498 , iteration: 124250 , train acc:0.816 , test acc:0.766 , train loss:0.412 ===\n",
      "=== epoch: 499 , iteration: 124500 , train acc:0.815 , test acc:0.768 , train loss:0.498 ===\n",
      "=== epoch: 500 , iteration: 124750 , train acc:0.816 , test acc:0.769 , train loss:0.394 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7675727457789486, inference_time:4.7908972985814994e-06\n",
      "[size = 4][epoch = 500][batch = 100][lr = 0.0005][layer = [6, 64, 64, 6]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2222\n",
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "#         eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "#         x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "class CustomOptimizer:\n",
    "    pass\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(input, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / input)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 32, 32, 6], lr=0.005):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag #\n",
    "        self.params = {}\n",
    "        self.params['mean'] = mean\n",
    "        self.params['std'] = std\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.W = {} #\n",
    "        self.layer_unit = layer_unit\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = Adam(lr)\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            self.layers['tanh{}'.format(i)] = tanh() #Activation Function\n",
    "            \n",
    "            # dropOut\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(1, self.layer_size):\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'tanh') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 확인\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag\n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        x2 = x.copy()\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key:\n",
    "                \n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "                \n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "        self.dropFlag=False\n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        self.__init_layer()\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = []):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 32, 32, 6]\n",
    "        self.layer_size = len(layers)\n",
    "        self.test_acc = None\n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch, True)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            if self.verbose is False and ((self.current_epoch <= 5) or (self.current_epoch>=98 and\\\n",
    "                                          self.current_epoch<=102) or\\\n",
    "                                          ((self.current_epoch) >= (self.epochs - 5))): \n",
    "                print(\"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "        self.test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "        if self.verbose:\n",
    "            \"\"\"제출 전 수정\"\"\"\n",
    "            file = open('./Result/SGD/[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].txt'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers), 'w')\n",
    "            file.write(\"=============== Final Test Accuracy ===============\\n\")\n",
    "            file.write(\"test acc: %f,  \" % self.test_acc)\n",
    "            file.write(\"inference_time: %f\\n\" % inference_time)\n",
    "            file.close()\n",
    "        else:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(self.test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            \n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class Tester:\n",
    "    \"\"\"\n",
    "    test 해주는 클래스. 수정불가\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_test, t_test, mini_batch_size=100, verbose=True):\n",
    "        self.network = network\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_test.shape[0]\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"\n",
    "        수정불가\n",
    "        \"\"\"\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            \n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time()-start_time)/x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(standardze = False, one_hot_label=False)\n",
    "    \n",
    "    mean = np.mean(x_train, axis = 0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "    # hyperparameter\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    epochs = [500]\n",
    "    batchs = [100]\n",
    "    learningRate = [0.0005]\n",
    "#     for i in range(10):\n",
    "#         learningRate.append(10 ** np.random.uniform (-6, -2))\n",
    "    layer_unit_list = [[6, 64, 64, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in batchs:\n",
    "            for lr in learningRate:\n",
    "                for layer_unit in layer_unit_list:\n",
    "                    sf = './Params/SGD/params[si={}][ep={}][ba={}][lr={}][la={}].pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(mean, std, True, layer_unit)\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=False, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf) \n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "                    x = np.arange(len(trainer.train_loss_list))\n",
    "                    plt.plot(x, trainer.train_loss_list)\n",
    "                    plt.legend([\"loss\"]) # 각주\n",
    "                    plt.title('acc: {}, layerSize: {}, epoch : {}\\nbatch : {}, lr: {} layer: {}'\\\n",
    "                              .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch, \\\n",
    "                                      round(lr, 3), layer_unit))\n",
    "                    plt.savefig('./Result/Adam/{}[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "                                .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch,\\\n",
    "                                        round(lr, 3), layer_unit), dpi=100)\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    #dropON tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch: 1 , iteration: 0 , train acc:0.171 , test acc:0.177 , train loss:1.954 ===\n",
      "=== epoch: 2 , iteration: 250 , train acc:0.171 , test acc:0.177 , train loss:1.875 ===\n",
      "=== epoch: 3 , iteration: 500 , train acc:0.171 , test acc:0.177 , train loss:1.973 ===\n",
      "=== epoch: 4 , iteration: 750 , train acc:0.171 , test acc:0.177 , train loss:1.888 ===\n",
      "=== epoch: 5 , iteration: 1000 , train acc:0.171 , test acc:0.177 , train loss:1.827 ===\n",
      "=== epoch: 98 , iteration: 24250 , train acc:0.171 , test acc:0.177 , train loss:1.901 ===\n",
      "=== epoch: 99 , iteration: 24500 , train acc:0.171 , test acc:0.177 , train loss:1.996 ===\n",
      "=== epoch: 100 , iteration: 24750 , train acc:0.171 , test acc:0.177 , train loss:1.897 ===\n",
      "=== epoch: 101 , iteration: 25000 , train acc:0.171 , test acc:0.177 , train loss:1.953 ===\n",
      "=== epoch: 102 , iteration: 25250 , train acc:0.171 , test acc:0.177 , train loss:1.858 ===\n",
      "=== epoch: 495 , iteration: 123500 , train acc:0.171 , test acc:0.177 , train loss:1.896 ===\n",
      "=== epoch: 496 , iteration: 123750 , train acc:0.171 , test acc:0.177 , train loss:1.946 ===\n",
      "=== epoch: 497 , iteration: 124000 , train acc:0.171 , test acc:0.177 , train loss:1.975 ===\n",
      "=== epoch: 498 , iteration: 124250 , train acc:0.171 , test acc:0.177 , train loss:1.958 ===\n",
      "=== epoch: 499 , iteration: 124500 , train acc:0.171 , test acc:0.177 , train loss:2.048 ===\n",
      "=== epoch: 500 , iteration: 124750 , train acc:0.171 , test acc:0.177 , train loss:2.027 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.17674529996407615, inference_time:2.1560165556929464e-06\n",
      "[size = 4][epoch = 500][batch = 100][lr = 0.0005][layer = [6, 64, 64, 6]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch: 1 , iteration: 0 , train acc:0.173 , test acc:0.169 , train loss:1.769 ===\n",
      "=== epoch: 2 , iteration: 250 , train acc:0.655 , test acc:0.651 , train loss:0.696 ===\n",
      "=== epoch: 3 , iteration: 500 , train acc:0.678 , test acc:0.674 , train loss:0.747 ===\n",
      "=== epoch: 4 , iteration: 750 , train acc:0.695 , test acc:0.69 , train loss:0.652 ===\n",
      "=== epoch: 5 , iteration: 1000 , train acc:0.702 , test acc:0.698 , train loss:0.611 ===\n",
      "=== epoch: 98 , iteration: 24250 , train acc:0.813 , test acc:0.776 , train loss:0.35 ===\n",
      "=== epoch: 99 , iteration: 24500 , train acc:0.809 , test acc:0.777 , train loss:0.333 ===\n",
      "=== epoch: 100 , iteration: 24750 , train acc:0.811 , test acc:0.781 , train loss:0.487 ===\n",
      "=== epoch: 101 , iteration: 25000 , train acc:0.814 , test acc:0.784 , train loss:0.463 ===\n",
      "=== epoch: 102 , iteration: 25250 , train acc:0.812 , test acc:0.776 , train loss:0.378 ===\n",
      "=== epoch: 495 , iteration: 123500 , train acc:0.868 , test acc:0.765 , train loss:0.25 ===\n",
      "=== epoch: 496 , iteration: 123750 , train acc:0.865 , test acc:0.764 , train loss:0.3 ===\n",
      "=== epoch: 497 , iteration: 124000 , train acc:0.866 , test acc:0.762 , train loss:0.252 ===\n",
      "=== epoch: 498 , iteration: 124250 , train acc:0.869 , test acc:0.762 , train loss:0.46 ===\n",
      "=== epoch: 499 , iteration: 124500 , train acc:0.869 , test acc:0.764 , train loss:0.285 ===\n",
      "=== epoch: 500 , iteration: 124750 , train acc:0.868 , test acc:0.756 , train loss:0.241 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7617051850077835, inference_time:2.5152289329239202e-06\n",
      "[size = 4][epoch = 500][batch = 100][lr = 0.0005][layer = [6, 64, 64, 6]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1111\n",
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "#         eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "#         x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "class CustomOptimizer:\n",
    "    pass\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(input, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / input)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 32, 32, 6], lr=0.005):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag #\n",
    "        self.params = {}\n",
    "        self.params['mean'] = mean\n",
    "        self.params['std'] = std\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.W = {} #\n",
    "        self.layer_unit = layer_unit\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = Adam(lr)\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            self.layers['Sigmoid{}'.format(i)] = Sigmoid() #Activation Function\n",
    "            \n",
    "            # dropOut\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(1, self.layer_size):\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'Sigmoid') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 확인\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag\n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        x2 = x.copy()\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key:\n",
    "                \n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "                \n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "        self.dropFlag=False\n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        self.__init_layer()\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = []):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 32, 32, 6]\n",
    "        self.layer_size = len(layers)\n",
    "        self.test_acc = None\n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch, True)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            if self.verbose is False and ((self.current_epoch <= 5) or (self.current_epoch>=98 and\\\n",
    "                                          self.current_epoch<=102) or\\\n",
    "                                          ((self.current_epoch) >= (self.epochs - 5))): \n",
    "                print(\"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "        self.test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "        if self.verbose:\n",
    "            \"\"\"제출 전 수정\"\"\"\n",
    "            file = open('./Result/SGD/[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].txt'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers), 'w')\n",
    "            file.write(\"=============== Final Test Accuracy ===============\\n\")\n",
    "            file.write(\"test acc: %f,  \" % self.test_acc)\n",
    "            file.write(\"inference_time: %f\\n\" % inference_time)\n",
    "            file.close()\n",
    "        else:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(self.test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            \n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class Tester:\n",
    "    \"\"\"\n",
    "    test 해주는 클래스. 수정불가\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_test, t_test, mini_batch_size=100, verbose=True):\n",
    "        self.network = network\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_test.shape[0]\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"\n",
    "        수정불가\n",
    "        \"\"\"\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            \n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time()-start_time)/x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(standardze = False, one_hot_label=False)\n",
    "    \n",
    "    mean = np.mean(x_train, axis = 0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "    # hyperparameter\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    epochs = [500]\n",
    "    batchs = [100]\n",
    "    learningRate = [0.0005]\n",
    "#     for i in range(10):\n",
    "#         learningRate.append(10 ** np.random.uniform (-6, -2))\n",
    "    layer_unit_list = [[6, 64, 64, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in batchs:\n",
    "            for lr in learningRate:\n",
    "                for layer_unit in layer_unit_list:\n",
    "                    sf = './Params/SGD/params[si={}][ep={}][ba={}][lr={}][la={}].pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(mean, std, False, layer_unit)\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=False, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf) \n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "                    x = np.arange(len(trainer.train_loss_list))\n",
    "                    plt.plot(x, trainer.train_loss_list)\n",
    "                    plt.legend([\"loss\"]) # 각주\n",
    "                    plt.title('acc: {}, layerSize: {}, epoch : {}\\nbatch : {}, lr: {} layer: {}'\\\n",
    "                              .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch, \\\n",
    "                                      round(lr, 3), layer_unit))\n",
    "                    plt.savefig('./Result/{}[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "                                .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch,\\\n",
    "                                        round(lr, 3), layer_unit), dpi=100)\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    #dropoff sig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
