{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "#         eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "#         x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "\n",
    "class CustomOptimizer:\n",
    "    \"\"\" Adam : Momentum 과 AdaGrad 를 융합한 방법 \n",
    "    v, h 가 각각 최초 0으로 설정되어 학습 초반에 0으로 biased 되는 문제를 해결하기 위해 고안한 방법 \n",
    "    \"\"\"\n",
    "    def __init__(self, lr = 0.0001):\n",
    "        self.lr = lr                # learningRate\n",
    "        self.B1 = 0.9               # 베타1 0~1사이 값\n",
    "        self.B2 = 0.999             # 베타2 0~1사이 값\n",
    "        self.t = 0                  # Initialize timestep\n",
    "        self.epsilon = 1e-7         # 1e-8 or 1e-7 무관\n",
    "        self.m, self.v = None, None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        # None 인경우 m, v 초기화\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                    \n",
    "                #  Initialize 1st moment vector\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                #  Initialize 2nd moment vector\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.t += 1 # t = t + 1\n",
    "        # 연산속도 높이기 위해 key와 관련없는 값 반복문에서 빼서 미리 계산, epsilon은 작은 값이라 영향X\n",
    "        lr1 = self.lr * np.sqrt(1.0 - self.B2**self.t) / (1.0 - self.B1**self.t)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            # dropOut를 위해 저장된 파라미터는 무관하므로 Pass\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "                \n",
    "            # Update biased first moment estimate\n",
    "            self.m[key] = (self.B1 * self.m[key]) + ((1 - self.B1) * grads[key])\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[key] = (self.B2 * self.v[key]) + (1 - self.B2) * grads[key]**2\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            mt = self.m[key] # / (1.0 - self.B1**self.t) 미리 계산\n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            vt = self.v[key] # / (1.0 - self.B2**self.t) 미리 계산\n",
    "            # Update parameters\n",
    "            params[key] -= lr1 * mt / (np.sqrt(vt) + self.epsilon)\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(input, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / input)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 64, 64, 64, 64, 6], lr=0.0001):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag          # dropout 레이어 생성 여부\n",
    "        self.params = {} \n",
    "        self.params['mean'] = mean        # \n",
    "        self.params['std'] = std\n",
    "        \n",
    "        self.W = {}                       # 초깃값 plot해보기 위해 설정했던 값\n",
    "        self.layer_unit = layer_unit      # 레이어 [6, 64, 64, 64, 64, 6]\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()              # 초기 파라미터 설정\n",
    "        self.layers = OrderedDict()       # Ordered딕셔너리로 초기화\n",
    "        self.last_layer = None            # softMaxwithLoss\n",
    "        self.__init_layer()               # 레이어 설정\n",
    "        self.optimizer = CustomOptimizer(lr) # Adam으로 optimizer설정\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            # Affine 레이어 초기화\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            # Activation Function으로 Sigmoid 사용\n",
    "            self.layers['Sigmoid{}'.format(i)] = Sigmoid() \n",
    "            \n",
    "            # Flag = True일 경우 dropout레이어 만든다.\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        # 마지막 레이어는 SoftmaxWithLoss\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 파라미터 초기화 Model에 입력된 layer에 따라 만들어진다.\n",
    "        for i in range(1, self.layer_size):\n",
    "            # 초깃값 Sigmoid: 1 / np.sqrt(self.layer_unit[i - 1]) 곱해준다.\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'Sigmoid') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            # i번째 레이어 unit수만큼 0으로 초기화\n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 분포 확인(plot)위해 self.W에 저장\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Trainer의 Train_step에서 update의 경우에 dropFlag = True이므로 attribute로 저장해둔다.\n",
    "        self.dropFlag = dropFlag\n",
    "        \n",
    "        # 각 계층 forward, backward 전파 후 결과 dw, db 받아온다.\n",
    "        grads = self.gradient(x, t)\n",
    "        # 파라미터 갱신 Adam 사용\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        \n",
    "        # featureScaling\n",
    "        x2 = x.copy()\n",
    "        # dataload시 Model에서 받아온 mean, std를 이용한 Scaling\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        \n",
    "        # 각 레이어 forward propagation 진행\n",
    "        for key, layer in self.layers.items():\n",
    "            # dropout 계층에서는 Train하는 경우에만 dropout을 적용하므로\n",
    "            if \"Dropout\" in key:\n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "            # forward propagation 진행\n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "                \n",
    "        # Trainer의 Update이외에는 dropout을 끄도록 해준다.\n",
    "        self.dropFlag=False\n",
    "        \n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward propagation\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward propagation\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        # 모든 계층 리스트화\n",
    "        la = list(self.layers.values())\n",
    "        # 역전파를 위한 reverse\n",
    "        la.reverse()\n",
    "        \n",
    "        # 모든 레이어 backPropagation 진행\n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        # backPropagation 후 각 Affine계층에 dw와 db를 grads 딕셔너리로 리턴\n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {} # params['mean'], params['std'] 도 함께 dump한다.\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val # params['mean'], params['std'] 도 함께 load된다.\n",
    "        self.__init_layer()\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=1000, mini_batch_size=100,\n",
    "                 learning_rate=0.0001, verbose=True, layers = [6, 64, 64, 64, 64, 6]):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "        \n",
    "        self.layers = layers # [6, 64, 64, 64, 64, 6]\n",
    "        self.layer_size = len(layers) # 6\n",
    "        self.test_acc = None # Test_acc 확인위해 설정\n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트, Train Update에서는 dropOut = True로\n",
    "        self.network.update(x_batch, t_batch, True) \n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            if self.verbose: \n",
    "                print(\"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "        self.test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(self.test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        y = self.network.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        acc += np.sum(y == t)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "                                                 \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "    parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "    parser.add_argument(\"--epochs\", required=False, default=500, help=\"epochs : default=20\")\n",
    "    parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "    parser.add_argument(\"--learning_rate\", required=False, default=0.0003, help=\"learning_rate : default=0.01\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(one_hot_label=False)\n",
    "    \n",
    "    mean = np.mean(x_train, axis = 0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "# hyperparameter\n",
    "# 레이어 층과 unit수 설정 input = 6, hidden = [64, 64, 64, 64], output = 6\n",
    "    layer_unit = [6, 64, 64, 64, 64, 6]\n",
    "    \n",
    "    # 모델 초기화 featureScaling(mean, std), dropout=True, layer전달\n",
    "    network = Model(mean, std, True, layer_unit)\n",
    "\n",
    "    # 트레이너 초기화\n",
    "    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                      learning_rate=args.learning_rate, verbose=True, \n",
    "                      layers = layer_unit)\n",
    "\n",
    "    # 트레이너를 사용해 모델 학습\n",
    "    trainer.train()\n",
    "    # 파라미터 보관\n",
    "    network.save_params(args.sf)\n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "#                     x = np.arange(len(trainer.train_loss_list))\n",
    "#                     plt.plot(x, trainer.train_loss_list)\n",
    "#                     plt.legend([\"loss\"]) # 각주\n",
    "#                     plt.title('acc: {}, layerSize: {}, epoch : {}\\nbatch : {}, lr: {} layer: {}'\\\n",
    "#                               .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch, \\\n",
    "#                                       round(lr, 3), layer_unit))\n",
    "#                     plt.savefig('./Result/Adam/tanh/{}[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "#                                 .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch,\\\n",
    "#                                         round(lr, 3), layer_unit), dpi=100)\n",
    "#                     plt.show()\n",
    "#                     plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:626: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea55c0cc44748ddac402eeee660e44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='전체', max=1.0, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:627: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6531c6680a41eeae21b86699b6c3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='lr', max=1.0, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:628: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e887af6c34ac4a60ac4e04bd0837f3c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='layer_unit', max=1.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch: 1 , iteration: 0 , train acc:0.175 , test acc:0.167 , train loss:1.728 ===\n",
      "=== epoch: 2 , iteration: 250 , train acc:0.634 , test acc:0.639 , train loss:0.82 ===\n",
      "=== epoch: 3 , iteration: 500 , train acc:0.673 , test acc:0.668 , train loss:0.731 ===\n",
      "=== epoch: 4 , iteration: 750 , train acc:0.682 , test acc:0.68 , train loss:0.748 ===\n",
      "=== epoch: 5 , iteration: 1000 , train acc:0.693 , test acc:0.691 , train loss:0.576 ===\n",
      "=== epoch: 98 , iteration: 24250 , train acc:0.797 , test acc:0.772 , train loss:0.405 ===\n",
      "=== epoch: 99 , iteration: 24500 , train acc:0.791 , test acc:0.766 , train loss:0.445 ===\n",
      "=== epoch: 100 , iteration: 24750 , train acc:0.799 , test acc:0.774 , train loss:0.447 ===\n",
      "=== epoch: 101 , iteration: 25000 , train acc:0.795 , test acc:0.766 , train loss:0.437 ===\n",
      "=== epoch: 102 , iteration: 25250 , train acc:0.797 , test acc:0.77 , train loss:0.498 ===\n",
      "=== epoch: 298 , iteration: 74250 , train acc:0.814 , test acc:0.778 , train loss:0.427 ===\n",
      "=== epoch: 299 , iteration: 74500 , train acc:0.816 , test acc:0.779 , train loss:0.321 ===\n",
      "=== epoch: 300 , iteration: 74750 , train acc:0.812 , test acc:0.782 , train loss:0.366 ===\n",
      "=== epoch: 301 , iteration: 75000 , train acc:0.814 , test acc:0.775 , train loss:0.426 ===\n",
      "=== epoch: 302 , iteration: 75250 , train acc:0.815 , test acc:0.776 , train loss:0.442 ===\n",
      "=== epoch: 498 , iteration: 124250 , train acc:0.825 , test acc:0.781 , train loss:0.375 ===\n",
      "=== epoch: 499 , iteration: 124500 , train acc:0.826 , test acc:0.78 , train loss:0.345 ===\n",
      "=== epoch: 500 , iteration: 124750 , train acc:0.825 , test acc:0.783 , train loss:0.433 ===\n",
      "=== epoch: 501 , iteration: 125000 , train acc:0.826 , test acc:0.778 , train loss:0.396 ===\n",
      "=== epoch: 502 , iteration: 125250 , train acc:0.823 , test acc:0.776 , train loss:0.561 ===\n",
      "=== epoch: 995 , iteration: 248500 , train acc:0.837 , test acc:0.784 , train loss:0.383 ===\n",
      "=== epoch: 996 , iteration: 248750 , train acc:0.835 , test acc:0.782 , train loss:0.329 ===\n",
      "=== epoch: 997 , iteration: 249000 , train acc:0.833 , test acc:0.78 , train loss:0.375 ===\n",
      "=== epoch: 998 , iteration: 249250 , train acc:0.835 , test acc:0.781 , train loss:0.346 ===\n",
      "=== epoch: 999 , iteration: 249500 , train acc:0.837 , test acc:0.782 , train loss:0.398 ===\n",
      "=== epoch: 1000 , iteration: 249750 , train acc:0.835 , test acc:0.782 , train loss:0.33 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7860136510597533, inference_time:6.707296187599866e-06\n",
      "[size = 6][epoch = 1000][batch = 100][lr = 0.0003][layer = [6, 64, 64, 64, 64, 6]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:656: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "C:\\Users\\Jun\\anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py:132: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "        x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "class CustomOptimizer:\n",
    "    \"\"\" Adam : Momentum 과 AdaGrad 를 융합한 방법 \n",
    "    v, h 가 각각 최초 0으로 설정되어 학습 초반에 0으로 biased 되는 문제를 해결하기 위해 고안한 방법 \n",
    "    \"\"\"\n",
    "    def __init__(self, lr = 0.0001):\n",
    "        self.lr = lr                # learningRate\n",
    "        self.B1 = 0.9               # 베타1 0~1사이 값\n",
    "        self.B2 = 0.999             # 베타2 0~1사이 값\n",
    "        self.t = 0                  # Initialize timestep\n",
    "        self.epsilon = 1e-7         # 1e-8 or 1e-7 무관\n",
    "        self.m, self.v = None, None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        # None 인경우 m, v 초기화\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                    \n",
    "                #  Initialize 1st moment vector\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                #  Initialize 2nd moment vector\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.t += 1 # t = t + 1\n",
    "        # 연산속도 높이기 위해 key와 관련없는 값 반복문에서 빼서 미리 계산, epsilon은 작은 값이라 영향X\n",
    "        lr1 = self.lr * np.sqrt(1.0 - self.B2**self.t) / (1.0 - self.B1**self.t)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            # dropOut를 위해 저장된 파라미터는 무관하므로 Pass\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "                \n",
    "            # Update biased first moment estimate\n",
    "            self.m[key] = (self.B1 * self.m[key]) + ((1 - self.B1) * grads[key])\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[key] = (self.B2 * self.v[key]) + (1 - self.B2) * grads[key]**2\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            mt = self.m[key] # / (1.0 - self.B1**self.t) 미리 계산\n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            vt = self.v[key] # / (1.0 - self.B2**self.t) 미리 계산\n",
    "            # Update parameters\n",
    "            params[key] -= lr1 * mt / (np.sqrt(vt) + self.epsilon)\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(input, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / input)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 32, 32, 6], lr=0.005):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag #\n",
    "        self.params = {}\n",
    "        self.params['mean'] = mean\n",
    "        self.params['std'] = std\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.W = {} #\n",
    "        self.layer_unit = layer_unit\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = CustomOptimizer(lr)\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            self.layers['Sigmoid{}'.format(i)] = Sigmoid() #Activation Function\n",
    "            \n",
    "            # dropOut\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(1, self.layer_size):\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'Sigmoid') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 확인\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag\n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        x2 = x.copy()\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key:\n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "        self.dropFlag=False\n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        self.__init_layer()\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = []):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 32, 32, 6]\n",
    "        self.layer_size = len(layers)\n",
    "        self.test_acc = None\n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch, True)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            if self.verbose is False and ((self.current_epoch <= 5) or (self.current_epoch>=98 and\\\n",
    "                                          self.current_epoch<=102) or (self.current_epoch>=298 and\\\n",
    "                                          self.current_epoch<=302) or (self.current_epoch>=498 and\\\n",
    "                                          self.current_epoch<=502) or\\\n",
    "                                          ((self.current_epoch) >= (self.epochs - 5))): \n",
    "                print(\"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "        self.test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "        if self.verbose:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(self.test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            \n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(one_hot_label=False)\n",
    "    \n",
    "    mean = np.mean(x_train, axis = 0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "    # hyperparameter\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    epochs = [1000]\n",
    "    batchs = [100]\n",
    "    learningRate = [0.0003]\n",
    "#     for i in range(10):\n",
    "#         learningRate.append(10 ** np.random.uniform (-6, -2))\n",
    "    layer_unit_list = [[6, 64, 64, 64, 64, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in batchs:\n",
    "            for lr in learningRate:\n",
    "                for layer_unit in layer_unit_list:\n",
    "                    sf = './Params/Adam/sig/params[si={}][ep={}][ba={}][lr={}][la={}]6.pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(mean, std, True, layer_unit)\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=False, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf) \n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "                    x = np.arange(len(trainer.train_loss_list))\n",
    "                    plt.plot(x, trainer.train_loss_list)\n",
    "                    plt.legend([\"loss\"]) # 각주\n",
    "                    plt.title('acc: {}, layerSize: {}, epoch : {}\\nbatch : {}, lr: {} layer: {}'\\\n",
    "                              .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch, \\\n",
    "                                      round(lr, 3), layer_unit))\n",
    "                    plt.savefig('./Result/Adam/tanh/{}[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "                                .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch,\\\n",
    "                                        round(lr, 3), layer_unit), dpi=100)\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    #final sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch: 1 , iteration: 0 , train acc:0.173 , test acc:0.169 , train loss:1.818 ===\n",
      "=== epoch: 2 , iteration: 250 , train acc:0.294 , test acc:0.281 , train loss:1.768 ===\n",
      "=== epoch: 3 , iteration: 500 , train acc:0.262 , test acc:0.25 , train loss:1.768 ===\n",
      "=== epoch: 4 , iteration: 750 , train acc:0.346 , test acc:0.338 , train loss:1.719 ===\n",
      "=== epoch: 5 , iteration: 1000 , train acc:0.389 , test acc:0.396 , train loss:1.497 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0adf8da35c8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m                     \u001b[1;31m# 트레이너를 사용해 모델 학습\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m                     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m                     \u001b[1;31m# 파라미터 보관\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m                     \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0adf8da35c8c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minference_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0adf8da35c8c>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[1;31m# 네트워크 업데이트\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0adf8da35c8c>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \"\"\"\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0adf8da35c8c>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    360\u001b[0m             \u001b[1;31m# forward propagation 진행\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m                 \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;31m# Trainer의 Update이외에는 dropout을 끄도록 해준다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0adf8da35c8c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "#         eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "#         x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "\n",
    "class CustomOptimizer:\n",
    "    \"\"\" Adam : Momentum 과 AdaGrad 를 융합한 방법 \n",
    "    v, h 가 각각 최초 0으로 설정되어 학습 초반에 0으로 biased 되는 문제를 해결하기 위해 고안한 방법 \n",
    "    \"\"\"\n",
    "    def __init__(self, lr = 0.0001):\n",
    "        self.lr = lr                # learningRate\n",
    "        self.B1 = 0.9               # 베타1 0~1사이 값\n",
    "        self.B2 = 0.999             # 베타2 0~1사이 값\n",
    "        self.t = 0                  # Initialize timestep\n",
    "        self.epsilon = 1e-7         # 1e-8 or 1e-7 무관\n",
    "        self.m, self.v = None, None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        # None 인경우 m, v 초기화\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                    \n",
    "                #  Initialize 1st moment vector\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                #  Initialize 2nd moment vector\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.t += 1 # t = t + 1\n",
    "        # 연산속도 높이기 위해 key와 관련없는 값 반복문에서 빼서 미리 계산, epsilon은 작은 값이라 영향X\n",
    "        lr1 = self.lr * np.sqrt(1.0 - self.B2**self.t) / (1.0 - self.B1**self.t)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            # dropOut를 위해 저장된 파라미터는 무관하므로 Pass\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "                \n",
    "            # Update biased first moment estimate\n",
    "            self.m[key] = (self.B1 * self.m[key]) + ((1 - self.B1) * grads[key])\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[key] = (self.B2 * self.v[key]) + (1 - self.B2) * grads[key]**2\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            mt = self.m[key] # / (1.0 - self.B1**self.t) 미리 계산\n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            vt = self.v[key] # / (1.0 - self.B2**self.t) 미리 계산\n",
    "            # Update parameters\n",
    "            params[key] -= lr1 * mt / (np.sqrt(vt) + self.epsilon)\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(n, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / n)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(n)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 64, 64, 64, 64, 6], lr=0.0001):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag          # dropout 레이어 생성 여부\n",
    "        self.params = {} \n",
    "        self.params['mean'] = mean        # \n",
    "        self.params['std'] = std\n",
    "        \n",
    "        self.W = {}                       # 초깃값 plot해보기 위해 설정했던 값\n",
    "        self.layer_unit = layer_unit      # 레이어 [6, 64, 64, 64, 64, 6]\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()              # 초기 파라미터 설정\n",
    "        self.layers = OrderedDict()       # Ordered딕셔너리로 초기화\n",
    "        self.last_layer = None            # softMaxwithLoss\n",
    "        self.__init_layer()               # 레이어 설정\n",
    "        self.optimizer = CustomOptimizer(lr) # Adam으로 optimizer설정\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            # Affine 레이어 초기화\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            # Activation Function으로 Sigmoid 사용\n",
    "            self.layers['Sigmoid{}'.format(i)] = Sigmoid() \n",
    "            \n",
    "            # Flag = True일 경우 dropout레이어 만든다.\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        # 마지막 레이어는 SoftmaxWithLoss\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 파라미터 초기화 Model에 입력된 layer에 따라 만들어진다.\n",
    "        for i in range(1, self.layer_size):\n",
    "            # 초깃값 Sigmoid: 1 / np.sqrt(self.layer_unit[i - 1]) 곱해준다.\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'Sigmoid') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            # i번째 레이어 unit수만큼 0으로 초기화\n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 분포 확인(plot)위해 self.W에 저장\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        # Trainer의 Train_step에서 update의 경우에 dropFlag = True이므로 attribute로 저장해둔다.\n",
    "        self.dropFlag = dropFlag\n",
    "        \n",
    "        # 각 계층 forward, backward 전파 후 결과 dw, db 받아온다.\n",
    "        grads = self.gradient(x, t)\n",
    "        # 파라미터 갱신 Adam 사용\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        \n",
    "        # featureScaling\n",
    "        x2 = x.copy()\n",
    "        # dataload시 Model에서 받아온 mean, std를 이용한 Scaling\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        \n",
    "        # 각 레이어 forward propagation 진행\n",
    "        for key, layer in self.layers.items():\n",
    "            # dropout 계층에서는 Train하는 경우에만 dropout을 적용하므로\n",
    "            if \"Dropout\" in key:\n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "            # forward propagation 진행\n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "                \n",
    "        # Trainer의 Update이외에는 dropout을 끄도록 해준다.\n",
    "        self.dropFlag=False\n",
    "        \n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward propagation\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward propagation\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        # 모든 계층 리스트화\n",
    "        la = list(self.layers.values())\n",
    "        # 역전파를 위한 reverse\n",
    "        la.reverse()\n",
    "        \n",
    "        # 모든 레이어 backPropagation 진행\n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        # backPropagation 후 각 Affine계층에 dw와 db를 grads 딕셔너리로 리턴\n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {} # params['mean'], params['std'] 도 함께 dump한다.\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val # params['mean'], params['std'] 도 함께 load된다.\n",
    "        self.__init_layer()\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = []):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 32, 32, 6]\n",
    "        self.layer_size = len(layers)\n",
    "        self.test_acc = None\n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch, True)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            if self.verbose is False and ((self.current_epoch <= 5) or (self.current_epoch>=98 and\\\n",
    "                                          self.current_epoch<=102) or (self.current_epoch>=298 and\\\n",
    "                                          self.current_epoch<=302) or (self.current_epoch>=498 and\\\n",
    "                                          self.current_epoch<=502) or\\\n",
    "                                          ((self.current_epoch) >= (self.epochs - 5))): \n",
    "                print(\"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "        self.test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "        if self.verbose:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(self.test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            \n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(one_hot_label=False)\n",
    "    \n",
    "    mean = np.mean(x_train, axis = 0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "    # hyperparameter\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    epochs = [1000]\n",
    "    batchs = [100]\n",
    "    learningRate = [0.005]\n",
    "#     for i in range(10):\n",
    "#         learningRate.append(10 ** np.random.uniform (-6, -2))\n",
    "    layer_unit_list = [[6, 64, 64, 64, 64, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in batchs:\n",
    "            for lr in learningRate:\n",
    "                for layer_unit in layer_unit_list:\n",
    "                    sf = './Params/Adam/sig/params[si={}][ep={}][ba={}][lr={}][la={}]6.pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(mean, std, True, layer_unit, args.learning_rate)\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=False, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf) \n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "                    x = np.arange(len(trainer.train_loss_list))\n",
    "                    plt.plot(x, trainer.train_loss_list)\n",
    "                    plt.legend([\"loss\"]) # 각주\n",
    "                    plt.title('acc: {}, layerSize: {}, epoch : {}\\nbatch : {}, lr: {} layer: {}'\\\n",
    "                              .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch, \\\n",
    "                                      round(lr, 3), layer_unit))\n",
    "                    plt.savefig('./Result/Adam/tanh/{}[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "                                .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch,\\\n",
    "                                        round(lr, 3), layer_unit), dpi=100)\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    #final sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch: 1 , iteration: 0 , train acc:0.293 , test acc:0.293 , train loss:1.843 ===\n",
      "=== epoch: 2 , iteration: 250 , train acc:0.712 , test acc:0.701 , train loss:0.813 ===\n",
      "=== epoch: 3 , iteration: 500 , train acc:0.733 , test acc:0.721 , train loss:0.625 ===\n",
      "=== epoch: 4 , iteration: 750 , train acc:0.739 , test acc:0.724 , train loss:0.527 ===\n",
      "=== epoch: 5 , iteration: 1000 , train acc:0.748 , test acc:0.737 , train loss:0.587 ===\n",
      "=== epoch: 98 , iteration: 24250 , train acc:0.821 , test acc:0.776 , train loss:0.378 ===\n",
      "=== epoch: 99 , iteration: 24500 , train acc:0.825 , test acc:0.777 , train loss:0.376 ===\n",
      "=== epoch: 100 , iteration: 24750 , train acc:0.822 , test acc:0.768 , train loss:0.471 ===\n",
      "=== epoch: 101 , iteration: 25000 , train acc:0.819 , test acc:0.768 , train loss:0.466 ===\n",
      "=== epoch: 102 , iteration: 25250 , train acc:0.824 , test acc:0.775 , train loss:0.425 ===\n",
      "=== epoch: 298 , iteration: 74250 , train acc:0.839 , test acc:0.766 , train loss:0.316 ===\n",
      "=== epoch: 299 , iteration: 74500 , train acc:0.842 , test acc:0.763 , train loss:0.211 ===\n",
      "=== epoch: 300 , iteration: 74750 , train acc:0.84 , test acc:0.77 , train loss:0.282 ===\n",
      "=== epoch: 301 , iteration: 75000 , train acc:0.84 , test acc:0.769 , train loss:0.373 ===\n",
      "=== epoch: 302 , iteration: 75250 , train acc:0.845 , test acc:0.766 , train loss:0.278 ===\n",
      "=== epoch: 495 , iteration: 123500 , train acc:0.85 , test acc:0.766 , train loss:0.355 ===\n",
      "=== epoch: 496 , iteration: 123750 , train acc:0.853 , test acc:0.77 , train loss:0.395 ===\n",
      "=== epoch: 497 , iteration: 124000 , train acc:0.849 , test acc:0.765 , train loss:0.317 ===\n",
      "=== epoch: 498 , iteration: 124250 , train acc:0.854 , test acc:0.77 , train loss:0.206 ===\n",
      "=== epoch: 499 , iteration: 124500 , train acc:0.849 , test acc:0.766 , train loss:0.388 ===\n",
      "=== epoch: 500 , iteration: 124750 , train acc:0.85 , test acc:0.762 , train loss:0.282 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.7646988384624596, inference_time:1.7966043305331368e-06\n",
      "[size = 4][epoch = 500][batch = 100][lr = 0.0005][layer = [6, 64, 64, 6]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "# 2020/인공지능/final/B511074/박준형\n",
    "import sys, os\n",
    "import argparse\n",
    "import time\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "from AReM import *\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)  # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        \n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        eMIN = -np.log(np.finfo(type(0.1)).max)\n",
    "        x = np.array(np.maximum(x, eMIN))\n",
    "        self.out = 1.0 / (1 + np.exp(-x))\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class tanh:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out**2)\n",
    "        \n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x, self.dw, self.db = None, None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dw = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.t = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if t.ndim == 1: #one hot 안되어 있는 경우\n",
    "            t = np.eye(6)[t]\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        \n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "                params[key] += self.v[key]\n",
    "                \n",
    "class Adagrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            for key in params.keys():\n",
    "                self.h[key] = grads[key] * grads[key]\n",
    "                params[key] -= self.lr * grads[key] / np.sqrt(self.h[key] + 1e-7)\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "class CustomOptimizer:\n",
    "    \"\"\" Adam : Momentum 과 AdaGrad 를 융합한 방법 \n",
    "    v, h 가 각각 최초 0으로 설정되어 학습 초반에 0으로 biased 되는 문제를 해결하기 위해 고안한 방법 \n",
    "    \"\"\"\n",
    "    def __init__(self, lr = 0.0001):\n",
    "        self.lr = lr                # learningRate\n",
    "        self.B1 = 0.9               # 베타1 0~1사이 값\n",
    "        self.B2 = 0.999             # 베타2 0~1사이 값\n",
    "        self.t = 0                  # Initialize timestep\n",
    "        self.epsilon = 1e-7         # 1e-8 or 1e-7 무관\n",
    "        self.m, self.v = None, None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        # None 인경우 m, v 초기화\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                # dropOut를 위해 저장된 파라미터는 갱신하지 않으므로 Pass\n",
    "                if key is 'mean' or key is 'std':\n",
    "                    continue\n",
    "                    \n",
    "                #  Initialize 1st moment vector\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                #  Initialize 2nd moment vector\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.t += 1 # t = t + 1\n",
    "        # 연산속도 높이기 위해 key와 관련없는 값 반복문에서 빼서 미리 계산, epsilon은 작은 값이라 영향X\n",
    "        lr1 = self.lr * np.sqrt(1.0 - self.B2**self.t) / (1.0 - self.B1**self.t)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            # dropOut를 위해 저장된 파라미터는 무관하므로 Pass\n",
    "            if key is 'mean' or key is 'std':\n",
    "                continue\n",
    "                \n",
    "            # Update biased first moment estimate\n",
    "            self.m[key] = (self.B1 * self.m[key]) + ((1 - self.B1) * grads[key])\n",
    "            # Update biased second raw moment estimate\n",
    "            self.v[key] = (self.B2 * self.v[key]) + (1 - self.B2) * grads[key]**2\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            mt = self.m[key] # / (1.0 - self.B1**self.t) 미리 계산\n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            vt = self.v[key] # / (1.0 - self.B2**self.t) 미리 계산\n",
    "            # Update parameters\n",
    "            params[key] -= lr1 * mt / (np.sqrt(vt) + self.epsilon)\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.1):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = False):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            \n",
    "            return x * self.mask\n",
    "        \n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    \n",
    "def weight_init_std(input, type = 'Relu'):\n",
    "    \"\"\" 가중치 초깃값 \"\"\"\n",
    "\n",
    "    # he 초깃값 -> Relu\n",
    "    if type is 'Relu':\n",
    "        return np.sqrt(2.0 / input)\n",
    "      \n",
    "    # Xavier 초깃값 -> sigmoid, tanh\n",
    "    elif type is 'Sigmoid' or type is 'tanh':\n",
    "        return 1.0 / np.sqrt(input)\n",
    "    \n",
    "class Model:\n",
    "    \"\"\"\n",
    "    네트워크 모델 입니다.\n",
    "\n",
    "    \"\"\"\n",
    "    \"\"\"제출 전 수정\"\"\"\n",
    "    def __init__(self, mean = None, std = None, dropFlag = False, layer_unit = [6, 32, 32, 6], lr=0.005):\n",
    "        \"\"\"\n",
    "        클래스 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag #\n",
    "        self.params = {}\n",
    "        self.params['mean'] = mean\n",
    "        self.params['std'] = std\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.W = {} #\n",
    "        self.layer_unit = layer_unit\n",
    "        self.layer_size = len(layer_unit) # 레이어 수\n",
    "        self.__init_weight()\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.__init_layer()\n",
    "        self.optimizer = CustomOptimizer(lr)\n",
    "        \n",
    "    def __init_layer(self):\n",
    "        \"\"\"\n",
    "        레이어를 생성하시면 됩니다.\n",
    "        \"\"\"\n",
    "        # Input layer -> hidden layer -> hidden...\n",
    "        for i in range(1, self.layer_size - 1):\n",
    "            self.layers['Affine{}'.format(i)] = \\\n",
    "                Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "            \n",
    "            self.layers['Relu{}'.format(i)] = Relu() #Activation Function\n",
    "            \n",
    "            # dropOut\n",
    "            if self.dropFlag:\n",
    "                self.layers['Dropout{}'.format(i)] = Dropout()\n",
    "        \n",
    "        # hidden layer -> output\n",
    "        i = self.layer_size - 1\n",
    "        self.layers['Affine{}'.format(i)] = \\\n",
    "            Affine(self.params['W{}'.format(i)], self.params['b{}'.format(i)])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def __init_weight(self):\n",
    "        \"\"\"\n",
    "        레이어에 탑재 될 파라미터들을 초기화 하시면 됩니다.\n",
    "        \"\"\"\n",
    "    \n",
    "        for i in range(1, self.layer_size):\n",
    "            self.params['W{}'.format(i)] = weight_init_std(self.layer_unit[i - 1], 'Relu') \\\n",
    "                        * np.random.randn(self.layer_unit[i - 1], self.layer_unit[i]) \n",
    "            self.params['b{}'.format(i)] = np.zeros(self.layer_unit[i])\n",
    "            \n",
    "            # 초기 Weight값 확인\n",
    "            self.W['W{}'.format(i)] = self.params['W{}'.format(i)].copy()\n",
    "            self.W['b{}'.format(i)] = self.params['b{}'.format(i)].copy()\n",
    "        \n",
    "    def update(self, x, t, dropFlag = False):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구한 뒤\n",
    "         옵티마이저 클래스를 사용해서 네트워크 파라미터를 업데이트 해주는 함수입니다.\n",
    "\n",
    "        :param x: train_data\n",
    "        :param t: test_data\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dropFlag = dropFlag\n",
    "        grads = self.gradient(x, t)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        데이터를 입력받아 정답을 예측하는 함수입니다.\n",
    "\n",
    "        :param x: data\n",
    "        :return: predicted answer\n",
    "        \"\"\"\n",
    "        x2 = x.copy()\n",
    "        x2 -= self.params['mean']\n",
    "        x2 /= self.params['std']\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"Dropout\" in key:\n",
    "                x2 = layer.forward(x2, self.dropFlag)\n",
    "            else:\n",
    "                x2 = layer.forward(x2)\n",
    "        self.dropFlag=False\n",
    "        return x2\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"\n",
    "        데이터와 레이블을 입력받아 로스를 구하는 함수입니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        train 데이터와 레이블을 사용해서 그라디언트를 구하는 함수입니다.\n",
    "        첫번째로 받은데이터를 forward propagation 시키고,\n",
    "        두번째로 back propagation 시켜 grads에 미분값을 리턴합니다.\n",
    "        :param x: data\n",
    "        :param t: data_label\n",
    "        :return: grads\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = self.last_layer.backward(1)\n",
    "        \n",
    "        la = list(self.layers.values())\n",
    "        la.reverse()\n",
    "        \n",
    "        for layer in la:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        \n",
    "        for i in range(1, self.layer_size):\n",
    "            grads['W{}'.format(i)] = self.layers['Affine{}'.format(i)].dw\n",
    "            grads['b{}'.format(i)] = self.layers['Affine{}'.format(i)].db\n",
    "                \n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        네트워크 파라미터를 피클 파일로 저장하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 저장할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        \"\"\"\n",
    "        저장된 파라미터를 읽어와 네트워크에 탑재하는 함수입니다.\n",
    "\n",
    "        :param file_name: 파라미터를 로드할 파일 이름입니다. 기본값은 \"params.pkl\" 입니다.\n",
    "        \"\"\"\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        self.__init_layer()\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    ex) 200개의 훈련데이터셋, 배치사이즈=5, 에폭=1000 일 경우 :\n",
    "    40개의 배치(배치당 5개 데이터)를 에폭 갯수 만큼 업데이트 하는것.=\n",
    "    (200 / 5) * 1000 = 40,000번 업데이트.\n",
    "\n",
    "    ----------\n",
    "    network : 네트워크\n",
    "    x_train : 트레인 데이터\n",
    "    t_train : 트레인 데이터에 대한 라벨\n",
    "    x_test : 발리데이션 데이터\n",
    "    t_test : 발리데이션 데이터에 대한 라벨\n",
    "    epochs : 에폭 수\n",
    "    mini_batch_size : 미니배치 사이즈\n",
    "    learning_rate : 학습률\n",
    "    verbose : 출력여부\n",
    "\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 learning_rate=0.01, verbose=True, layers = []):\n",
    "        self.network = network\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = int(epochs)\n",
    "        self.batch_size = int(mini_batch_size)\n",
    "        self.lr = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = int(max(self.train_size / self.batch_size, 1))\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        \"\"\"제출 전 수정\"\"\"\n",
    "        self.layers = layers # List : ex) [6, 32, 32, 6]\n",
    "        self.layer_size = len(layers)\n",
    "        self.test_acc = None\n",
    "        \n",
    "    def train_step(self):\n",
    "        # 렌덤 트레인 배치 생성\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "\n",
    "        # 네트워크 업데이트\n",
    "        self.network.update(x_batch, t_batch, True)\n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            train_acc, _ = self.accuracy(self.x_train, self.t_train)\n",
    "            test_acc, _ = self.accuracy(self.x_test, self.t_test)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            if self.verbose is False and ((self.current_epoch <= 5) or (self.current_epoch>=98 and\\\n",
    "                                          self.current_epoch<=102) or (self.current_epoch>=298 and\\\n",
    "                                          self.current_epoch<=302) or (self.current_epoch>=498 and\\\n",
    "                                          self.current_epoch<=502) or\\\n",
    "                                          ((self.current_epoch) >= (self.epochs - 5))): \n",
    "                print(\"=== epoch:\", str(round(self.current_epoch, 3)), \", iteration:\", str(round(self.current_iter, 3)),\n",
    "                \", train acc:\" + str(round(train_acc, 3)), \", test acc:\" + str(round(test_acc, 3)), \", train loss:\" + str(round(loss, 3)) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "        self.test_acc, inference_time = self.accuracy(self.x_test, self.t_test)\n",
    "        if self.verbose:\n",
    "            pass\n",
    "        else:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(self.test_acc) + \", inference_time:\" + str(inference_time))\n",
    "            print('[size = {}][epoch = {}][batch = {}][lr = {}][layer = {}]'\\\n",
    "                        .format(self.layer_size, self.epochs, self.batch_size, self.lr, self.layers))\n",
    "            print(\"\")\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(int(x.shape[0] / self.batch_size)):\n",
    "            \n",
    "            tx = x[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            tt = t[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            y = self.network.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        inference_time = (time.time() - start_time) / x.shape[0]\n",
    "\n",
    "        return acc / x.shape[0], inference_time\n",
    "\n",
    "class arg:\n",
    "    def __init__(self, epochs, mini_batch_size, learning_rate, sf):\n",
    "        self.sf = sf\n",
    "        self.epochs =  epochs\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"train.py --help 로 설명을 보시면 됩니다.\"\n",
    "#                                                  \"사용예)python train.py --sf=myparam --epochs=10\")\n",
    "#     parser.add_argument(\"--sf\", required=False, default=\"params.pkl\", help=\"save_file_name\")\n",
    "#     parser.add_argument(\"--epochs\", required=False, default=20, help=\"epochs : default=20\")\n",
    "#     parser.add_argument(\"--mini_batch_size\", required=False, default=100, help=\"mini_batch_size : default=100\")\n",
    "#     parser.add_argument(\"--learning_rate\", required=False, default=0.01, help=\"learning_rate : default=0.01\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "    # 데이터셋 탑재\n",
    "    (x_train, t_train), (x_test, t_test) = load_AReM(one_hot_label=False)\n",
    "    \n",
    "    mean = np.mean(x_train, axis = 0)\n",
    "    std = np.std(x_train, axis=0)\n",
    "    \n",
    "    \"\"\"제출전 수정\"\"\"\n",
    "    # hyperparameter\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    epochs = [500]\n",
    "    batchs = [100]\n",
    "    learningRate = [0.0005]\n",
    "#     for i in range(10):\n",
    "#         learningRate.append(10 ** np.random.uniform (-6, -2))\n",
    "    layer_unit_list = [[6, 64, 64, 6]]\n",
    "    for epoch in epochs:\n",
    "        for batch in batchs:\n",
    "            for lr in learningRate:\n",
    "                for layer_unit in layer_unit_list:\n",
    "                    sf = './Params/Adam/sig/params[si={}][ep={}][ba={}][lr={}][la={}]6.pkl'\\\n",
    "                        .format(len(layer_unit), epoch, batch, lr, layer_unit)\n",
    "                    args = arg(epoch, batch, lr, sf)\n",
    "\n",
    "                    # 모델 초기화\n",
    "                    network = Model(mean, std, False, layer_unit)\n",
    "\n",
    "                    # 트레이너 초기화\n",
    "                    trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                                      epochs=args.epochs, mini_batch_size=args.mini_batch_size,\n",
    "                                      learning_rate=args.learning_rate, verbose=False, \n",
    "                                      layers = layer_unit)\n",
    "\n",
    "                    # 트레이너를 사용해 모델 학습\n",
    "                    trainer.train()\n",
    "                    # 파라미터 보관\n",
    "                    network.save_params(args.sf) \n",
    "                    \n",
    "                    # loss와 training accuracy를 Plot\n",
    "                    x = np.arange(len(trainer.train_loss_list))\n",
    "                    plt.plot(x, trainer.train_loss_list)\n",
    "                    plt.legend([\"loss\"]) # 각주\n",
    "                    plt.title('acc: {}, layerSize: {}, epoch : {}\\nbatch : {}, lr: {} layer: {}'\\\n",
    "                              .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch, \\\n",
    "                                      round(lr, 3), layer_unit))\n",
    "                    plt.savefig('./Result/Adam/tanh/{}[si_{}]_[ep_{}]_[ba_{}]_[lr_{}]_[la_{}].png'\\\n",
    "                                .format(round(trainer.test_acc, 3), len(layer_unit), epoch, batch,\\\n",
    "                                        round(lr, 3), layer_unit), dpi=100)\n",
    "                    plt.show()\n",
    "                    plt.clf()\n",
    "                    #final sigmoid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
