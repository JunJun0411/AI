{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 loss, accuracy:  10.631825637915814 0.3333333333333333\n",
      "1 번째 loss, accuracy:  10.62377288213036 0.3333333333333333\n",
      "2 번째 loss, accuracy:  10.614766220235456 0.3333333333333333\n",
      "3 번째 loss, accuracy:  10.604661205157118 0.3333333333333333\n",
      "4 번째 loss, accuracy:  10.59328984352217 0.3333333333333333\n",
      "5 번째 loss, accuracy:  10.58045754155592 0.3333333333333333\n",
      "6 번째 loss, accuracy:  10.565940281869695 0.3333333333333333\n",
      "7 번째 loss, accuracy:  10.549482469416189 0.3333333333333333\n",
      "8 번째 loss, accuracy:  10.530796093136304 0.3333333333333333\n",
      "9 번째 loss, accuracy:  10.509562083005534 0.3333333333333333\n",
      "10 번째 loss, accuracy:  10.485434947747342 0.3333333333333333\n",
      "11 번째 loss, accuracy:  10.45805185615352 0.3333333333333333\n",
      "12 번째 loss, accuracy:  10.427047123751628 0.3333333333333333\n",
      "13 번째 loss, accuracy:  10.392072415061156 0.3333333333333333\n",
      "14 번째 loss, accuracy:  10.352821764227981 0.3333333333333333\n",
      "15 번째 loss, accuracy:  10.309058854256799 0.3333333333333333\n",
      "16 번째 loss, accuracy:  10.26064231458829 0.3333333333333333\n",
      "17 번째 loss, accuracy:  10.20754385061908 0.3333333333333333\n",
      "18 번째 loss, accuracy:  10.14985457188188 0.3333333333333333\n",
      "19 번째 loss, accuracy:  10.087777195400008 0.3333333333333333\n",
      "20 번째 loss, accuracy:  10.021605234639244 0.3333333333333333\n",
      "21 번째 loss, accuracy:  9.95169349554845 0.3333333333333333\n",
      "22 번째 loss, accuracy:  9.878425878522537 0.3333333333333333\n",
      "23 번째 loss, accuracy:  9.802186133710512 0.3333333333333333\n",
      "24 번째 loss, accuracy:  9.723335346851835 0.3333333333333333\n",
      "25 번째 loss, accuracy:  9.642197582519676 0.3333333333333333\n",
      "26 번째 loss, accuracy:  9.559053186801993 0.3333333333333333\n",
      "27 번째 loss, accuracy:  9.474138161938477 0.3333333333333333\n",
      "28 번째 loss, accuracy:  9.38764772180493 0.3333333333333333\n",
      "29 번째 loss, accuracy:  9.299742343509559 0.3333333333333333\n",
      "30 번째 loss, accuracy:  9.210555055468934 0.3333333333333333\n",
      "31 번째 loss, accuracy:  9.120199141141685 0.3333333333333333\n",
      "32 번째 loss, accuracy:  9.02877578345283 0.3333333333333333\n",
      "33 번째 loss, accuracy:  8.936381391862518 0.3333333333333333\n",
      "34 번째 loss, accuracy:  8.84311444558942 0.3333333333333333\n",
      "35 번째 loss, accuracy:  8.749081675979983 0.3333333333333333\n",
      "36 번째 loss, accuracy:  8.65440333296822 0.3333333333333333\n",
      "37 번째 loss, accuracy:  8.559217179257239 0.3333333333333333\n",
      "38 번째 loss, accuracy:  8.46368078357597 0.3333333333333333\n",
      "39 번째 loss, accuracy:  8.36797169479894 0.3333333333333333\n",
      "40 번째 loss, accuracy:  8.27228521281057 0.3333333333333333\n",
      "41 번째 loss, accuracy:  8.176829739300187 0.3333333333333333\n",
      "42 번째 loss, accuracy:  8.0818200558819 0.3333333333333333\n",
      "43 번째 loss, accuracy:  7.987469256150032 0.3333333333333333\n",
      "44 번째 loss, accuracy:  7.893980347095832 0.3333333333333333\n",
      "45 번째 loss, accuracy:  7.8015386429825035 0.3333333333333333\n",
      "46 번째 loss, accuracy:  7.710305963047184 0.3333333333333333\n",
      "47 번째 loss, accuracy:  7.6204173444515675 0.3333333333333333\n",
      "48 번째 loss, accuracy:  7.531980580856162 0.3333333333333333\n",
      "49 번째 loss, accuracy:  7.445078501276447 0.3333333333333333\n",
      "50 번째 loss, accuracy:  7.359773599287536 0.3333333333333333\n",
      "51 번째 loss, accuracy:  7.276114449539822 0.3333333333333333\n",
      "52 번째 loss, accuracy:  7.194143299442108 0.3333333333333333\n",
      "53 번째 loss, accuracy:  7.113904257901115 0.3333333333333333\n",
      "54 번째 loss, accuracy:  7.035451566346201 0.3333333333333333\n",
      "55 번째 loss, accuracy:  6.958857480454352 0.3333333333333333\n",
      "56 번째 loss, accuracy:  6.884219279752361 0.3333333333333333\n",
      "57 번째 loss, accuracy:  6.811664843562539 0.3333333333333333\n",
      "58 번째 loss, accuracy:  6.741356098295211 0.3333333333333333\n",
      "59 번째 loss, accuracy:  6.673489496572182 0.3333333333333333\n",
      "60 번째 loss, accuracy:  6.608292611510821 0.3333333333333333\n",
      "61 번째 loss, accuracy:  6.546016027849125 0.3333333333333333\n",
      "62 번째 loss, accuracy:  6.486920098469744 0.3333333333333333\n",
      "63 번째 loss, accuracy:  6.431256875465422 0.3333333333333333\n",
      "64 번째 loss, accuracy:  6.379248565245131 0.3333333333333333\n",
      "65 번째 loss, accuracy:  6.331064970741273 0.3333333333333333\n",
      "66 번째 loss, accuracy:  6.286803193127226 0.3333333333333333\n",
      "67 번째 loss, accuracy:  6.24647297508841 0.3333333333333333\n",
      "68 번째 loss, accuracy:  6.20999027057048 0.3333333333333333\n",
      "69 번째 loss, accuracy:  6.177180063142755 0.3416666666666667\n",
      "70 번째 loss, accuracy:  6.147787601530575 0.35\n",
      "71 번째 loss, accuracy:  6.121495675716805 0.36666666666666664\n",
      "72 번째 loss, accuracy:  6.097944763005074 0.38333333333333336\n",
      "73 번째 loss, accuracy:  6.07675293982989 0.38333333333333336\n",
      "74 번째 loss, accuracy:  6.057533174554741 0.38333333333333336\n",
      "75 번째 loss, accuracy:  6.039906626098032 0.39166666666666666\n",
      "76 번째 loss, accuracy:  6.02351153627748 0.4083333333333333\n",
      "77 번째 loss, accuracy:  6.008008017159466 0.43333333333333335\n",
      "78 번째 loss, accuracy:  5.9930794420339835 0.45\n",
      "79 번째 loss, accuracy:  5.978431294495517 0.4583333333333333\n",
      "80 번째 loss, accuracy:  5.963788301608083 0.4583333333333333\n",
      "81 번째 loss, accuracy:  5.9488905608120835 0.4583333333333333\n",
      "82 번째 loss, accuracy:  5.933489231323539 0.475\n",
      "83 번째 loss, accuracy:  5.917342239131841 0.4666666666666667\n",
      "84 번째 loss, accuracy:  5.90021035912078 0.5\n",
      "85 번째 loss, accuracy:  5.881853992262915 0.5083333333333333\n",
      "86 번째 loss, accuracy:  5.862030944167356 0.5\n",
      "87 번째 loss, accuracy:  5.840495519797927 0.5083333333333333\n",
      "88 번째 loss, accuracy:  5.816999257157322 0.5166666666666667\n",
      "89 번째 loss, accuracy:  5.791293602434407 0.5166666666666667\n",
      "90 번째 loss, accuracy:  5.763134747645323 0.5083333333333333\n",
      "91 번째 loss, accuracy:  5.732290677253404 0.5083333333333333\n",
      "92 번째 loss, accuracy:  5.698550184083477 0.5083333333333333\n",
      "93 번째 loss, accuracy:  5.661733229828273 0.5083333333333333\n",
      "94 번째 loss, accuracy:  5.621701602878289 0.48333333333333334\n",
      "95 번째 loss, accuracy:  5.578368479808845 0.48333333333333334\n",
      "96 번째 loss, accuracy:  5.531705369263891 0.475\n",
      "97 번째 loss, accuracy:  5.481745124793919 0.475\n",
      "98 번째 loss, accuracy:  5.428580278972375 0.4666666666666667\n",
      "99 번째 loss, accuracy:  5.372356762164704 0.4666666666666667\n",
      "100 번째 loss, accuracy:  5.313263900940904 0.4583333333333333\n",
      "101 번째 loss, accuracy:  5.251522195707656 0.4583333333333333\n",
      "102 번째 loss, accuracy:  5.187370592818815 0.4583333333333333\n",
      "103 번째 loss, accuracy:  5.121054780694796 0.4583333333333333\n",
      "104 번째 loss, accuracy:  5.052817574042803 0.45\n",
      "105 번째 loss, accuracy:  4.982891886966403 0.45\n",
      "106 번째 loss, accuracy:  4.911496294879788 0.45\n",
      "107 번째 loss, accuracy:  4.838832837965547 0.45\n",
      "108 번째 loss, accuracy:  4.765086543831264 0.44166666666666665\n",
      "109 번째 loss, accuracy:  4.690426113505128 0.44166666666666665\n",
      "110 번째 loss, accuracy:  4.615005273335088 0.44166666666666665\n",
      "111 번째 loss, accuracy:  4.538964398713378 0.44166666666666665\n",
      "112 번째 loss, accuracy:  4.46243212951246 0.44166666666666665\n",
      "113 번째 loss, accuracy:  4.385526801475239 0.44166666666666665\n",
      "114 번째 loss, accuracy:  4.308357603329491 0.44166666666666665\n",
      "115 번째 loss, accuracy:  4.231025433888473 0.44166666666666665\n",
      "116 번째 loss, accuracy:  4.153623478269419 0.44166666666666665\n",
      "117 번째 loss, accuracy:  4.076237550475937 0.44166666666666665\n",
      "118 번째 loss, accuracy:  3.9989462641389624 0.44166666666666665\n",
      "119 번째 loss, accuracy:  3.921821097200197 0.45\n",
      "120 번째 loss, accuracy:  3.8449264124985367 0.45\n",
      "121 번째 loss, accuracy:  3.768319487045491 0.45\n",
      "122 번째 loss, accuracy:  3.6920505904645133 0.45\n",
      "123 번째 loss, accuracy:  3.6161631395374414 0.45\n",
      "124 번째 loss, accuracy:  3.540693942643288 0.45\n",
      "125 번째 loss, accuracy:  3.465673536277839 0.4583333333333333\n",
      "126 번째 loss, accuracy:  3.3911266065785544 0.4583333333333333\n",
      "127 번째 loss, accuracy:  3.3170724822156434 0.4666666666666667\n",
      "128 번째 loss, accuracy:  3.243525681141001 0.4666666666666667\n",
      "129 번째 loss, accuracy:  3.1704964922533576 0.4666666666666667\n",
      "130 번째 loss, accuracy:  3.0979915735913237 0.4666666666666667\n",
      "131 번째 loss, accuracy:  3.0260145506866496 0.4666666666666667\n",
      "132 번째 loss, accuracy:  2.954566601674735 0.4666666666666667\n",
      "133 번째 loss, accuracy:  2.8836470192010513 0.4666666666666667\n",
      "134 번째 loss, accuracy:  2.8132537427014737 0.4666666666666667\n",
      "135 번째 loss, accuracy:  2.7433838579962297 0.4666666666666667\n",
      "136 번째 loss, accuracy:  2.6740340641362974 0.4666666666666667\n",
      "137 번째 loss, accuracy:  2.605201109980511 0.4666666666666667\n",
      "138 번째 loss, accuracy:  2.5368822050206727 0.4666666666666667\n",
      "139 번째 loss, accuracy:  2.469075410520403 0.4666666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 번째 loss, accuracy:  2.4017800181160367 0.4666666666666667\n",
      "141 번째 loss, accuracy:  2.3349969236803747 0.4666666666666667\n",
      "142 번째 loss, accuracy:  2.2687290045077266 0.4666666666666667\n",
      "143 번째 loss, accuracy:  2.202981507742625 0.475\n",
      "144 번째 loss, accuracy:  2.137762457442275 0.475\n",
      "145 번째 loss, accuracy:  2.073083086680684 0.48333333333333334\n",
      "146 번째 loss, accuracy:  2.008958299589938 0.48333333333333334\n",
      "147 번째 loss, accuracy:  1.9454071660703443 0.48333333333333334\n",
      "148 번째 loss, accuracy:  1.8824534489245304 0.49166666666666664\n",
      "149 번째 loss, accuracy:  1.820126159192511 0.49166666666666664\n",
      "150 번째 loss, accuracy:  1.7584601302839142 0.49166666666666664\n",
      "151 번째 loss, accuracy:  1.6974965949326726 0.5\n",
      "152 번째 loss, accuracy:  1.6372837409255667 0.5\n",
      "153 번째 loss, accuracy:  1.5778772120062377 0.5\n",
      "154 번째 loss, accuracy:  1.5193405096090247 0.5\n",
      "155 번째 loss, accuracy:  1.4617452397833663 0.5\n",
      "156 번째 loss, accuracy:  1.4051711389978272 0.5\n",
      "157 번째 loss, accuracy:  1.34970580425176 0.5083333333333333\n",
      "158 번째 loss, accuracy:  1.2954440495219268 0.5083333333333333\n",
      "159 번째 loss, accuracy:  1.2424868149972264 0.5083333333333333\n",
      "160 번째 loss, accuracy:  1.1909395709183963 0.5083333333333333\n",
      "161 번째 loss, accuracy:  1.1409101866894327 0.5083333333333333\n",
      "162 번째 loss, accuracy:  1.0925062792630864 0.5083333333333333\n",
      "163 번째 loss, accuracy:  1.0458321109481574 0.5166666666666667\n",
      "164 번째 loss, accuracy:  1.0009851704912913 0.525\n",
      "165 번째 loss, accuracy:  0.9580526335358842 0.525\n",
      "166 번째 loss, accuracy:  0.9171079475608838 0.525\n",
      "167 번째 loss, accuracy:  0.8782078099411375 0.5333333333333333\n",
      "168 번째 loss, accuracy:  0.8413897965475627 0.5333333333333333\n",
      "169 번째 loss, accuracy:  0.806670849267333 0.5333333333333333\n",
      "170 번째 loss, accuracy:  0.774046749155424 0.5333333333333333\n",
      "171 번째 loss, accuracy:  0.7434926006235812 0.5416666666666666\n",
      "172 번째 loss, accuracy:  0.7149642489123178 0.5416666666666666\n",
      "173 번째 loss, accuracy:  0.6884004659616505 0.55\n",
      "174 번째 loss, accuracy:  0.6637256820461436 0.5583333333333333\n",
      "175 번째 loss, accuracy:  0.6408530180525334 0.5916666666666667\n",
      "176 번째 loss, accuracy:  0.6196873840787167 0.6333333333333333\n",
      "177 번째 loss, accuracy:  0.6001284461737181 0.7083333333333334\n",
      "178 번째 loss, accuracy:  0.5820733137038859 0.7416666666666667\n",
      "179 번째 loss, accuracy:  0.5654188543892741 0.8\n",
      "180 번째 loss, accuracy:  0.5500635941986114 0.85\n",
      "181 번째 loss, accuracy:  0.5359091999382112 0.8833333333333333\n",
      "182 번째 loss, accuracy:  0.5228615715315815 0.925\n",
      "183 번째 loss, accuracy:  0.5108315890149939 0.9333333333333333\n",
      "184 번째 loss, accuracy:  0.49973556793295254 0.9333333333333333\n",
      "185 번째 loss, accuracy:  0.48949547844364977 0.9416666666666667\n",
      "186 번째 loss, accuracy:  0.4800389803654923 0.9416666666666667\n",
      "187 번째 loss, accuracy:  0.4712993205928944 0.9416666666666667\n",
      "188 번째 loss, accuracy:  0.4632151322910241 0.9416666666666667\n",
      "189 번째 loss, accuracy:  0.4557301680821992 0.9416666666666667\n",
      "190 번째 loss, accuracy:  0.44879299270087025 0.9416666666666667\n",
      "191 번째 loss, accuracy:  0.4423566546576444 0.9333333333333333\n",
      "192 번째 loss, accuracy:  0.436378351444068 0.9333333333333333\n",
      "193 번째 loss, accuracy:  0.4308190987261595 0.9416666666666667\n",
      "194 번째 loss, accuracy:  0.42564341073801404 0.9333333333333333\n",
      "195 번째 loss, accuracy:  0.42081899658477034 0.9416666666666667\n",
      "196 번째 loss, accuracy:  0.41631647527491095 0.9416666666666667\n",
      "197 번째 loss, accuracy:  0.41210911090836694 0.9416666666666667\n",
      "198 번째 loss, accuracy:  0.40817256844498195 0.9416666666666667\n",
      "199 번째 loss, accuracy:  0.40448468977921037 0.9416666666666667\n",
      "200 번째 loss, accuracy:  0.40102528937836035 0.9416666666666667\n",
      "201 번째 loss, accuracy:  0.39777596844490365 0.9416666666666667\n",
      "202 번째 loss, accuracy:  0.3947199463925261 0.9416666666666667\n",
      "203 번째 loss, accuracy:  0.39184190834508986 0.9416666666666667\n",
      "204 번째 loss, accuracy:  0.3891278673506695 0.9416666666666667\n",
      "205 번째 loss, accuracy:  0.386565040028965 0.9416666666666667\n",
      "206 번째 loss, accuracy:  0.38414173442480704 0.9416666666666667\n",
      "207 번째 loss, accuracy:  0.38184724891207583 0.9416666666666667\n",
      "208 번째 loss, accuracy:  0.3796717810734319 0.9416666666666667\n",
      "209 번째 loss, accuracy:  0.377606345566227 0.9416666666666667\n",
      "210 번째 loss, accuracy:  0.3756427000698984 0.9416666666666667\n",
      "211 번째 loss, accuracy:  0.3737732784927118 0.9416666666666667\n",
      "212 번째 loss, accuracy:  0.37199113069413764 0.9416666666666667\n",
      "213 번째 loss, accuracy:  0.37028986805247305 0.9416666666666667\n",
      "214 번째 loss, accuracy:  0.36866361427522515 0.9416666666666667\n",
      "215 번째 loss, accuracy:  0.3671069609118998 0.9416666666666667\n",
      "216 번째 loss, accuracy:  0.3656149270854269 0.9416666666666667\n",
      "217 번째 loss, accuracy:  0.36418292300963806 0.9416666666666667\n",
      "218 번째 loss, accuracy:  0.36280671690632327 0.9416666666666667\n",
      "219 번째 loss, accuracy:  0.36148240497677225 0.9416666666666667\n",
      "220 번째 loss, accuracy:  0.36020638411977235 0.9416666666666667\n",
      "221 번째 loss, accuracy:  0.35897532712110825 0.9416666666666667\n",
      "222 번째 loss, accuracy:  0.3577861600691171 0.9416666666666667\n",
      "223 번째 loss, accuracy:  0.3566360417771349 0.9416666666666667\n",
      "224 번째 loss, accuracy:  0.3555223450170102 0.9416666666666667\n",
      "225 번째 loss, accuracy:  0.35444263938867854 0.9416666666666667\n",
      "226 번째 loss, accuracy:  0.3533946756691992 0.9416666666666667\n",
      "227 번째 loss, accuracy:  0.3523763715011252 0.9416666666666667\n",
      "228 번째 loss, accuracy:  0.3513857982945697 0.9416666666666667\n",
      "229 번째 loss, accuracy:  0.3504211692303637 0.9416666666666667\n",
      "230 번째 loss, accuracy:  0.3494808282631568 0.9416666666666667\n",
      "231 번째 loss, accuracy:  0.34856324003356554 0.9416666666666667\n",
      "232 번째 loss, accuracy:  0.3476669806076115 0.9416666666666667\n",
      "233 번째 loss, accuracy:  0.3467907289697718 0.9416666666666667\n",
      "234 번째 loss, accuracy:  0.3459332592032151 0.9416666666666667\n",
      "235 번째 loss, accuracy:  0.3450934332972633 0.9416666666666667\n",
      "236 번째 loss, accuracy:  0.34427019452785684 0.9416666666666667\n",
      "237 번째 loss, accuracy:  0.34346256136196496 0.9416666666666667\n",
      "238 번째 loss, accuracy:  0.3426696218415136 0.9416666666666667\n",
      "239 번째 loss, accuracy:  0.3418905284064953 0.9416666666666667\n",
      "240 번째 loss, accuracy:  0.3411244931206629 0.9416666666666667\n",
      "241 번째 loss, accuracy:  0.3403707832665361 0.9416666666666667\n",
      "242 번째 loss, accuracy:  0.33962871727941996 0.9416666666666667\n",
      "243 번째 loss, accuracy:  0.33889766099286067 0.9416666666666667\n",
      "244 번째 loss, accuracy:  0.33817702417037315 0.9416666666666667\n",
      "245 번째 loss, accuracy:  0.33746625730047175 0.9416666666666667\n",
      "246 번째 loss, accuracy:  0.33676484863403217 0.9416666666666667\n",
      "247 번째 loss, accuracy:  0.3360723214447837 0.9416666666666667\n",
      "248 번째 loss, accuracy:  0.3353882314953697 0.9416666666666667\n",
      "249 번째 loss, accuracy:  0.33471216469290344 0.9416666666666667\n",
      "250 번째 loss, accuracy:  0.33404373491924194 0.9416666666666667\n",
      "251 번째 loss, accuracy:  0.3333825820224778 0.9416666666666667\n",
      "252 번째 loss, accuracy:  0.33272836995720073 0.9416666666666667\n",
      "253 번째 loss, accuracy:  0.3320807850621173 0.95\n",
      "254 번째 loss, accuracy:  0.33143953446456087 0.95\n",
      "255 번째 loss, accuracy:  0.3308043446022012 0.95\n",
      "256 번째 loss, accuracy:  0.330174959853105 0.95\n",
      "257 번째 loss, accuracy:  0.3295511412659511 0.95\n",
      "258 번째 loss, accuracy:  0.328932665382861 0.95\n",
      "259 번째 loss, accuracy:  0.3283193231479121 0.95\n",
      "260 번째 loss, accuracy:  0.3277109188949146 0.95\n",
      "261 번째 loss, accuracy:  0.32710726940854695 0.95\n",
      "262 번째 loss, accuracy:  0.3265082030534001 0.95\n",
      "263 번째 loss, accuracy:  0.32591355896589685 0.95\n",
      "264 번째 loss, accuracy:  0.3253231863044197 0.95\n",
      "265 번째 loss, accuracy:  0.3247369435533739 0.95\n",
      "266 번째 loss, accuracy:  0.32415469787720647 0.95\n",
      "267 번째 loss, accuracy:  0.32357632452070356 0.95\n",
      "268 번째 loss, accuracy:  0.3230017062521912 0.95\n",
      "269 번째 loss, accuracy:  0.32243073284648216 0.95\n",
      "270 번째 loss, accuracy:  0.32186330060468393 0.95\n",
      "271 번째 loss, accuracy:  0.32129931190816996 0.95\n",
      "272 번째 loss, accuracy:  0.3207386748042334 0.95\n",
      "273 번째 loss, accuracy:  0.32018130262112715 0.95\n",
      "274 번째 loss, accuracy:  0.31962711361035256 0.95\n",
      "275 번째 loss, accuracy:  0.31907603061422685 0.95\n",
      "276 번째 loss, accuracy:  0.3185279807569112 0.95\n",
      "277 번째 loss, accuracy:  0.3179828951571911 0.95\n",
      "278 번째 loss, accuracy:  0.3174407086614655 0.95\n",
      "279 번째 loss, accuracy:  0.3169013595954625 0.95\n",
      "280 번째 loss, accuracy:  0.31636478953337155 0.95\n",
      "281 번째 loss, accuracy:  0.31583094308310833 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282 번째 loss, accuracy:  0.3152997676865861 0.95\n",
      "283 번째 loss, accuracy:  0.31477121343391073 0.95\n",
      "284 번째 loss, accuracy:  0.314245232890497 0.95\n",
      "285 번째 loss, accuracy:  0.3137217809362026 0.95\n",
      "286 번째 loss, accuracy:  0.31320081461561033 0.95\n",
      "287 번째 loss, accuracy:  0.31268229299867895 0.95\n",
      "288 번째 loss, accuracy:  0.3121661770510116 0.95\n",
      "289 번째 loss, accuracy:  0.31165242951308325 0.95\n",
      "290 번째 loss, accuracy:  0.31114101478776696 0.95\n",
      "291 번째 loss, accuracy:  0.310631898835598 0.95\n",
      "292 번째 loss, accuracy:  0.3101250490772099 0.95\n",
      "293 번째 loss, accuracy:  0.30962043430245284 0.95\n",
      "294 번째 loss, accuracy:  0.3091180245857154 0.95\n",
      "295 번째 loss, accuracy:  0.30861779120701965 0.95\n",
      "296 번째 loss, accuracy:  0.30811970657847954 0.95\n",
      "297 번째 loss, accuracy:  0.30762374417575544 0.95\n",
      "298 번째 loss, accuracy:  0.3071298784741476 0.95\n",
      "299 번째 loss, accuracy:  0.3066380848890068 0.95\n",
      "300 번째 loss, accuracy:  0.30614833972016786 0.95\n",
      "301 번째 loss, accuracy:  0.30566062010012135 0.95\n",
      "302 번째 loss, accuracy:  0.3051749039456604 0.95\n",
      "303 번째 loss, accuracy:  0.30469116991276773 0.95\n",
      "304 번째 loss, accuracy:  0.3042093973545201 0.95\n",
      "305 번째 loss, accuracy:  0.30372956628179154 0.95\n",
      "306 번째 loss, accuracy:  0.30325165732657766 0.95\n",
      "307 번째 loss, accuracy:  0.30277565170774007 0.95\n",
      "308 번째 loss, accuracy:  0.30230153119903164 0.95\n",
      "309 번째 loss, accuracy:  0.3018292780992173 0.95\n",
      "310 번째 loss, accuracy:  0.3013588752041699 0.95\n",
      "311 번째 loss, accuracy:  0.3008903057807897 0.9583333333333334\n",
      "312 번째 loss, accuracy:  0.3004235535426334 0.9583333333333334\n",
      "313 번째 loss, accuracy:  0.299958602627133 0.9583333333333334\n",
      "314 번째 loss, accuracy:  0.2994954375742905 0.9583333333333334\n",
      "315 번째 loss, accuracy:  0.2990340433067589 0.9583333333333334\n",
      "316 번째 loss, accuracy:  0.2985744051112073 0.9583333333333334\n",
      "317 번째 loss, accuracy:  0.2981165086208878 0.9583333333333334\n",
      "318 번째 loss, accuracy:  0.29766033979931145 0.9583333333333334\n",
      "319 번째 loss, accuracy:  0.29720588492497957 0.9583333333333334\n",
      "320 번째 loss, accuracy:  0.2967531305770817 0.9583333333333334\n",
      "321 번째 loss, accuracy:  0.2963020636220991 0.9583333333333334\n",
      "322 번째 loss, accuracy:  0.29585267120125297 0.9583333333333334\n",
      "323 번째 loss, accuracy:  0.29540494071874845 0.9583333333333334\n",
      "324 번째 loss, accuracy:  0.2949588598307455 0.9583333333333334\n",
      "325 번째 loss, accuracy:  0.2945144164350268 0.9583333333333334\n",
      "326 번째 loss, accuracy:  0.29407159866129684 0.9583333333333334\n",
      "327 번째 loss, accuracy:  0.2936303948620835 0.9583333333333334\n",
      "328 번째 loss, accuracy:  0.2931907936042002 0.9583333333333334\n",
      "329 번째 loss, accuracy:  0.29275278366072366 0.9583333333333334\n",
      "330 번째 loss, accuracy:  0.29231635400346057 0.9583333333333334\n",
      "331 번째 loss, accuracy:  0.29188149379587086 0.9583333333333334\n",
      "332 번째 loss, accuracy:  0.29144819238640896 0.9583333333333334\n",
      "333 번째 loss, accuracy:  0.29101643930226456 0.9583333333333334\n",
      "334 번째 loss, accuracy:  0.2905862242434722 0.9583333333333334\n",
      "335 번째 loss, accuracy:  0.2901575370773654 0.9583333333333334\n",
      "336 번째 loss, accuracy:  0.28973036783335504 0.9666666666666667\n",
      "337 번째 loss, accuracy:  0.28930470669800373 0.9666666666666667\n",
      "338 번째 loss, accuracy:  0.2888805440103862 0.9666666666666667\n",
      "339 번째 loss, accuracy:  0.2884578702577085 0.9666666666666667\n",
      "340 번째 loss, accuracy:  0.2880366760711749 0.9666666666666667\n",
      "341 번째 loss, accuracy:  0.2876169522220828 0.9666666666666667\n",
      "342 번째 loss, accuracy:  0.2871986896181301 0.9666666666666667\n",
      "343 번째 loss, accuracy:  0.2867818792999255 0.9666666666666667\n",
      "344 번째 loss, accuracy:  0.28636651243768146 0.9666666666666667\n",
      "345 번째 loss, accuracy:  0.28595258032808396 0.9666666666666667\n",
      "346 번째 loss, accuracy:  0.28554007439132467 0.9666666666666667\n",
      "347 번째 loss, accuracy:  0.28512898616828525 0.9666666666666667\n",
      "348 번째 loss, accuracy:  0.2847193073178624 0.9666666666666667\n",
      "349 번째 loss, accuracy:  0.28431102961442783 0.9666666666666667\n",
      "350 번째 loss, accuracy:  0.2839041449454109 0.9666666666666667\n",
      "351 번째 loss, accuracy:  0.2834986453089962 0.9666666666666667\n",
      "352 번째 loss, accuracy:  0.2830945228119296 0.9666666666666667\n",
      "353 번째 loss, accuracy:  0.2826917696674268 0.9666666666666667\n",
      "354 번째 loss, accuracy:  0.2822903781931744 0.9666666666666667\n",
      "355 번째 loss, accuracy:  0.28189034080942077 0.9666666666666667\n",
      "356 번째 loss, accuracy:  0.2814916500371468 0.9666666666666667\n",
      "357 번째 loss, accuracy:  0.2810942984963136 0.9666666666666667\n",
      "358 번째 loss, accuracy:  0.2806982789041843 0.9666666666666667\n",
      "359 번째 loss, accuracy:  0.28030358407371003 0.9666666666666667\n",
      "360 번째 loss, accuracy:  0.2799102069119806 0.9666666666666667\n",
      "361 번째 loss, accuracy:  0.279518140418731 0.9666666666666667\n",
      "362 번째 loss, accuracy:  0.2791273776849021 0.9666666666666667\n",
      "363 번째 loss, accuracy:  0.27873791189125624 0.9666666666666667\n",
      "364 번째 loss, accuracy:  0.2783497363070346 0.9666666666666667\n",
      "365 번째 loss, accuracy:  0.27796284428865997 0.9666666666666667\n",
      "366 번째 loss, accuracy:  0.2775772292784863 0.9666666666666667\n",
      "367 번째 loss, accuracy:  0.27719288480357623 0.9666666666666667\n",
      "368 번째 loss, accuracy:  0.27680980447452325 0.9666666666666667\n",
      "369 번째 loss, accuracy:  0.2764279819843022 0.9666666666666667\n",
      "370 번째 loss, accuracy:  0.2760474111071537 0.9666666666666667\n",
      "371 번째 loss, accuracy:  0.2756680856974943 0.9666666666666667\n",
      "372 번째 loss, accuracy:  0.2752899996888563 0.9666666666666667\n",
      "373 번째 loss, accuracy:  0.27491314709285014 0.9666666666666667\n",
      "374 번째 loss, accuracy:  0.2745375219981524 0.9666666666666667\n",
      "375 번째 loss, accuracy:  0.27416311856951275 0.9666666666666667\n",
      "376 번째 loss, accuracy:  0.2737899310467836 0.9666666666666667\n",
      "377 번째 loss, accuracy:  0.27341795374396843 0.9666666666666667\n",
      "378 번째 loss, accuracy:  0.2730471810482842 0.9666666666666667\n",
      "379 번째 loss, accuracy:  0.27267760741924174 0.9666666666666667\n",
      "380 번째 loss, accuracy:  0.2723092273877444 0.9666666666666667\n",
      "381 번째 loss, accuracy:  0.27194203555519664 0.9666666666666667\n",
      "382 번째 loss, accuracy:  0.27157602659262364 0.9666666666666667\n",
      "383 번째 loss, accuracy:  0.27121119523981135 0.9666666666666667\n",
      "384 번째 loss, accuracy:  0.27084753630444836 0.9666666666666667\n",
      "385 번째 loss, accuracy:  0.2704850446612828 0.9666666666666667\n",
      "386 번째 loss, accuracy:  0.2701237152512901 0.9666666666666667\n",
      "387 번째 loss, accuracy:  0.2697635430808464 0.9666666666666667\n",
      "388 번째 loss, accuracy:  0.26940452322091013 0.9666666666666667\n",
      "389 번째 loss, accuracy:  0.2690466508062164 0.9666666666666667\n",
      "390 번째 loss, accuracy:  0.26868992103447165 0.9666666666666667\n",
      "391 번째 loss, accuracy:  0.26833432916556027 0.9666666666666667\n",
      "392 번째 loss, accuracy:  0.2679798705207561 0.9666666666666667\n",
      "393 번째 loss, accuracy:  0.26762654048193574 0.9666666666666667\n",
      "394 번째 loss, accuracy:  0.2672743344908045 0.9666666666666667\n",
      "395 번째 loss, accuracy:  0.26692324804812034 0.9666666666666667\n",
      "396 번째 loss, accuracy:  0.26657327671292735 0.9666666666666667\n",
      "397 번째 loss, accuracy:  0.26622441610179204 0.9666666666666667\n",
      "398 번째 loss, accuracy:  0.2658766618880412 0.9666666666666667\n",
      "399 번째 loss, accuracy:  0.2655300098010075 0.9666666666666667\n",
      "400 번째 loss, accuracy:  0.2651844556252775 0.9666666666666667\n",
      "401 번째 loss, accuracy:  0.2648399951999418 0.9666666666666667\n",
      "402 번째 loss, accuracy:  0.26449662441784977 0.9666666666666667\n",
      "403 번째 loss, accuracy:  0.26415433922486686 0.9666666666666667\n",
      "404 번째 loss, accuracy:  0.2638131356191333 0.9666666666666667\n",
      "405 번째 loss, accuracy:  0.263473009650329 0.9666666666666667\n",
      "406 번째 loss, accuracy:  0.2631339574189387 0.9666666666666667\n",
      "407 번째 loss, accuracy:  0.2627959750755196 0.9666666666666667\n",
      "408 번째 loss, accuracy:  0.26245905881997206 0.9666666666666667\n",
      "409 번째 loss, accuracy:  0.26212320490081426 0.9666666666666667\n",
      "410 번째 loss, accuracy:  0.261788409614455 0.9666666666666667\n",
      "411 번째 loss, accuracy:  0.26145466930447375 0.9666666666666667\n",
      "412 번째 loss, accuracy:  0.2611219803608992 0.9666666666666667\n",
      "413 번째 loss, accuracy:  0.26079033921949285 0.9666666666666667\n",
      "414 번째 loss, accuracy:  0.26045974236103214 0.9666666666666667\n",
      "415 번째 loss, accuracy:  0.2601301863105988 0.9666666666666667\n",
      "416 번째 loss, accuracy:  0.2598016676368669 0.9666666666666667\n",
      "417 번째 loss, accuracy:  0.2594741829513932 0.9666666666666667\n",
      "418 번째 loss, accuracy:  0.25914772890791243 0.9666666666666667\n",
      "419 번째 loss, accuracy:  0.25882230220163266 0.9666666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 번째 loss, accuracy:  0.25849789956853225 0.9666666666666667\n",
      "421 번째 loss, accuracy:  0.2581745177846615 0.9666666666666667\n",
      "422 번째 loss, accuracy:  0.257852153665445 0.9666666666666667\n",
      "423 번째 loss, accuracy:  0.25753080406498724 0.9666666666666667\n",
      "424 번째 loss, accuracy:  0.25721046587538 0.9666666666666667\n",
      "425 번째 loss, accuracy:  0.2568911360260134 0.9666666666666667\n",
      "426 번째 loss, accuracy:  0.2565728114828882 0.9666666666666667\n",
      "427 번째 loss, accuracy:  0.2562554892479314 0.9666666666666667\n",
      "428 번째 loss, accuracy:  0.255939166358316 0.9666666666666667\n",
      "429 번째 loss, accuracy:  0.2556238398857805 0.9666666666666667\n",
      "430 번째 loss, accuracy:  0.2553095069359542 0.9666666666666667\n",
      "431 번째 loss, accuracy:  0.2549961646476842 0.9666666666666667\n",
      "432 번째 loss, accuracy:  0.2546838101923645 0.9666666666666667\n",
      "433 번째 loss, accuracy:  0.25437244077327226 0.9666666666666667\n",
      "434 번째 loss, accuracy:  0.25406205362489953 0.9666666666666667\n",
      "435 번째 loss, accuracy:  0.2537526460122988 0.9666666666666667\n",
      "436 번째 loss, accuracy:  0.2534442152304223 0.9666666666666667\n",
      "437 번째 loss, accuracy:  0.2531367586034701 0.9666666666666667\n",
      "438 번째 loss, accuracy:  0.25283027348424125 0.9666666666666667\n",
      "439 번째 loss, accuracy:  0.25252475725348583 0.9666666666666667\n",
      "440 번째 loss, accuracy:  0.25222020731926464 0.9666666666666667\n",
      "441 번째 loss, accuracy:  0.2519166211163109 0.9666666666666667\n",
      "442 번째 loss, accuracy:  0.25161399610539464 0.9666666666666667\n",
      "443 번째 loss, accuracy:  0.25131232977269274 0.9666666666666667\n",
      "444 번째 loss, accuracy:  0.25101161962916324 0.9666666666666667\n",
      "445 번째 loss, accuracy:  0.25071186320992345 0.9666666666666667\n",
      "446 번째 loss, accuracy:  0.25041305807363157 0.9666666666666667\n",
      "447 번째 loss, accuracy:  0.2501152018018751 0.9666666666666667\n",
      "448 번째 loss, accuracy:  0.2498182919985594 0.9666666666666667\n",
      "449 번째 loss, accuracy:  0.24952232628930707 0.9666666666666667\n",
      "450 번째 loss, accuracy:  0.2492273023208564 0.9666666666666667\n",
      "451 번째 loss, accuracy:  0.24893321776046604 0.9666666666666667\n",
      "452 번째 loss, accuracy:  0.24864007029532806 0.9666666666666667\n",
      "453 번째 loss, accuracy:  0.248347857631981 0.9666666666666667\n",
      "454 번째 loss, accuracy:  0.24805657749573143 0.9666666666666667\n",
      "455 번째 loss, accuracy:  0.24776622763007936 0.9666666666666667\n",
      "456 번째 loss, accuracy:  0.24747680579614778 0.9666666666666667\n",
      "457 번째 loss, accuracy:  0.24718830977212083 0.9666666666666667\n",
      "458 번째 loss, accuracy:  0.24690073735268317 0.9666666666666667\n",
      "459 번째 loss, accuracy:  0.24661408634846863 0.9666666666666667\n",
      "460 번째 loss, accuracy:  0.24632835458551175 0.9666666666666667\n",
      "461 번째 loss, accuracy:  0.2460435399047072 0.9666666666666667\n",
      "462 번째 loss, accuracy:  0.2457596401612717 0.9666666666666667\n",
      "463 번째 loss, accuracy:  0.24547665322421738 0.9666666666666667\n",
      "464 번째 loss, accuracy:  0.24519457697582378 0.9666666666666667\n",
      "465 번째 loss, accuracy:  0.24491340931112096 0.9666666666666667\n",
      "466 번째 loss, accuracy:  0.24463314813737824 0.9666666666666667\n",
      "467 번째 loss, accuracy:  0.2443537913735966 0.9666666666666667\n",
      "468 번째 loss, accuracy:  0.24407533695000996 0.9666666666666667\n",
      "469 번째 loss, accuracy:  0.2437977828075892 0.9666666666666667\n",
      "470 번째 loss, accuracy:  0.24352112689755845 0.9666666666666667\n",
      "471 번째 loss, accuracy:  0.24324536718091114 0.9666666666666667\n",
      "472 번째 loss, accuracy:  0.24297050162793538 0.9666666666666667\n",
      "473 번째 loss, accuracy:  0.24269652821774834 0.9666666666666667\n",
      "474 번째 loss, accuracy:  0.24242344493783227 0.9666666666666667\n",
      "475 번째 loss, accuracy:  0.2421512497835799 0.9666666666666667\n",
      "476 번째 loss, accuracy:  0.241879940757849 0.9666666666666667\n",
      "477 번째 loss, accuracy:  0.2416095158705169 0.9666666666666667\n",
      "478 번째 loss, accuracy:  0.24133997313804792 0.9666666666666667\n",
      "479 번째 loss, accuracy:  0.24107131058306502 0.9666666666666667\n",
      "480 번째 loss, accuracy:  0.2408035262339295 0.9666666666666667\n",
      "481 번째 loss, accuracy:  0.24053661812432484 0.9666666666666667\n",
      "482 번째 loss, accuracy:  0.2402705842928527 0.9666666666666667\n",
      "483 번째 loss, accuracy:  0.24000542278262937 0.9666666666666667\n",
      "484 번째 loss, accuracy:  0.2397411316408939 0.9666666666666667\n",
      "485 번째 loss, accuracy:  0.23947770891862216 0.9666666666666667\n",
      "486 번째 loss, accuracy:  0.239215152670148 0.9666666666666667\n",
      "487 번째 loss, accuracy:  0.23895346095279008 0.9666666666666667\n",
      "488 번째 loss, accuracy:  0.23869263182648773 0.9666666666666667\n",
      "489 번째 loss, accuracy:  0.23843266335344318 0.9666666666666667\n",
      "490 번째 loss, accuracy:  0.23817355359777118 0.9666666666666667\n",
      "491 번째 loss, accuracy:  0.2379153006251548 0.9666666666666667\n",
      "492 번째 loss, accuracy:  0.23765790250250962 0.9666666666666667\n",
      "493 번째 loss, accuracy:  0.2374013572976554 0.9666666666666667\n",
      "494 번째 loss, accuracy:  0.2371456630789938 0.9666666666666667\n",
      "495 번째 loss, accuracy:  0.23689081791519334 0.9666666666666667\n",
      "496 번째 loss, accuracy:  0.23663681987488333 0.9666666666666667\n",
      "497 번째 loss, accuracy:  0.23638366702635233 0.9666666666666667\n",
      "498 번째 loss, accuracy:  0.23613135743725733 0.9666666666666667\n",
      "499 번째 loss, accuracy:  0.23587988917433644 0.9666666666666667\n",
      "500 번째 loss, accuracy:  0.23562926030313122 0.9666666666666667\n",
      "501 번째 loss, accuracy:  0.23537946888771605 0.9666666666666667\n",
      "502 번째 loss, accuracy:  0.23513051299043333 0.9666666666666667\n",
      "503 번째 loss, accuracy:  0.23488239067163724 0.9666666666666667\n",
      "504 번째 loss, accuracy:  0.23463509998944473 0.9666666666666667\n",
      "505 번째 loss, accuracy:  0.23438863899949305 0.9666666666666667\n",
      "506 번째 loss, accuracy:  0.2341430057547041 0.9666666666666667\n",
      "507 번째 loss, accuracy:  0.23389819830505645 0.9666666666666667\n",
      "508 번째 loss, accuracy:  0.2336542146973658 0.9666666666666667\n",
      "509 번째 loss, accuracy:  0.23341105297506962 0.9666666666666667\n",
      "510 번째 loss, accuracy:  0.23316871117802068 0.9666666666666667\n",
      "511 번째 loss, accuracy:  0.23292718734228923 0.9666666666666667\n",
      "512 번째 loss, accuracy:  0.23268647949996796 0.9666666666666667\n",
      "513 번째 loss, accuracy:  0.23244658567898638 0.9666666666666667\n",
      "514 번째 loss, accuracy:  0.23220750390293468 0.9666666666666667\n",
      "515 번째 loss, accuracy:  0.23196923219088864 0.9666666666666667\n",
      "516 번째 loss, accuracy:  0.2317317685572461 0.9666666666666667\n",
      "517 번째 loss, accuracy:  0.23149511101156955 0.9666666666666667\n",
      "518 번째 loss, accuracy:  0.23125925755843366 0.9666666666666667\n",
      "519 번째 loss, accuracy:  0.23102420619727967 0.9666666666666667\n",
      "520 번째 loss, accuracy:  0.23078995492227902 0.9666666666666667\n",
      "521 번째 loss, accuracy:  0.23055650172220096 0.9666666666666667\n",
      "522 번째 loss, accuracy:  0.23032384458028723 0.9666666666666667\n",
      "523 번째 loss, accuracy:  0.23009198147413543 0.9666666666666667\n",
      "524 번째 loss, accuracy:  0.2298609103755855 0.9666666666666667\n",
      "525 번째 loss, accuracy:  0.22963062925061392 0.9666666666666667\n",
      "526 번째 loss, accuracy:  0.22940113605923632 0.9666666666666667\n",
      "527 번째 loss, accuracy:  0.2291724287554122 0.9666666666666667\n",
      "528 번째 loss, accuracy:  0.22894450528695967 0.9666666666666667\n",
      "529 번째 loss, accuracy:  0.22871736359547482 0.9666666666666667\n",
      "530 번째 loss, accuracy:  0.22849100161625746 0.9666666666666667\n",
      "531 번째 loss, accuracy:  0.22826541727824104 0.9666666666666667\n",
      "532 번째 loss, accuracy:  0.22804060850393207 0.9666666666666667\n",
      "533 번째 loss, accuracy:  0.22781657320935172 0.9666666666666667\n",
      "534 번째 loss, accuracy:  0.22759330930398594 0.9666666666666667\n",
      "535 번째 loss, accuracy:  0.22737081469074047 0.9666666666666667\n",
      "536 번째 loss, accuracy:  0.22714908726590124 0.9666666666666667\n",
      "537 번째 loss, accuracy:  0.2269281249190983 0.9666666666666667\n",
      "538 번째 loss, accuracy:  0.2267079255332811 0.9666666666666667\n",
      "539 번째 loss, accuracy:  0.22648848698469262 0.9666666666666667\n",
      "540 번째 loss, accuracy:  0.2262698071428531 0.9666666666666667\n",
      "541 번째 loss, accuracy:  0.22605188387054595 0.9666666666666667\n",
      "542 번째 loss, accuracy:  0.22583471502381305 0.9666666666666667\n",
      "543 번째 loss, accuracy:  0.22561829845194917 0.9666666666666667\n",
      "544 번째 loss, accuracy:  0.22540263199750896 0.9666666666666667\n",
      "545 번째 loss, accuracy:  0.22518771349631125 0.9666666666666667\n",
      "546 번째 loss, accuracy:  0.22497354077745246 0.9666666666666667\n",
      "547 번째 loss, accuracy:  0.22476011166332538 0.9666666666666667\n",
      "548 번째 loss, accuracy:  0.22454742396963906 0.9666666666666667\n",
      "549 번째 loss, accuracy:  0.2243354755054471 0.9666666666666667\n",
      "550 번째 loss, accuracy:  0.2241242640731791 0.9666666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551 번째 loss, accuracy:  0.22391378746867294 0.9666666666666667\n",
      "552 번째 loss, accuracy:  0.22370404348121814 0.9666666666666667\n",
      "553 번째 loss, accuracy:  0.22349502989359868 0.9666666666666667\n",
      "554 번째 loss, accuracy:  0.2232867444821414 0.9666666666666667\n",
      "555 번째 loss, accuracy:  0.22307918501676693 0.9666666666666667\n",
      "556 번째 loss, accuracy:  0.222872349261047 0.9666666666666667\n",
      "557 번째 loss, accuracy:  0.22266623497226437 0.9666666666666667\n",
      "558 번째 loss, accuracy:  0.2224608399014771 0.9666666666666667\n",
      "559 번째 loss, accuracy:  0.222256161793585 0.9666666666666667\n",
      "560 번째 loss, accuracy:  0.2220521983874015 0.9666666666666667\n",
      "561 번째 loss, accuracy:  0.22184894741572717 0.9666666666666667\n",
      "562 번째 loss, accuracy:  0.22164640660542953 0.975\n",
      "563 번째 loss, accuracy:  0.2214445736775224 0.975\n",
      "564 번째 loss, accuracy:  0.2212434463472519 0.975\n",
      "565 번째 loss, accuracy:  0.22104302232418427 0.975\n",
      "566 번째 loss, accuracy:  0.22084329931229407 0.975\n",
      "567 번째 loss, accuracy:  0.22064427501006198 0.975\n",
      "568 번째 loss, accuracy:  0.22044594711056759 0.975\n",
      "569 번째 loss, accuracy:  0.2202483133015907 0.975\n",
      "570 번째 loss, accuracy:  0.22005137126571309 0.975\n",
      "571 번째 loss, accuracy:  0.21985511868042268 0.975\n",
      "572 번째 loss, accuracy:  0.21965955321822145 0.975\n",
      "573 번째 loss, accuracy:  0.21946467254673419 0.975\n",
      "574 번째 loss, accuracy:  0.2192704743288198 0.975\n",
      "575 번째 loss, accuracy:  0.2190769562226869 0.975\n",
      "576 번째 loss, accuracy:  0.21888411588201054 0.975\n",
      "577 번째 loss, accuracy:  0.2186919509560475 0.975\n",
      "578 번째 loss, accuracy:  0.21850045908976015 0.975\n",
      "579 번째 loss, accuracy:  0.21830963792393648 0.975\n",
      "580 번째 loss, accuracy:  0.21811948509531476 0.975\n",
      "581 번째 loss, accuracy:  0.21792999823670958 0.975\n",
      "582 번째 loss, accuracy:  0.21774117497713877 0.975\n",
      "583 번째 loss, accuracy:  0.21755301294195337 0.975\n",
      "584 번째 loss, accuracy:  0.21736550975296667 0.975\n",
      "585 번째 loss, accuracy:  0.21717866302858754 0.975\n",
      "586 번째 loss, accuracy:  0.2169924703839521 0.975\n",
      "587 번째 loss, accuracy:  0.21680692943106 0.975\n",
      "588 번째 loss, accuracy:  0.21662203777890887 0.975\n",
      "589 번째 loss, accuracy:  0.21643779303363125 0.975\n",
      "590 번째 loss, accuracy:  0.2162541927986319 0.975\n",
      "591 번째 loss, accuracy:  0.21607123467472836 0.975\n",
      "592 번째 loss, accuracy:  0.21588891626028728 0.975\n",
      "593 번째 loss, accuracy:  0.2157072351513682 0.975\n",
      "594 번째 loss, accuracy:  0.21552618894186215 0.975\n",
      "595 번째 loss, accuracy:  0.21534577522363427 0.975\n",
      "596 번째 loss, accuracy:  0.2151659915866664 0.975\n",
      "597 번째 loss, accuracy:  0.21498683561920018 0.975\n",
      "598 번째 loss, accuracy:  0.2148083049078781 0.975\n",
      "599 번째 loss, accuracy:  0.21463039703789047 0.975\n",
      "600 번째 loss, accuracy:  0.21445310959311625 0.975\n",
      "601 번째 loss, accuracy:  0.21427644015626954 0.975\n",
      "602 번째 loss, accuracy:  0.2141003863090427 0.975\n",
      "603 번째 loss, accuracy:  0.21392494563225162 0.975\n",
      "604 번째 loss, accuracy:  0.21375011570597907 0.975\n",
      "605 번째 loss, accuracy:  0.2135758941097209 0.975\n",
      "606 번째 loss, accuracy:  0.2134022784225297 0.975\n",
      "607 번째 loss, accuracy:  0.21322926622315824 0.975\n",
      "608 번째 loss, accuracy:  0.21305685509020625 0.975\n",
      "609 번째 loss, accuracy:  0.21288504260226163 0.975\n",
      "610 번째 loss, accuracy:  0.2127138263380462 0.975\n",
      "611 번째 loss, accuracy:  0.2125432038765575 0.975\n",
      "612 번째 loss, accuracy:  0.21237317279721363 0.975\n",
      "613 번째 loss, accuracy:  0.21220373067999374 0.9833333333333333\n",
      "614 번째 loss, accuracy:  0.21203487510558208 0.9833333333333333\n",
      "615 번째 loss, accuracy:  0.2118666036555081 0.9833333333333333\n",
      "616 번째 loss, accuracy:  0.2116989139122884 0.9833333333333333\n",
      "617 번째 loss, accuracy:  0.21153180345956682 0.9833333333333333\n",
      "618 번째 loss, accuracy:  0.21136526988225293 0.9833333333333333\n",
      "619 번째 loss, accuracy:  0.21119931076666335 0.9833333333333333\n",
      "620 번째 loss, accuracy:  0.21103392370065854 0.9833333333333333\n",
      "621 번째 loss, accuracy:  0.2108691062737798 0.9833333333333333\n",
      "622 번째 loss, accuracy:  0.21070485607738695 0.9833333333333333\n",
      "623 번째 loss, accuracy:  0.21054117070479356 0.9833333333333333\n",
      "624 번째 loss, accuracy:  0.21037804775140237 0.9833333333333333\n",
      "625 번째 loss, accuracy:  0.21021548481483712 0.9833333333333333\n",
      "626 번째 loss, accuracy:  0.21005347949507872 0.9833333333333333\n",
      "627 번째 loss, accuracy:  0.20989202939459478 0.9833333333333333\n",
      "628 번째 loss, accuracy:  0.20973113211847086 0.9833333333333333\n",
      "629 번째 loss, accuracy:  0.20957078527454104 0.9833333333333333\n",
      "630 번째 loss, accuracy:  0.20941098647351541 0.9833333333333333\n",
      "631 번째 loss, accuracy:  0.20925173332910868 0.9833333333333333\n",
      "632 번째 loss, accuracy:  0.2090930234581662 0.9833333333333333\n",
      "633 번째 loss, accuracy:  0.20893485448078838 0.9833333333333333\n",
      "634 번째 loss, accuracy:  0.2087772240204559 0.9833333333333333\n",
      "635 번째 loss, accuracy:  0.20862012970415117 0.9833333333333333\n",
      "636 번째 loss, accuracy:  0.20846356916248154 0.9833333333333333\n",
      "637 번째 loss, accuracy:  0.20830754002979737 0.9833333333333333\n",
      "638 번째 loss, accuracy:  0.2081520399443118 0.9833333333333333\n",
      "639 번째 loss, accuracy:  0.2079970665482192 0.9833333333333333\n",
      "640 번째 loss, accuracy:  0.20784261748780952 0.9833333333333333\n",
      "641 번째 loss, accuracy:  0.20768869041358495 0.9833333333333333\n",
      "642 번째 loss, accuracy:  0.2075352829803712 0.9833333333333333\n",
      "643 번째 loss, accuracy:  0.20738239284743157 0.9833333333333333\n",
      "644 번째 loss, accuracy:  0.20723001767857652 0.9833333333333333\n",
      "645 번째 loss, accuracy:  0.20707815514227262 0.9833333333333333\n",
      "646 번째 loss, accuracy:  0.20692680291175067 0.9833333333333333\n",
      "647 번째 loss, accuracy:  0.20677595866511236 0.9833333333333333\n",
      "648 번째 loss, accuracy:  0.2066256200854331 0.9833333333333333\n",
      "649 번째 loss, accuracy:  0.20647578486086784 0.9833333333333333\n",
      "650 번째 loss, accuracy:  0.20632645068475158 0.9833333333333333\n",
      "651 번째 loss, accuracy:  0.20617761525569886 0.9833333333333333\n",
      "652 번째 loss, accuracy:  0.20602927627770432 0.9833333333333333\n",
      "653 번째 loss, accuracy:  0.20588143146023843 0.9833333333333333\n",
      "654 번째 loss, accuracy:  0.20573407851834336 0.9833333333333333\n",
      "655 번째 loss, accuracy:  0.2055872151727281 0.9833333333333333\n",
      "656 번째 loss, accuracy:  0.2054408391498601 0.9833333333333333\n",
      "657 번째 loss, accuracy:  0.2052949481820571 0.9833333333333333\n",
      "658 번째 loss, accuracy:  0.20514954000757485 0.9833333333333333\n",
      "659 번째 loss, accuracy:  0.20500461237069736 0.9833333333333333\n",
      "660 번째 loss, accuracy:  0.204860163021822 0.9833333333333333\n",
      "661 번째 loss, accuracy:  0.2047161897175447 0.9833333333333333\n",
      "662 번째 loss, accuracy:  0.2045726902207429 0.9833333333333333\n",
      "663 번째 loss, accuracy:  0.2044296623006578 0.9833333333333333\n",
      "664 번째 loss, accuracy:  0.20428710373297332 0.9833333333333333\n",
      "665 번째 loss, accuracy:  0.20414501229989604 0.9833333333333333\n",
      "666 번째 loss, accuracy:  0.2040033857902316 0.9833333333333333\n",
      "667 번째 loss, accuracy:  0.20386222199946016 0.9833333333333333\n",
      "668 번째 loss, accuracy:  0.2037215187298101 0.9833333333333333\n",
      "669 번째 loss, accuracy:  0.20358127379033034 0.9833333333333333\n",
      "670 번째 loss, accuracy:  0.20344148499696152 0.9833333333333333\n",
      "671 번째 loss, accuracy:  0.2033021501726046 0.9833333333333333\n",
      "672 번째 loss, accuracy:  0.20316326714718916 0.9833333333333333\n",
      "673 번째 loss, accuracy:  0.20302483375773947 0.9833333333333333\n",
      "674 번째 loss, accuracy:  0.2028868478484385 0.9833333333333333\n",
      "675 번째 loss, accuracy:  0.20274930727069138 0.9833333333333333\n",
      "676 번째 loss, accuracy:  0.20261220988318687 0.9833333333333333\n",
      "677 번째 loss, accuracy:  0.2024755535519575 0.9833333333333333\n",
      "678 번째 loss, accuracy:  0.2023393361504387 0.9833333333333333\n",
      "679 번째 loss, accuracy:  0.20220355555952338 0.9833333333333333\n",
      "680 번째 loss, accuracy:  0.20206820966762037 0.9833333333333333\n",
      "681 번째 loss, accuracy:  0.20193329637070787 0.9833333333333333\n",
      "682 번째 loss, accuracy:  0.2017988135723839 0.9833333333333333\n",
      "683 번째 loss, accuracy:  0.20166475918391955 0.9833333333333333\n",
      "684 번째 loss, accuracy:  0.20153113112430826 0.9833333333333333\n",
      "685 번째 loss, accuracy:  0.20139792732031328 0.9833333333333333\n",
      "686 번째 loss, accuracy:  0.20126514570651524 0.9833333333333333\n",
      "687 번째 loss, accuracy:  0.20113278422535671 0.9833333333333333\n",
      "688 번째 loss, accuracy:  0.20100084082718622 0.9833333333333333\n",
      "689 번째 loss, accuracy:  0.2008693134703016 0.9833333333333333\n",
      "690 번째 loss, accuracy:  0.2007382001209899 0.9833333333333333\n",
      "691 번째 loss, accuracy:  0.20060749875356768 0.9833333333333333\n",
      "692 번째 loss, accuracy:  0.2004772073504196 0.9833333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693 번째 loss, accuracy:  0.2003473239020345 0.9833333333333333\n",
      "694 번째 loss, accuracy:  0.20021784640704254 0.9833333333333333\n",
      "695 번째 loss, accuracy:  0.20008877287224733 0.9833333333333333\n",
      "696 번째 loss, accuracy:  0.19996010131266123 0.9833333333333333\n",
      "697 번째 loss, accuracy:  0.19983182975153546 0.9833333333333333\n",
      "698 번째 loss, accuracy:  0.19970395622039075 0.9833333333333333\n",
      "699 번째 loss, accuracy:  0.19957647875904688 0.9833333333333333\n",
      "700 번째 loss, accuracy:  0.19944939541565004 0.9833333333333333\n",
      "701 번째 loss, accuracy:  0.19932270424669898 0.9833333333333333\n",
      "702 번째 loss, accuracy:  0.1991964033170719 0.9833333333333333\n",
      "703 번째 loss, accuracy:  0.19907049070004862 0.9833333333333333\n",
      "704 번째 loss, accuracy:  0.19894496447733448 0.9833333333333333\n",
      "705 번째 loss, accuracy:  0.19881982273908186 0.9833333333333333\n",
      "706 번째 loss, accuracy:  0.19869506358391048 0.9833333333333333\n",
      "707 번째 loss, accuracy:  0.1985706851189267 0.9833333333333333\n",
      "708 번째 loss, accuracy:  0.1984466854597416 0.9833333333333333\n",
      "709 번째 loss, accuracy:  0.1983230627304886 0.9833333333333333\n",
      "710 번째 loss, accuracy:  0.19819981506383808 0.9833333333333333\n",
      "711 번째 loss, accuracy:  0.1980769406010126 0.9833333333333333\n",
      "712 번째 loss, accuracy:  0.1979544374918013 0.9833333333333333\n",
      "713 번째 loss, accuracy:  0.19783230389457174 0.9833333333333333\n",
      "714 번째 loss, accuracy:  0.1977105379762812 0.9833333333333333\n",
      "715 번째 loss, accuracy:  0.19758913791248756 0.9833333333333333\n",
      "716 번째 loss, accuracy:  0.19746810188735794 0.9833333333333333\n",
      "717 번째 loss, accuracy:  0.19734742809367797 0.9833333333333333\n",
      "718 번째 loss, accuracy:  0.19722711473285803 0.9833333333333333\n",
      "719 번째 loss, accuracy:  0.1971071600149416 0.9833333333333333\n",
      "720 번째 loss, accuracy:  0.19698756215860724 0.9833333333333333\n",
      "721 번째 loss, accuracy:  0.19686831939117588 0.9833333333333333\n",
      "722 번째 loss, accuracy:  0.19674942994861302 0.9833333333333333\n",
      "723 번째 loss, accuracy:  0.19663089207553094 0.9833333333333333\n",
      "724 번째 loss, accuracy:  0.19651270402519055 0.9833333333333333\n",
      "725 번째 loss, accuracy:  0.19639486405950266 0.9833333333333333\n",
      "726 번째 loss, accuracy:  0.1962773704490262 0.9833333333333333\n",
      "727 번째 loss, accuracy:  0.19616022147296816 0.9833333333333333\n",
      "728 번째 loss, accuracy:  0.19604341541918185 0.9833333333333333\n",
      "729 번째 loss, accuracy:  0.19592695058416204 0.9833333333333333\n",
      "730 번째 loss, accuracy:  0.19581082527304383 0.9833333333333333\n",
      "731 번째 loss, accuracy:  0.19569503779959596 0.9833333333333333\n",
      "732 번째 loss, accuracy:  0.19557958648621626 0.9833333333333333\n",
      "733 번째 loss, accuracy:  0.19546446966392594 0.9833333333333333\n",
      "734 번째 loss, accuracy:  0.19534968567236172 0.9833333333333333\n",
      "735 번째 loss, accuracy:  0.19523523285976807 0.9833333333333333\n",
      "736 번째 loss, accuracy:  0.19512110958298948 0.9833333333333333\n",
      "737 번째 loss, accuracy:  0.19500731420746112 0.9833333333333333\n",
      "738 번째 loss, accuracy:  0.1948938451071974 0.9833333333333333\n",
      "739 번째 loss, accuracy:  0.19478070066478334 0.9833333333333333\n",
      "740 번째 loss, accuracy:  0.19466787927136217 0.9833333333333333\n",
      "741 번째 loss, accuracy:  0.19455537932662256 0.9833333333333333\n",
      "742 번째 loss, accuracy:  0.1944431992387873 0.9833333333333333\n",
      "743 번째 loss, accuracy:  0.19433133742459818 0.9833333333333333\n",
      "744 번째 loss, accuracy:  0.1942197923093039 0.9833333333333333\n",
      "745 번째 loss, accuracy:  0.1941085623266437 0.9833333333333333\n",
      "746 번째 loss, accuracy:  0.19399764591883237 0.9833333333333333\n",
      "747 번째 loss, accuracy:  0.19388704153654462 0.9833333333333333\n",
      "748 번째 loss, accuracy:  0.1937767476388989 0.9833333333333333\n",
      "749 번째 loss, accuracy:  0.19366676269343883 0.9833333333333333\n",
      "750 번째 loss, accuracy:  0.1935570851761174 0.9833333333333333\n",
      "751 번째 loss, accuracy:  0.1934477135712767 0.9833333333333333\n",
      "752 번째 loss, accuracy:  0.19333864637163115 0.975\n",
      "753 번째 loss, accuracy:  0.19322988207824573 0.975\n",
      "754 번째 loss, accuracy:  0.19312141920051829 0.975\n",
      "755 번째 loss, accuracy:  0.19301325625615826 0.975\n",
      "756 번째 loss, accuracy:  0.19290539177116567 0.975\n",
      "757 번째 loss, accuracy:  0.19279782427980965 0.975\n",
      "758 번째 loss, accuracy:  0.19269055232460636 0.975\n",
      "759 번째 loss, accuracy:  0.1925835744562981 0.975\n",
      "760 번째 loss, accuracy:  0.1924768892338284 0.975\n",
      "761 번째 loss, accuracy:  0.1923704952243203 0.975\n",
      "762 번째 loss, accuracy:  0.19226439100305234 0.975\n",
      "763 번째 loss, accuracy:  0.19215857515343415 0.975\n",
      "764 번째 loss, accuracy:  0.19205304626698194 0.975\n",
      "765 번째 loss, accuracy:  0.19194780294329383 0.975\n",
      "766 번째 loss, accuracy:  0.19184284379002464 0.975\n",
      "767 번째 loss, accuracy:  0.19173816742285993 0.975\n",
      "768 번째 loss, accuracy:  0.19163377246549013 0.975\n",
      "769 번째 loss, accuracy:  0.19152965754958415 0.975\n",
      "770 번째 loss, accuracy:  0.19142582131476266 0.975\n",
      "771 번째 loss, accuracy:  0.19132226240857042 0.975\n",
      "772 번째 loss, accuracy:  0.19121897948644992 0.975\n",
      "773 번째 loss, accuracy:  0.1911159712117126 0.975\n",
      "774 번째 loss, accuracy:  0.19101323625551114 0.975\n",
      "775 번째 loss, accuracy:  0.19091077329681194 0.975\n",
      "776 번째 loss, accuracy:  0.19080858102236486 0.975\n",
      "777 번째 loss, accuracy:  0.19070665812667567 0.975\n",
      "778 번째 loss, accuracy:  0.1906050033119761 0.975\n",
      "779 번째 loss, accuracy:  0.19050361528819473 0.975\n",
      "780 번째 loss, accuracy:  0.19040249277292678 0.975\n",
      "781 번째 loss, accuracy:  0.19030163449140533 0.975\n",
      "782 번째 loss, accuracy:  0.190201039176469 0.975\n",
      "783 번째 loss, accuracy:  0.19010070556853365 0.975\n",
      "784 번째 loss, accuracy:  0.1900006324155603 0.975\n",
      "785 번째 loss, accuracy:  0.18990081847302526 0.975\n",
      "786 번째 loss, accuracy:  0.189801262503888 0.975\n",
      "787 번째 loss, accuracy:  0.18970196327856068 0.975\n",
      "788 번째 loss, accuracy:  0.18960291957487638 0.975\n",
      "789 번째 loss, accuracy:  0.18950413017805665 0.975\n",
      "790 번째 loss, accuracy:  0.1894055938806816 0.975\n",
      "791 번째 loss, accuracy:  0.18930730948265595 0.975\n",
      "792 번째 loss, accuracy:  0.18920927579117777 0.975\n",
      "793 번째 loss, accuracy:  0.18911149162070592 0.975\n",
      "794 번째 loss, accuracy:  0.18901395579292773 0.975\n",
      "795 번째 loss, accuracy:  0.18891666713672634 0.975\n",
      "796 번째 loss, accuracy:  0.18881962448814762 0.975\n",
      "797 번째 loss, accuracy:  0.18872282669036794 0.975\n",
      "798 번째 loss, accuracy:  0.18862627259366052 0.975\n",
      "799 번째 loss, accuracy:  0.1885299610553622 0.975\n",
      "800 번째 loss, accuracy:  0.1884338909398412 0.975\n",
      "801 번째 loss, accuracy:  0.18833806111846302 0.975\n",
      "802 번째 loss, accuracy:  0.18824247046955694 0.975\n",
      "803 번째 loss, accuracy:  0.18814711787838317 0.975\n",
      "804 번째 loss, accuracy:  0.18805200223709798 0.975\n",
      "805 번째 loss, accuracy:  0.18795712244472168 0.975\n",
      "806 번째 loss, accuracy:  0.18786247740710355 0.975\n",
      "807 번째 loss, accuracy:  0.18776806603688895 0.975\n",
      "808 번째 loss, accuracy:  0.18767388725348447 0.975\n",
      "809 번째 loss, accuracy:  0.18757993998302472 0.975\n",
      "810 번째 loss, accuracy:  0.1874862231583381 0.975\n",
      "811 번째 loss, accuracy:  0.18739273571891285 0.975\n",
      "812 번째 loss, accuracy:  0.18729947661086285 0.975\n",
      "813 번째 loss, accuracy:  0.18720644478689405 0.975\n",
      "814 번째 loss, accuracy:  0.18711363920626897 0.975\n",
      "815 번째 loss, accuracy:  0.18702105883477416 0.975\n",
      "816 번째 loss, accuracy:  0.18692870264468464 0.975\n",
      "817 번째 loss, accuracy:  0.18683656961473105 0.975\n",
      "818 번째 loss, accuracy:  0.18674465873006413 0.975\n",
      "819 번째 loss, accuracy:  0.18665296898222117 0.975\n",
      "820 번째 loss, accuracy:  0.18656149936909155 0.975\n",
      "821 번째 loss, accuracy:  0.18647024889488256 0.975\n",
      "822 번째 loss, accuracy:  0.1863792165700854 0.975\n",
      "823 번째 loss, accuracy:  0.18628840141144062 0.975\n",
      "824 번째 loss, accuracy:  0.1861978024419041 0.975\n",
      "825 번째 loss, accuracy:  0.18610741869061267 0.975\n",
      "826 번째 loss, accuracy:  0.18601724919285018 0.975\n",
      "827 번째 loss, accuracy:  0.18592729299001293 0.975\n",
      "828 번째 loss, accuracy:  0.1858375491295763 0.975\n",
      "829 번째 loss, accuracy:  0.18574801666505933 0.975\n",
      "830 번째 loss, accuracy:  0.18565869465599263 0.975\n",
      "831 번째 loss, accuracy:  0.18556958216788205 0.975\n",
      "832 번째 loss, accuracy:  0.18548067827217665 0.975\n",
      "833 번째 loss, accuracy:  0.18539198204623364 0.975\n",
      "834 번째 loss, accuracy:  0.18530349257328457 0.975\n",
      "835 번째 loss, accuracy:  0.18521520894240182 0.975\n",
      "836 번째 loss, accuracy:  0.18512713024846528 0.975\n",
      "837 번째 loss, accuracy:  0.18503925559212686 0.975\n",
      "838 번째 loss, accuracy:  0.1849515840797789 0.975\n",
      "839 번째 loss, accuracy:  0.18486411482351897 0.975\n",
      "840 번째 loss, accuracy:  0.1847768469411171 0.975\n",
      "841 번째 loss, accuracy:  0.1846897795559825 0.975\n",
      "842 번째 loss, accuracy:  0.18460291179712923 0.975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843 번째 loss, accuracy:  0.18451624279914355 0.975\n",
      "844 번째 loss, accuracy:  0.18442977170215039 0.975\n",
      "845 번째 loss, accuracy:  0.18434349765178026 0.975\n",
      "846 번째 loss, accuracy:  0.18425741979913599 0.975\n",
      "847 번째 loss, accuracy:  0.18417153730075983 0.975\n",
      "848 번째 loss, accuracy:  0.18408584931860042 0.975\n",
      "849 번째 loss, accuracy:  0.1840003550199794 0.975\n",
      "850 번째 loss, accuracy:  0.18391505357756008 0.975\n",
      "851 번째 loss, accuracy:  0.18382994416931303 0.975\n",
      "852 번째 loss, accuracy:  0.1837450259784846 0.975\n",
      "853 번째 loss, accuracy:  0.18366029819356366 0.975\n",
      "854 번째 loss, accuracy:  0.1835757600082499 0.975\n",
      "855 번째 loss, accuracy:  0.18349141062142085 0.975\n",
      "856 번째 loss, accuracy:  0.1834072492371 0.975\n",
      "857 번째 loss, accuracy:  0.1833232750644246 0.975\n",
      "858 번째 loss, accuracy:  0.18323948731761366 0.975\n",
      "859 번째 loss, accuracy:  0.18315588521593615 0.975\n",
      "860 번째 loss, accuracy:  0.18307246798367896 0.975\n",
      "861 번째 loss, accuracy:  0.18298923485011506 0.975\n",
      "862 번째 loss, accuracy:  0.1829061850494726 0.975\n",
      "863 번째 loss, accuracy:  0.1828233178209021 0.975\n",
      "864 번째 loss, accuracy:  0.1827406324084464 0.975\n",
      "865 번째 loss, accuracy:  0.1826581280610089 0.975\n",
      "866 번째 loss, accuracy:  0.1825758040323221 0.975\n",
      "867 번째 loss, accuracy:  0.18249365958091684 0.975\n",
      "868 번째 loss, accuracy:  0.18241169397009163 0.975\n",
      "869 번째 loss, accuracy:  0.18232990646788108 0.975\n",
      "870 번째 loss, accuracy:  0.18224829634702608 0.975\n",
      "871 번째 loss, accuracy:  0.18216686288494258 0.975\n",
      "872 번째 loss, accuracy:  0.1820856053636912 0.975\n",
      "873 번째 loss, accuracy:  0.18200452306994733 0.975\n",
      "874 번째 loss, accuracy:  0.1819236152949703 0.975\n",
      "875 번째 loss, accuracy:  0.18184288133457432 0.975\n",
      "876 번째 loss, accuracy:  0.18176232048909668 0.975\n",
      "877 번째 loss, accuracy:  0.18168193206337008 0.975\n",
      "878 번째 loss, accuracy:  0.18160171536669095 0.975\n",
      "879 번째 loss, accuracy:  0.1815216697127912 0.975\n",
      "880 번째 loss, accuracy:  0.18144179441980846 0.975\n",
      "881 번째 loss, accuracy:  0.18136208881025614 0.975\n",
      "882 번째 loss, accuracy:  0.1812825522109949 0.975\n",
      "883 번째 loss, accuracy:  0.18120318395320348 0.975\n",
      "884 번째 loss, accuracy:  0.18112398337234883 0.975\n",
      "885 번째 loss, accuracy:  0.18104494980815883 0.975\n",
      "886 번째 loss, accuracy:  0.18096608260459254 0.975\n",
      "887 번째 loss, accuracy:  0.18088738110981145 0.975\n",
      "888 번째 loss, accuracy:  0.18080884467615185 0.975\n",
      "889 번째 loss, accuracy:  0.18073047266009604 0.975\n",
      "890 번째 loss, accuracy:  0.18065226442224452 0.975\n",
      "891 번째 loss, accuracy:  0.18057421932728712 0.975\n",
      "892 번째 loss, accuracy:  0.1804963367439761 0.975\n",
      "893 번째 loss, accuracy:  0.18041861604509804 0.975\n",
      "894 번째 loss, accuracy:  0.18034105660744562 0.975\n",
      "895 번째 loss, accuracy:  0.18026365781179068 0.975\n",
      "896 번째 loss, accuracy:  0.1801864190428568 0.975\n",
      "897 번째 loss, accuracy:  0.1801093396892922 0.975\n",
      "898 번째 loss, accuracy:  0.18003241914364232 0.975\n",
      "899 번째 loss, accuracy:  0.179955656802323 0.975\n",
      "900 번째 loss, accuracy:  0.17987905206559343 0.975\n",
      "901 번째 loss, accuracy:  0.1798026043375299 0.975\n",
      "902 번째 loss, accuracy:  0.17972631302599887 0.975\n",
      "903 번째 loss, accuracy:  0.17965017754263027 0.975\n",
      "904 번째 loss, accuracy:  0.17957419730279228 0.975\n",
      "905 번째 loss, accuracy:  0.1794983717255637 0.975\n",
      "906 번째 loss, accuracy:  0.17942270023370951 0.975\n",
      "907 번째 loss, accuracy:  0.17934718225365362 0.975\n",
      "908 번째 loss, accuracy:  0.17927181721545385 0.975\n",
      "909 번째 loss, accuracy:  0.17919660455277608 0.975\n",
      "910 번째 loss, accuracy:  0.1791215437028691 0.975\n",
      "911 번째 loss, accuracy:  0.17904663410653826 0.975\n",
      "912 번째 loss, accuracy:  0.17897187520812205 0.975\n",
      "913 번째 loss, accuracy:  0.17889726645546514 0.975\n",
      "914 번째 loss, accuracy:  0.1788228072998944 0.975\n",
      "915 번째 loss, accuracy:  0.17874849719619448 0.975\n",
      "916 번째 loss, accuracy:  0.17867433560258203 0.975\n",
      "917 번째 loss, accuracy:  0.17860032198068257 0.975\n",
      "918 번째 loss, accuracy:  0.17852645579550427 0.975\n",
      "919 번째 loss, accuracy:  0.17845273651541582 0.975\n",
      "920 번째 loss, accuracy:  0.1783791636121204 0.975\n",
      "921 번째 loss, accuracy:  0.17830573656063284 0.975\n",
      "922 번째 loss, accuracy:  0.17823245483925554 0.975\n",
      "923 번째 loss, accuracy:  0.17815931792955433 0.975\n",
      "924 번째 loss, accuracy:  0.17808632531633595 0.975\n",
      "925 번째 loss, accuracy:  0.1780134764876227 0.975\n",
      "926 번째 loss, accuracy:  0.17794077093463134 0.975\n",
      "927 번째 loss, accuracy:  0.17786820815174806 0.975\n",
      "928 번째 loss, accuracy:  0.177795787636507 0.975\n",
      "929 번째 loss, accuracy:  0.17772350888956603 0.975\n",
      "930 번째 loss, accuracy:  0.17765137141468473 0.975\n",
      "931 번째 loss, accuracy:  0.177579374718701 0.975\n",
      "932 번째 loss, accuracy:  0.17750751831150974 0.975\n",
      "933 번째 loss, accuracy:  0.1774358017060393 0.975\n",
      "934 번째 loss, accuracy:  0.17736422441822997 0.975\n",
      "935 번째 loss, accuracy:  0.17729278596701115 0.975\n",
      "936 번째 loss, accuracy:  0.1772214858742802 0.975\n",
      "937 번째 loss, accuracy:  0.1771503236648801 0.975\n",
      "938 번째 loss, accuracy:  0.17707929886657794 0.975\n",
      "939 번째 loss, accuracy:  0.17700841101004255 0.975\n",
      "940 번째 loss, accuracy:  0.17693765962882424 0.975\n",
      "941 번째 loss, accuracy:  0.176867044259333 0.975\n",
      "942 번째 loss, accuracy:  0.17679656444081626 0.975\n",
      "943 번째 loss, accuracy:  0.17672621971533953 0.975\n",
      "944 번째 loss, accuracy:  0.1766560096277642 0.975\n",
      "945 번째 loss, accuracy:  0.17658593372572726 0.975\n",
      "946 번째 loss, accuracy:  0.17651599155962053 0.975\n",
      "947 번째 loss, accuracy:  0.17644618268257017 0.975\n",
      "948 번째 loss, accuracy:  0.17637650665041565 0.975\n",
      "949 번째 loss, accuracy:  0.17630696302169013 0.975\n",
      "950 번째 loss, accuracy:  0.17623755135760028 0.975\n",
      "951 번째 loss, accuracy:  0.17616827122200554 0.975\n",
      "952 번째 loss, accuracy:  0.1760991221813989 0.975\n",
      "953 번째 loss, accuracy:  0.1760301038048863 0.975\n",
      "954 번째 loss, accuracy:  0.17596121566416795 0.975\n",
      "955 번째 loss, accuracy:  0.17589245733351794 0.975\n",
      "956 번째 loss, accuracy:  0.17582382838976532 0.975\n",
      "957 번째 loss, accuracy:  0.1757553284122739 0.975\n",
      "958 번째 loss, accuracy:  0.1756869569829247 0.975\n",
      "959 번째 loss, accuracy:  0.17561871368609505 0.975\n",
      "960 번째 loss, accuracy:  0.17555059810864088 0.975\n",
      "961 번째 loss, accuracy:  0.1754826098398768 0.975\n",
      "962 번째 loss, accuracy:  0.17541474847155905 0.975\n",
      "963 번째 loss, accuracy:  0.1753470135978653 0.975\n",
      "964 번째 loss, accuracy:  0.17527940481537693 0.975\n",
      "965 번째 loss, accuracy:  0.17521192172306024 0.975\n",
      "966 번째 loss, accuracy:  0.17514456392224945 0.975\n",
      "967 번째 loss, accuracy:  0.17507733101662634 0.975\n",
      "968 번째 loss, accuracy:  0.1750102226122048 0.975\n",
      "969 번째 loss, accuracy:  0.17494323831731037 0.975\n",
      "970 번째 loss, accuracy:  0.17487637774256523 0.975\n",
      "971 번째 loss, accuracy:  0.17480964050086822 0.975\n",
      "972 번째 loss, accuracy:  0.1747430262073784 0.975\n",
      "973 번째 loss, accuracy:  0.17467653447949788 0.975\n",
      "974 번째 loss, accuracy:  0.1746101649368542 0.975\n",
      "975 번째 loss, accuracy:  0.17454391720128196 0.975\n",
      "976 번째 loss, accuracy:  0.17447779089680815 0.975\n",
      "977 번째 loss, accuracy:  0.17441178564963328 0.975\n",
      "978 번째 loss, accuracy:  0.17434590108811518 0.975\n",
      "979 번째 loss, accuracy:  0.17428013684275254 0.975\n",
      "980 번째 loss, accuracy:  0.17421449254616814 0.975\n",
      "981 번째 loss, accuracy:  0.17414896783309153 0.975\n",
      "982 번째 loss, accuracy:  0.17408356234034397 0.975\n",
      "983 번째 loss, accuracy:  0.1740182757068211 0.975\n",
      "984 번째 loss, accuracy:  0.1739531075734778 0.975\n",
      "985 번째 loss, accuracy:  0.1738880575833107 0.975\n",
      "986 번째 loss, accuracy:  0.1738231253813434 0.975\n",
      "987 번째 loss, accuracy:  0.1737583106146109 0.975\n",
      "988 번째 loss, accuracy:  0.17369361293214217 0.975\n",
      "989 번째 loss, accuracy:  0.17362903198494611 0.975\n",
      "990 번째 loss, accuracy:  0.17356456742599563 0.975\n",
      "991 번째 loss, accuracy:  0.17350021891021206 0.975\n",
      "992 번째 loss, accuracy:  0.17343598609444957 0.975\n",
      "993 번째 loss, accuracy:  0.17337186863748097 0.975\n",
      "994 번째 loss, accuracy:  0.17330786619998112 0.975\n",
      "995 번째 loss, accuracy:  0.1732439784445133 0.975\n",
      "996 번째 loss, accuracy:  0.17318020503551382 0.975\n",
      "997 번째 loss, accuracy:  0.17311654563927714 0.975\n",
      "998 번째 loss, accuracy:  0.17305299992394102 0.975\n",
      "999 번째 loss, accuracy:  0.17298956755947253 0.975\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from tlnn2 import TwoLayerNeuralNetwork2\n",
    "\n",
    "\"\"\" iris Data \"\"\"\n",
    "iris = load_iris()\n",
    "X = iris.data # iris data input\n",
    "y = iris.target # iris target (label)\n",
    "\n",
    "# 데이터 Split Use training : testing = 8 : 2 => 120 : 30\n",
    "suffle = np.random.choice(X.shape[0], X.shape[0], replace=False)\n",
    "for_train = suffle[:120]\n",
    "for_test = suffle[120:]\n",
    "\n",
    "# for training data (X, y)\n",
    "X_train = X[for_train]\n",
    "y_train = y[for_train]\n",
    "# for testing data (X, y)\n",
    "X_test = X[for_test]\n",
    "y_test = y[for_test]\n",
    "\n",
    "\"\"\" hidden layer의 Unit 수 = 5 \"\"\"\n",
    "input_size = 4\n",
    "hidden_size = 100\n",
    "output_size = 3\n",
    "tn2 = TwoLayerNeuralNetwork2(input_size, hidden_size, output_size)\n",
    "tn2.init_data(X_train, y_train)\n",
    "\n",
    "\"\"\" hyperParameter 값 \"\"\"\n",
    "lr = 0.005\n",
    "epoch = 1000\n",
    "batch_size = 120\n",
    "check = True # loss와 accuracy의 추이와 Plt 확인\n",
    "\n",
    "lossPlt = []\n",
    "accPlt = []\n",
    "lossPlt, accPlt = tn2.learn(lr, epoch, batch_size, check) # epoch번 반복한 loss와 accuracy의 List를 return \n",
    "    \n",
    "lo = round(tn2.loss(X_train, y_train), 5)\n",
    "Tr = round(tn2.accuracy(X_train, y_train), 5)\n",
    "Te = round(tn2.accuracy(X_test, y_test), 5)\n",
    "\n",
    "# loss와 training accuracy를 Plot\n",
    "x = np.arange(epoch)\n",
    "plt.plot(x, lossPlt, x, accPlt)\n",
    "plt.legend([\"loss\", \"training accuracy\"]) # 각주\n",
    "plt.title('hiddenSize: {}, lr: {}, epoch: {}, batchSize: {}\\n loss: {}, Train_Acc: {}, Test_Acc: {}'\\\n",
    "          .format(hidden_size, lr, epoch, batch_size, lo, Tr, Te))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 loss, accuracy:  1.8540750157596197 0.35833333333333334\n",
      "1 번째 loss, accuracy:  1.7861612680547974 0.35833333333333334\n",
      "2 번째 loss, accuracy:  1.722607853932252 0.35833333333333334\n",
      "3 번째 loss, accuracy:  1.6633601225028423 0.35833333333333334\n",
      "4 번째 loss, accuracy:  1.6083959497697284 0.35833333333333334\n",
      "5 번째 loss, accuracy:  1.557783637575757 0.35833333333333334\n",
      "6 번째 loss, accuracy:  1.511250055743723 0.35833333333333334\n",
      "7 번째 loss, accuracy:  1.4688775457239347 0.35833333333333334\n",
      "8 번째 loss, accuracy:  1.4302994771878554 0.35833333333333334\n",
      "9 번째 loss, accuracy:  1.3952198626829952 0.35833333333333334\n",
      "10 번째 loss, accuracy:  1.3635717254164423 0.35833333333333334\n",
      "11 번째 loss, accuracy:  1.3351466269080434 0.35833333333333334\n",
      "12 번째 loss, accuracy:  1.309408477286657 0.35833333333333334\n",
      "13 번째 loss, accuracy:  1.2863942802751371 0.35833333333333334\n",
      "14 번째 loss, accuracy:  1.2656739132397663 0.35833333333333334\n",
      "15 번째 loss, accuracy:  1.247127316419874 0.35833333333333334\n",
      "16 번째 loss, accuracy:  1.2305505936013823 0.35833333333333334\n",
      "17 번째 loss, accuracy:  1.2157044261284562 0.35833333333333334\n",
      "18 번째 loss, accuracy:  1.2023621661024368 0.35833333333333334\n",
      "19 번째 loss, accuracy:  1.1904436796273496 0.35833333333333334\n",
      "20 번째 loss, accuracy:  1.1798468058633205 0.35833333333333334\n",
      "21 번째 loss, accuracy:  1.1703472415964236 0.35833333333333334\n",
      "22 번째 loss, accuracy:  1.1618123562838263 0.35833333333333334\n",
      "23 번째 loss, accuracy:  1.1542012975687934 0.35833333333333334\n",
      "24 번째 loss, accuracy:  1.1474521317743216 0.35833333333333334\n",
      "25 번째 loss, accuracy:  1.1413790933897758 0.35833333333333334\n",
      "26 번째 loss, accuracy:  1.1359794478055771 0.35833333333333334\n",
      "27 번째 loss, accuracy:  1.1311757497941413 0.35833333333333334\n",
      "28 번째 loss, accuracy:  1.1268726074073385 0.35833333333333334\n",
      "29 번째 loss, accuracy:  1.123023608396473 0.35833333333333334\n",
      "30 번째 loss, accuracy:  1.1196233985396877 0.35833333333333334\n",
      "31 번째 loss, accuracy:  1.116568830353199 0.35833333333333334\n",
      "32 번째 loss, accuracy:  1.1138211270733867 0.35833333333333334\n",
      "33 번째 loss, accuracy:  1.1113920692904633 0.35833333333333334\n",
      "34 번째 loss, accuracy:  1.1091948655586503 0.35833333333333334\n",
      "35 번째 loss, accuracy:  1.1072639142688634 0.35833333333333334\n",
      "36 번째 loss, accuracy:  1.1055685479462032 0.35833333333333334\n",
      "37 번째 loss, accuracy:  1.1040552214265407 0.35833333333333334\n",
      "38 번째 loss, accuracy:  1.1026579626806652 0.35833333333333334\n",
      "39 번째 loss, accuracy:  1.1014281400964265 0.35833333333333334\n",
      "40 번째 loss, accuracy:  1.1003468028866517 0.35833333333333334\n",
      "41 번째 loss, accuracy:  1.0993748215518557 0.35833333333333334\n",
      "42 번째 loss, accuracy:  1.0985138758778596 0.35833333333333334\n",
      "43 번째 loss, accuracy:  1.0977356426781635 0.35833333333333334\n",
      "44 번째 loss, accuracy:  1.0970388743213642 0.35833333333333334\n",
      "45 번째 loss, accuracy:  1.0964114131445235 0.35833333333333334\n",
      "46 번째 loss, accuracy:  1.0958444680017005 0.35833333333333334\n",
      "47 번째 loss, accuracy:  1.0953384765309768 0.35833333333333334\n",
      "48 번째 loss, accuracy:  1.094890094981289 0.35833333333333334\n",
      "49 번째 loss, accuracy:  1.0944735097597593 0.35833333333333334\n",
      "50 번째 loss, accuracy:  1.0940966078931804 0.35833333333333334\n",
      "51 번째 loss, accuracy:  1.093767220467143 0.35833333333333334\n",
      "52 번째 loss, accuracy:  1.0934667546271983 0.35833333333333334\n",
      "53 번째 loss, accuracy:  1.0931892352769372 0.35833333333333334\n",
      "54 번째 loss, accuracy:  1.09293122710359 0.35833333333333334\n",
      "55 번째 loss, accuracy:  1.0926980217859386 0.35833333333333334\n",
      "56 번째 loss, accuracy:  1.0924871684410355 0.35833333333333334\n",
      "57 번째 loss, accuracy:  1.0922913438813069 0.35833333333333334\n",
      "58 번째 loss, accuracy:  1.0921094646137044 0.35833333333333334\n",
      "59 번째 loss, accuracy:  1.091944890794901 0.35833333333333334\n",
      "60 번째 loss, accuracy:  1.0917886172149045 0.35833333333333334\n",
      "61 번째 loss, accuracy:  1.091646983296575 0.35833333333333334\n",
      "62 번째 loss, accuracy:  1.0915142081676348 0.35833333333333334\n",
      "63 번째 loss, accuracy:  1.091380343209997 0.35833333333333334\n",
      "64 번째 loss, accuracy:  1.0912547099116163 0.35833333333333334\n",
      "65 번째 loss, accuracy:  1.0911352058388233 0.35833333333333334\n",
      "66 번째 loss, accuracy:  1.091015287794584 0.35833333333333334\n",
      "67 번째 loss, accuracy:  1.090902610903955 0.35833333333333334\n",
      "68 번째 loss, accuracy:  1.0907930462444952 0.35833333333333334\n",
      "69 번째 loss, accuracy:  1.0906841388127166 0.35833333333333334\n",
      "70 번째 loss, accuracy:  1.0905793298647164 0.35833333333333334\n",
      "71 번째 loss, accuracy:  1.0904739593522215 0.35833333333333334\n",
      "72 번째 loss, accuracy:  1.0903718754350058 0.35833333333333334\n",
      "73 번째 loss, accuracy:  1.0902657738038468 0.35833333333333334\n",
      "74 번째 loss, accuracy:  1.090163787798465 0.35833333333333334\n",
      "75 번째 loss, accuracy:  1.0900628923101432 0.35833333333333334\n",
      "76 번째 loss, accuracy:  1.089958002429519 0.35833333333333334\n",
      "77 번째 loss, accuracy:  1.0898525480227643 0.35833333333333334\n",
      "78 번째 loss, accuracy:  1.089746721777407 0.35833333333333334\n",
      "79 번째 loss, accuracy:  1.0896393403591902 0.35833333333333334\n",
      "80 번째 loss, accuracy:  1.089529688817136 0.35833333333333334\n",
      "81 번째 loss, accuracy:  1.0894196298089696 0.35833333333333334\n",
      "82 번째 loss, accuracy:  1.089307053155972 0.35833333333333334\n",
      "83 번째 loss, accuracy:  1.0891941201797275 0.35833333333333334\n",
      "84 번째 loss, accuracy:  1.0890790223743423 0.35833333333333334\n",
      "85 번째 loss, accuracy:  1.0889619573750162 0.35833333333333334\n",
      "86 번째 loss, accuracy:  1.0888416278537985 0.35833333333333334\n",
      "87 번째 loss, accuracy:  1.088718915149838 0.35833333333333334\n",
      "88 번째 loss, accuracy:  1.0885931980655146 0.35833333333333334\n",
      "89 번째 loss, accuracy:  1.0884639565842313 0.35833333333333334\n",
      "90 번째 loss, accuracy:  1.08833108490497 0.35833333333333334\n",
      "91 번째 loss, accuracy:  1.0881957674034501 0.35833333333333334\n",
      "92 번째 loss, accuracy:  1.0880557574134277 0.35833333333333334\n",
      "93 번째 loss, accuracy:  1.087912617885854 0.35833333333333334\n",
      "94 번째 loss, accuracy:  1.0877645374263394 0.35833333333333334\n",
      "95 번째 loss, accuracy:  1.0876127038902104 0.35833333333333334\n",
      "96 번째 loss, accuracy:  1.0874566049691576 0.35833333333333334\n",
      "97 번째 loss, accuracy:  1.0872960695531946 0.35833333333333334\n",
      "98 번째 loss, accuracy:  1.0871302736534385 0.35833333333333334\n",
      "99 번째 loss, accuracy:  1.0869591177415547 0.35833333333333334\n",
      "100 번째 loss, accuracy:  1.0867827844633748 0.35833333333333334\n",
      "101 번째 loss, accuracy:  1.0866012136193481 0.35833333333333334\n",
      "102 번째 loss, accuracy:  1.0864136547712502 0.35833333333333334\n",
      "103 번째 loss, accuracy:  1.0862201518765928 0.35833333333333334\n",
      "104 번째 loss, accuracy:  1.0860197755122571 0.35833333333333334\n",
      "105 번째 loss, accuracy:  1.0858127786521625 0.35833333333333334\n",
      "106 번째 loss, accuracy:  1.0855990422027888 0.35833333333333334\n",
      "107 번째 loss, accuracy:  1.085378077589897 0.35833333333333334\n",
      "108 번째 loss, accuracy:  1.0851493859567762 0.35833333333333334\n",
      "109 번째 loss, accuracy:  1.0849124506947467 0.35833333333333334\n",
      "110 번째 loss, accuracy:  1.0846673517691208 0.35833333333333334\n",
      "111 번째 loss, accuracy:  1.0844134862758983 0.35833333333333334\n",
      "112 번째 loss, accuracy:  1.0841505975131323 0.35833333333333334\n",
      "113 번째 loss, accuracy:  1.0838773875177026 0.35833333333333334\n",
      "114 번째 loss, accuracy:  1.0835943740978649 0.35833333333333334\n",
      "115 번째 loss, accuracy:  1.083301968045546 0.35833333333333334\n",
      "116 번째 loss, accuracy:  1.0829972027803334 0.35833333333333334\n",
      "117 번째 loss, accuracy:  1.0826808527107241 0.35833333333333334\n",
      "118 번째 loss, accuracy:  1.082352487066869 0.35833333333333334\n",
      "119 번째 loss, accuracy:  1.0820114241566021 0.35833333333333334\n",
      "120 번째 loss, accuracy:  1.0816560092262804 0.35833333333333334\n",
      "121 번째 loss, accuracy:  1.0812874450674959 0.35833333333333334\n",
      "122 번째 loss, accuracy:  1.0809028733089356 0.35833333333333334\n",
      "123 번째 loss, accuracy:  1.0805026425983837 0.35833333333333334\n",
      "124 번째 loss, accuracy:  1.080086456771207 0.35833333333333334\n",
      "125 번째 loss, accuracy:  1.0796541339459338 0.35833333333333334\n",
      "126 번째 loss, accuracy:  1.0792048882685157 0.35833333333333334\n",
      "127 번째 loss, accuracy:  1.0787365310866013 0.35833333333333334\n",
      "128 번째 loss, accuracy:  1.0782471549809762 0.35833333333333334\n",
      "129 번째 loss, accuracy:  1.0777361785447928 0.35833333333333334\n",
      "130 번째 loss, accuracy:  1.0772038243138975 0.35833333333333334\n",
      "131 번째 loss, accuracy:  1.0766480094003248 0.35833333333333334\n",
      "132 번째 loss, accuracy:  1.0760701050805495 0.35833333333333334\n",
      "133 번째 loss, accuracy:  1.0754662609517662 0.35833333333333334\n",
      "134 번째 loss, accuracy:  1.074838795255524 0.35833333333333334\n",
      "135 번째 loss, accuracy:  1.0741858258491805 0.35833333333333334\n",
      "136 번째 loss, accuracy:  1.0735032462492198 0.35833333333333334\n",
      "137 번째 loss, accuracy:  1.0727910323206122 0.35833333333333334\n",
      "138 번째 loss, accuracy:  1.0720534619494495 0.35833333333333334\n",
      "139 번째 loss, accuracy:  1.0712813580956921 0.35833333333333334\n",
      "140 번째 loss, accuracy:  1.070479074771818 0.35833333333333334\n",
      "141 번째 loss, accuracy:  1.0696444422476918 0.35833333333333334\n",
      "142 번째 loss, accuracy:  1.068775860557372 0.35833333333333334\n",
      "143 번째 loss, accuracy:  1.0678706471123467 0.35833333333333334\n",
      "144 번째 loss, accuracy:  1.0669357397816623 0.35833333333333334\n",
      "145 번째 loss, accuracy:  1.065961212343772 0.35833333333333334\n",
      "146 번째 loss, accuracy:  1.0649551247728495 0.35833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 번째 loss, accuracy:  1.063909678876821 0.35833333333333334\n",
      "148 번째 loss, accuracy:  1.0628262221412172 0.35833333333333334\n",
      "149 번째 loss, accuracy:  1.0617080305707898 0.35833333333333334\n",
      "150 번째 loss, accuracy:  1.060555585950325 0.35833333333333334\n",
      "151 번째 loss, accuracy:  1.0593662303206601 0.35833333333333334\n",
      "152 번째 loss, accuracy:  1.0581444726032698 0.35833333333333334\n",
      "153 번째 loss, accuracy:  1.056881552013016 0.35833333333333334\n",
      "154 번째 loss, accuracy:  1.055589020683317 0.35833333333333334\n",
      "155 번째 loss, accuracy:  1.0542675378137936 0.35833333333333334\n",
      "156 번째 loss, accuracy:  1.0529162579796774 0.35833333333333334\n",
      "157 번째 loss, accuracy:  1.0515337396195599 0.35833333333333334\n",
      "158 번째 loss, accuracy:  1.0501278283836966 0.35833333333333334\n",
      "159 번째 loss, accuracy:  1.0486925122228936 0.35833333333333334\n",
      "160 번째 loss, accuracy:  1.0472283283152213 0.36666666666666664\n",
      "161 번째 loss, accuracy:  1.0457467332664794 0.36666666666666664\n",
      "162 번째 loss, accuracy:  1.0442374067016609 0.36666666666666664\n",
      "163 번째 loss, accuracy:  1.042703791707371 0.375\n",
      "164 번째 loss, accuracy:  1.0411549807540377 0.375\n",
      "165 번째 loss, accuracy:  1.0395801806875664 0.375\n",
      "166 번째 loss, accuracy:  1.0379883443142846 0.375\n",
      "167 번째 loss, accuracy:  1.0363822674087848 0.39166666666666666\n",
      "168 번째 loss, accuracy:  1.0347652390848125 0.4166666666666667\n",
      "169 번째 loss, accuracy:  1.033132883392597 0.43333333333333335\n",
      "170 번째 loss, accuracy:  1.0314855303894637 0.44166666666666665\n",
      "171 번째 loss, accuracy:  1.0298152393825284 0.4666666666666667\n",
      "172 번째 loss, accuracy:  1.0281320605565059 0.475\n",
      "173 번째 loss, accuracy:  1.026442664569362 0.5166666666666667\n",
      "174 번째 loss, accuracy:  1.0247329681083213 0.5333333333333333\n",
      "175 번째 loss, accuracy:  1.0230066615022733 0.55\n",
      "176 번째 loss, accuracy:  1.0212675535994038 0.5583333333333333\n",
      "177 번째 loss, accuracy:  1.0195179757744728 0.5833333333333334\n",
      "178 번째 loss, accuracy:  1.017757885724899 0.5916666666666667\n",
      "179 번째 loss, accuracy:  1.0159808808529318 0.6\n",
      "180 번째 loss, accuracy:  1.0141952926602202 0.625\n",
      "181 번째 loss, accuracy:  1.0123986962986555 0.6333333333333333\n",
      "182 번째 loss, accuracy:  1.0105915988905243 0.65\n",
      "183 번째 loss, accuracy:  1.0087709597073824 0.65\n",
      "184 번째 loss, accuracy:  1.0069430505802788 0.6666666666666666\n",
      "185 번째 loss, accuracy:  1.0051013287524955 0.675\n",
      "186 번째 loss, accuracy:  1.0032524738584252 0.6833333333333333\n",
      "187 번째 loss, accuracy:  1.0013918173732255 0.6833333333333333\n",
      "188 번째 loss, accuracy:  0.9995237145455286 0.6916666666666667\n",
      "189 번째 loss, accuracy:  0.9976480595246981 0.6916666666666667\n",
      "190 번째 loss, accuracy:  0.9957602685004275 0.6916666666666667\n",
      "191 번째 loss, accuracy:  0.9938628006792946 0.6916666666666667\n",
      "192 번째 loss, accuracy:  0.9919594101642363 0.6916666666666667\n",
      "193 번째 loss, accuracy:  0.9900457204981132 0.6916666666666667\n",
      "194 번째 loss, accuracy:  0.988126362799719 0.6916666666666667\n",
      "195 번째 loss, accuracy:  0.9861995141405894 0.6916666666666667\n",
      "196 번째 loss, accuracy:  0.9842643956616797 0.6916666666666667\n",
      "197 번째 loss, accuracy:  0.9823218168994712 0.6916666666666667\n",
      "198 번째 loss, accuracy:  0.9803741858658082 0.6916666666666667\n",
      "199 번째 loss, accuracy:  0.9784201051182845 0.6916666666666667\n",
      "200 번째 loss, accuracy:  0.9764598710660325 0.6916666666666667\n",
      "201 번째 loss, accuracy:  0.9744944310611492 0.6916666666666667\n",
      "202 번째 loss, accuracy:  0.9725233633893087 0.6916666666666667\n",
      "203 번째 loss, accuracy:  0.9705466414732629 0.6916666666666667\n",
      "204 번째 loss, accuracy:  0.968565995684934 0.6916666666666667\n",
      "205 번째 loss, accuracy:  0.9665796389018115 0.6916666666666667\n",
      "206 번째 loss, accuracy:  0.9645901480107405 0.6916666666666667\n",
      "207 번째 loss, accuracy:  0.9625960196305478 0.6916666666666667\n",
      "208 번째 loss, accuracy:  0.9605996564173532 0.6916666666666667\n",
      "209 번째 loss, accuracy:  0.9585984731133855 0.6916666666666667\n",
      "210 번째 loss, accuracy:  0.9565951040140789 0.6916666666666667\n",
      "211 번째 loss, accuracy:  0.9545884972384099 0.6916666666666667\n",
      "212 번째 loss, accuracy:  0.9525799984307944 0.6916666666666667\n",
      "213 번째 loss, accuracy:  0.9505700566649533 0.6916666666666667\n",
      "214 번째 loss, accuracy:  0.9485584831771096 0.6916666666666667\n",
      "215 번째 loss, accuracy:  0.946546250535811 0.6916666666666667\n",
      "216 번째 loss, accuracy:  0.9445325208937521 0.6916666666666667\n",
      "217 번째 loss, accuracy:  0.9425177001351631 0.6916666666666667\n",
      "218 번째 loss, accuracy:  0.9405014945532196 0.6916666666666667\n",
      "219 번째 loss, accuracy:  0.9384848490195208 0.6916666666666667\n",
      "220 번째 loss, accuracy:  0.9364694420922722 0.6916666666666667\n",
      "221 번째 loss, accuracy:  0.934454027700011 0.6916666666666667\n",
      "222 번째 loss, accuracy:  0.9324392703387385 0.6916666666666667\n",
      "223 번째 loss, accuracy:  0.9304259287788904 0.6916666666666667\n",
      "224 번째 loss, accuracy:  0.9284125737609441 0.6916666666666667\n",
      "225 번째 loss, accuracy:  0.9264013668313562 0.6916666666666667\n",
      "226 번째 loss, accuracy:  0.9243913139798292 0.6916666666666667\n",
      "227 번째 loss, accuracy:  0.9223834625357864 0.6916666666666667\n",
      "228 번째 loss, accuracy:  0.9203775165364635 0.6916666666666667\n",
      "229 번째 loss, accuracy:  0.918373906575582 0.6916666666666667\n",
      "230 번째 loss, accuracy:  0.916373107630226 0.6916666666666667\n",
      "231 번째 loss, accuracy:  0.9143745243140091 0.6916666666666667\n",
      "232 번째 loss, accuracy:  0.9123801081694578 0.6916666666666667\n",
      "233 번째 loss, accuracy:  0.9103892012273732 0.6916666666666667\n",
      "234 번째 loss, accuracy:  0.9084018805515968 0.6916666666666667\n",
      "235 번째 loss, accuracy:  0.9064183089280846 0.6916666666666667\n",
      "236 번째 loss, accuracy:  0.904437338089615 0.6916666666666667\n",
      "237 번째 loss, accuracy:  0.9024611731068913 0.6916666666666667\n",
      "238 번째 loss, accuracy:  0.9004878558254326 0.6916666666666667\n",
      "239 번째 loss, accuracy:  0.898520480329218 0.6916666666666667\n",
      "240 번째 loss, accuracy:  0.8965583286181555 0.6916666666666667\n",
      "241 번째 loss, accuracy:  0.8946010390842736 0.6916666666666667\n",
      "242 번째 loss, accuracy:  0.8926482515009505 0.6916666666666667\n",
      "243 번째 loss, accuracy:  0.8907007474786633 0.6916666666666667\n",
      "244 번째 loss, accuracy:  0.8887589182029985 0.6916666666666667\n",
      "245 번째 loss, accuracy:  0.8868224079256029 0.6916666666666667\n",
      "246 번째 loss, accuracy:  0.884892377633245 0.6916666666666667\n",
      "247 번째 loss, accuracy:  0.8829665867679279 0.6916666666666667\n",
      "248 번째 loss, accuracy:  0.8810477330019181 0.6916666666666667\n",
      "249 번째 loss, accuracy:  0.8791355746031216 0.6916666666666667\n",
      "250 번째 loss, accuracy:  0.8772294725884111 0.6916666666666667\n",
      "251 번째 loss, accuracy:  0.8753272967452499 0.6916666666666667\n",
      "252 번째 loss, accuracy:  0.873434322904767 0.6916666666666667\n",
      "253 번째 loss, accuracy:  0.8715468011521982 0.6916666666666667\n",
      "254 번째 loss, accuracy:  0.8696673379420913 0.6916666666666667\n",
      "255 번째 loss, accuracy:  0.8677936121411423 0.6916666666666667\n",
      "256 번째 loss, accuracy:  0.8659278911688125 0.6916666666666667\n",
      "257 번째 loss, accuracy:  0.8640688783198569 0.6916666666666667\n",
      "258 번째 loss, accuracy:  0.8622170144232495 0.6916666666666667\n",
      "259 번째 loss, accuracy:  0.8603731349011139 0.6916666666666667\n",
      "260 번째 loss, accuracy:  0.8585360929707254 0.6916666666666667\n",
      "261 번째 loss, accuracy:  0.8567069209674197 0.6916666666666667\n",
      "262 번째 loss, accuracy:  0.8548833917981997 0.6916666666666667\n",
      "263 번째 loss, accuracy:  0.8530695145154535 0.6916666666666667\n",
      "264 번째 loss, accuracy:  0.8512631758167283 0.6916666666666667\n",
      "265 번째 loss, accuracy:  0.8494637612360026 0.6916666666666667\n",
      "266 번째 loss, accuracy:  0.8476728378786346 0.6916666666666667\n",
      "267 번째 loss, accuracy:  0.8458901272209914 0.6916666666666667\n",
      "268 번째 loss, accuracy:  0.8441149816611732 0.6916666666666667\n",
      "269 번째 loss, accuracy:  0.842347234385122 0.6916666666666667\n",
      "270 번째 loss, accuracy:  0.8405882604534857 0.6916666666666667\n",
      "271 번째 loss, accuracy:  0.838837246319523 0.6916666666666667\n",
      "272 번째 loss, accuracy:  0.8370942151668601 0.6916666666666667\n",
      "273 번째 loss, accuracy:  0.8353596790860524 0.6916666666666667\n",
      "274 번째 loss, accuracy:  0.8336310709203416 0.6916666666666667\n",
      "275 번째 loss, accuracy:  0.8319126855657585 0.6916666666666667\n",
      "276 번째 loss, accuracy:  0.8302031841931694 0.6916666666666667\n",
      "277 번째 loss, accuracy:  0.8285011366761271 0.6916666666666667\n",
      "278 번째 loss, accuracy:  0.826808565790149 0.6916666666666667\n",
      "279 번째 loss, accuracy:  0.8251244368789059 0.6916666666666667\n",
      "280 번째 loss, accuracy:  0.8234482892758344 0.6916666666666667\n",
      "281 번째 loss, accuracy:  0.8217804590578802 0.6916666666666667\n",
      "282 번째 loss, accuracy:  0.8201192516495491 0.6916666666666667\n",
      "283 번째 loss, accuracy:  0.8184688237234432 0.6916666666666667\n",
      "284 번째 loss, accuracy:  0.8168269712705869 0.6916666666666667\n",
      "285 번째 loss, accuracy:  0.8151929497193213 0.6916666666666667\n",
      "286 번째 loss, accuracy:  0.8135681129043165 0.6916666666666667\n",
      "287 번째 loss, accuracy:  0.8119505501695556 0.6916666666666667\n",
      "288 번째 loss, accuracy:  0.8103426387266452 0.6916666666666667\n",
      "289 번째 loss, accuracy:  0.8087430187760752 0.6916666666666667\n",
      "290 번째 loss, accuracy:  0.8071522548111857 0.6916666666666667\n",
      "291 번째 loss, accuracy:  0.8055699527094932 0.6916666666666667\n",
      "292 번째 loss, accuracy:  0.8039962959359819 0.6916666666666667\n",
      "293 번째 loss, accuracy:  0.8024306011123291 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294 번째 loss, accuracy:  0.8008738643505089 0.6916666666666667\n",
      "295 번째 loss, accuracy:  0.799325123522697 0.6916666666666667\n",
      "296 번째 loss, accuracy:  0.7977850342287283 0.6916666666666667\n",
      "297 번째 loss, accuracy:  0.7962530610791771 0.6916666666666667\n",
      "298 번째 loss, accuracy:  0.7947301646632987 0.6916666666666667\n",
      "299 번째 loss, accuracy:  0.7932147143256214 0.6916666666666667\n",
      "300 번째 loss, accuracy:  0.7917084336900456 0.6916666666666667\n",
      "301 번째 loss, accuracy:  0.7902108731358872 0.6916666666666667\n",
      "302 번째 loss, accuracy:  0.7887208509008427 0.6916666666666667\n",
      "303 번째 loss, accuracy:  0.787240311636555 0.6916666666666667\n",
      "304 번째 loss, accuracy:  0.7857675646857654 0.6916666666666667\n",
      "305 번째 loss, accuracy:  0.7843034960329568 0.6916666666666667\n",
      "306 번째 loss, accuracy:  0.7828473626092785 0.6916666666666667\n",
      "307 번째 loss, accuracy:  0.7813994675773124 0.6916666666666667\n",
      "308 번째 loss, accuracy:  0.7799583400187601 0.6916666666666667\n",
      "309 번째 loss, accuracy:  0.7785271407884474 0.6916666666666667\n",
      "310 번째 loss, accuracy:  0.7771040865167623 0.6916666666666667\n",
      "311 번째 loss, accuracy:  0.7756888255709842 0.6916666666666667\n",
      "312 번째 loss, accuracy:  0.7742815492569686 0.6916666666666667\n",
      "313 번째 loss, accuracy:  0.7728831030664008 0.6916666666666667\n",
      "314 번째 loss, accuracy:  0.7714926268359902 0.6916666666666667\n",
      "315 번째 loss, accuracy:  0.7701099233370656 0.6916666666666667\n",
      "316 번째 loss, accuracy:  0.7687356714412156 0.6916666666666667\n",
      "317 번째 loss, accuracy:  0.7673692862943604 0.6916666666666667\n",
      "318 번째 loss, accuracy:  0.7660103352939994 0.6916666666666667\n",
      "319 번째 loss, accuracy:  0.7646602685492622 0.6916666666666667\n",
      "320 번째 loss, accuracy:  0.7633179016648864 0.6916666666666667\n",
      "321 번째 loss, accuracy:  0.7619833658930639 0.6916666666666667\n",
      "322 번째 loss, accuracy:  0.7606566830147643 0.6916666666666667\n",
      "323 번째 loss, accuracy:  0.7593374235484298 0.6916666666666667\n",
      "324 번째 loss, accuracy:  0.7580257226031542 0.6916666666666667\n",
      "325 번째 loss, accuracy:  0.7567215763502176 0.6916666666666667\n",
      "326 번째 loss, accuracy:  0.7554260488669831 0.6916666666666667\n",
      "327 번째 loss, accuracy:  0.7541380199964538 0.6916666666666667\n",
      "328 번째 loss, accuracy:  0.7528576908808535 0.6916666666666667\n",
      "329 번째 loss, accuracy:  0.7515850789152576 0.6916666666666667\n",
      "330 번째 loss, accuracy:  0.7503196852370732 0.6916666666666667\n",
      "331 번째 loss, accuracy:  0.74906192679341 0.6916666666666667\n",
      "332 번째 loss, accuracy:  0.7478116315542759 0.6916666666666667\n",
      "333 번째 loss, accuracy:  0.7465686640735666 0.6916666666666667\n",
      "334 번째 loss, accuracy:  0.7453333215331843 0.6916666666666667\n",
      "335 번째 loss, accuracy:  0.7441053707055645 0.6916666666666667\n",
      "336 번째 loss, accuracy:  0.7428846822494873 0.6916666666666667\n",
      "337 번째 loss, accuracy:  0.7416711438537692 0.6916666666666667\n",
      "338 번째 loss, accuracy:  0.7404645753359158 0.6916666666666667\n",
      "339 번째 loss, accuracy:  0.7392654938278357 0.6916666666666667\n",
      "340 번째 loss, accuracy:  0.738073102693616 0.6916666666666667\n",
      "341 번째 loss, accuracy:  0.7368882353416865 0.6916666666666667\n",
      "342 번째 loss, accuracy:  0.735710608973798 0.6916666666666667\n",
      "343 번째 loss, accuracy:  0.7345397708356624 0.6916666666666667\n",
      "344 번째 loss, accuracy:  0.7333755924957723 0.6916666666666667\n",
      "345 번째 loss, accuracy:  0.7322187069111801 0.6916666666666667\n",
      "346 번째 loss, accuracy:  0.7310686083167707 0.6916666666666667\n",
      "347 번째 loss, accuracy:  0.729925151731387 0.6916666666666667\n",
      "348 번째 loss, accuracy:  0.7287889153419688 0.6916666666666667\n",
      "349 번째 loss, accuracy:  0.7276594850844461 0.6916666666666667\n",
      "350 번째 loss, accuracy:  0.7265367737343633 0.6916666666666667\n",
      "351 번째 loss, accuracy:  0.7254206840136491 0.6916666666666667\n",
      "352 번째 loss, accuracy:  0.7243111583623161 0.6916666666666667\n",
      "353 번째 loss, accuracy:  0.7232079351740939 0.6916666666666667\n",
      "354 번째 loss, accuracy:  0.722111754074967 0.6916666666666667\n",
      "355 번째 loss, accuracy:  0.7210215591126198 0.6916666666666667\n",
      "356 번째 loss, accuracy:  0.7199385087088448 0.6916666666666667\n",
      "357 번째 loss, accuracy:  0.7188618045809193 0.6916666666666667\n",
      "358 번째 loss, accuracy:  0.7177916319904611 0.6916666666666667\n",
      "359 번째 loss, accuracy:  0.7167278233095603 0.6916666666666667\n",
      "360 번째 loss, accuracy:  0.7156702118587774 0.6916666666666667\n",
      "361 번째 loss, accuracy:  0.7146190306799209 0.6916666666666667\n",
      "362 번째 loss, accuracy:  0.7135734471931502 0.6916666666666667\n",
      "363 번째 loss, accuracy:  0.7125345930719126 0.6916666666666667\n",
      "364 번째 loss, accuracy:  0.7115019890064772 0.6916666666666667\n",
      "365 번째 loss, accuracy:  0.7104755478013812 0.6916666666666667\n",
      "366 번째 loss, accuracy:  0.7094549093014535 0.6916666666666667\n",
      "367 번째 loss, accuracy:  0.7084405500620081 0.6916666666666667\n",
      "368 번째 loss, accuracy:  0.7074322261950258 0.6916666666666667\n",
      "369 번째 loss, accuracy:  0.7064297872558517 0.6916666666666667\n",
      "370 번째 loss, accuracy:  0.7054332679883282 0.6916666666666667\n",
      "371 번째 loss, accuracy:  0.7044424481807965 0.6916666666666667\n",
      "372 번째 loss, accuracy:  0.7034575329893035 0.6916666666666667\n",
      "373 번째 loss, accuracy:  0.7024783620085854 0.6916666666666667\n",
      "374 번째 loss, accuracy:  0.7015051663747651 0.6916666666666667\n",
      "375 번째 loss, accuracy:  0.700537762907437 0.6916666666666667\n",
      "376 번째 loss, accuracy:  0.6995756747706068 0.6916666666666667\n",
      "377 번째 loss, accuracy:  0.6986197037842021 0.6916666666666667\n",
      "378 번째 loss, accuracy:  0.6976693397558397 0.6916666666666667\n",
      "379 번째 loss, accuracy:  0.6967245373074272 0.6916666666666667\n",
      "380 번째 loss, accuracy:  0.6957851011212599 0.6916666666666667\n",
      "381 번째 loss, accuracy:  0.6948510829594563 0.6916666666666667\n",
      "382 번째 loss, accuracy:  0.6939220364704881 0.6916666666666667\n",
      "383 번째 loss, accuracy:  0.692999051181596 0.6916666666666667\n",
      "384 번째 loss, accuracy:  0.6920816006306896 0.6916666666666667\n",
      "385 번째 loss, accuracy:  0.691169108067225 0.6916666666666667\n",
      "386 번째 loss, accuracy:  0.6902623416357826 0.6916666666666667\n",
      "387 번째 loss, accuracy:  0.6893608572174457 0.6916666666666667\n",
      "388 번째 loss, accuracy:  0.6884643471969211 0.6916666666666667\n",
      "389 번째 loss, accuracy:  0.6875732993124408 0.6916666666666667\n",
      "390 번째 loss, accuracy:  0.6866872033174427 0.6916666666666667\n",
      "391 번째 loss, accuracy:  0.685806401085562 0.6916666666666667\n",
      "392 번째 loss, accuracy:  0.6849306507124623 0.6916666666666667\n",
      "393 번째 loss, accuracy:  0.6840599098420796 0.6916666666666667\n",
      "394 번째 loss, accuracy:  0.6831942929614906 0.6916666666666667\n",
      "395 번째 loss, accuracy:  0.6823326064979478 0.6916666666666667\n",
      "396 번째 loss, accuracy:  0.6814769944242056 0.6916666666666667\n",
      "397 번째 loss, accuracy:  0.6806254210466616 0.6916666666666667\n",
      "398 번째 loss, accuracy:  0.679779509003562 0.6916666666666667\n",
      "399 번째 loss, accuracy:  0.6789386023491293 0.6916666666666667\n",
      "400 번째 loss, accuracy:  0.6781024836964652 0.6916666666666667\n",
      "401 번째 loss, accuracy:  0.677271129172032 0.6916666666666667\n",
      "402 번째 loss, accuracy:  0.6764445531467899 0.6916666666666667\n",
      "403 번째 loss, accuracy:  0.6756227071130134 0.6916666666666667\n",
      "404 번째 loss, accuracy:  0.6748055172011026 0.6916666666666667\n",
      "405 번째 loss, accuracy:  0.6739930435738836 0.6916666666666667\n",
      "406 번째 loss, accuracy:  0.6731850637130852 0.6916666666666667\n",
      "407 번째 loss, accuracy:  0.6723819322528531 0.6916666666666667\n",
      "408 번째 loss, accuracy:  0.6715832689864274 0.6916666666666667\n",
      "409 번째 loss, accuracy:  0.6707890032071718 0.6916666666666667\n",
      "410 번째 loss, accuracy:  0.6699994141710425 0.6916666666666667\n",
      "411 번째 loss, accuracy:  0.6692140522710287 0.6916666666666667\n",
      "412 번째 loss, accuracy:  0.6684333702477756 0.6916666666666667\n",
      "413 번째 loss, accuracy:  0.6676570456202603 0.6916666666666667\n",
      "414 번째 loss, accuracy:  0.6668850302532146 0.6916666666666667\n",
      "415 번째 loss, accuracy:  0.6661174425866301 0.6916666666666667\n",
      "416 번째 loss, accuracy:  0.6653542140348778 0.6916666666666667\n",
      "417 번째 loss, accuracy:  0.6645950357041899 0.6916666666666667\n",
      "418 번째 loss, accuracy:  0.6638401800869681 0.6916666666666667\n",
      "419 번째 loss, accuracy:  0.6630894298207468 0.6916666666666667\n",
      "420 번째 loss, accuracy:  0.6623431528268303 0.6916666666666667\n",
      "421 번째 loss, accuracy:  0.6616007022427345 0.6916666666666667\n",
      "422 번째 loss, accuracy:  0.6608623726431546 0.6916666666666667\n",
      "423 번째 loss, accuracy:  0.6601284849245458 0.6916666666666667\n",
      "424 번째 loss, accuracy:  0.6593986538574261 0.6916666666666667\n",
      "425 번째 loss, accuracy:  0.6586728744561656 0.6916666666666667\n",
      "426 번째 loss, accuracy:  0.6579510979186826 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "427 번째 loss, accuracy:  0.6572333851129897 0.6916666666666667\n",
      "428 번째 loss, accuracy:  0.6565194053431088 0.6916666666666667\n",
      "429 번째 loss, accuracy:  0.6558095478684358 0.6916666666666667\n",
      "430 번째 loss, accuracy:  0.6551036648101539 0.6916666666666667\n",
      "431 번째 loss, accuracy:  0.6544015505341624 0.6916666666666667\n",
      "432 번째 loss, accuracy:  0.6537032965322374 0.6916666666666667\n",
      "433 번째 loss, accuracy:  0.6530088681980627 0.6916666666666667\n",
      "434 번째 loss, accuracy:  0.6523183103773587 0.6916666666666667\n",
      "435 번째 loss, accuracy:  0.6516313366804328 0.6916666666666667\n",
      "436 번째 loss, accuracy:  0.6509483252140084 0.6916666666666667\n",
      "437 번째 loss, accuracy:  0.6502690225835749 0.6916666666666667\n",
      "438 번째 loss, accuracy:  0.649593353555558 0.6916666666666667\n",
      "439 번째 loss, accuracy:  0.6489214208766596 0.6916666666666667\n",
      "440 번째 loss, accuracy:  0.64825317173698 0.6916666666666667\n",
      "441 번째 loss, accuracy:  0.6475884099781912 0.6916666666666667\n",
      "442 번째 loss, accuracy:  0.6469271524304384 0.6916666666666667\n",
      "443 번째 loss, accuracy:  0.6462696337600071 0.6916666666666667\n",
      "444 번째 loss, accuracy:  0.6456155128783013 0.6916666666666667\n",
      "445 번째 loss, accuracy:  0.6449650985432013 0.6916666666666667\n",
      "446 번째 loss, accuracy:  0.6443180607552587 0.6916666666666667\n",
      "447 번째 loss, accuracy:  0.6436745315649242 0.6916666666666667\n",
      "448 번째 loss, accuracy:  0.6430344143518634 0.6916666666666667\n",
      "449 번째 loss, accuracy:  0.642397794024604 0.6916666666666667\n",
      "450 번째 loss, accuracy:  0.6417645921823646 0.6916666666666667\n",
      "451 번째 loss, accuracy:  0.6411347461892726 0.6916666666666667\n",
      "452 번째 loss, accuracy:  0.6405082361578499 0.6916666666666667\n",
      "453 번째 loss, accuracy:  0.6398849718406249 0.6916666666666667\n",
      "454 번째 loss, accuracy:  0.6392650737703381 0.6916666666666667\n",
      "455 번째 loss, accuracy:  0.6386484057700647 0.6916666666666667\n",
      "456 번째 loss, accuracy:  0.6380349200427726 0.6916666666666667\n",
      "457 번째 loss, accuracy:  0.6374247588707278 0.6916666666666667\n",
      "458 번째 loss, accuracy:  0.6368178971193247 0.6916666666666667\n",
      "459 번째 loss, accuracy:  0.6362140526545895 0.6916666666666667\n",
      "460 번째 loss, accuracy:  0.6356135093210374 0.6916666666666667\n",
      "461 번째 loss, accuracy:  0.6350161105617596 0.6916666666666667\n",
      "462 번째 loss, accuracy:  0.6344217880571071 0.6916666666666667\n",
      "463 번째 loss, accuracy:  0.6338305732958144 0.6916666666666667\n",
      "464 번째 loss, accuracy:  0.6332424826615273 0.6916666666666667\n",
      "465 번째 loss, accuracy:  0.6326575187193026 0.6916666666666667\n",
      "466 번째 loss, accuracy:  0.6320755789818797 0.6916666666666667\n",
      "467 번째 loss, accuracy:  0.6314966513908694 0.6916666666666667\n",
      "468 번째 loss, accuracy:  0.6309206701988662 0.6916666666666667\n",
      "469 번째 loss, accuracy:  0.6303475773025785 0.6916666666666667\n",
      "470 번째 loss, accuracy:  0.6297776240752253 0.6916666666666667\n",
      "471 번째 loss, accuracy:  0.6292105855095984 0.6916666666666667\n",
      "472 번째 loss, accuracy:  0.6286464374134301 0.6916666666666667\n",
      "473 번째 loss, accuracy:  0.6280851728813707 0.6916666666666667\n",
      "474 번째 loss, accuracy:  0.627526691379519 0.6916666666666667\n",
      "475 번째 loss, accuracy:  0.6269710905138982 0.6916666666666667\n",
      "476 번째 loss, accuracy:  0.626418396873219 0.6916666666666667\n",
      "477 번째 loss, accuracy:  0.6258685834733596 0.6916666666666667\n",
      "478 번째 loss, accuracy:  0.625321472843464 0.6916666666666667\n",
      "479 번째 loss, accuracy:  0.624777093617729 0.6916666666666667\n",
      "480 번째 loss, accuracy:  0.6242355766635991 0.6916666666666667\n",
      "481 번째 loss, accuracy:  0.6236968224779864 0.6916666666666667\n",
      "482 번째 loss, accuracy:  0.6231606973340954 0.6916666666666667\n",
      "483 번째 loss, accuracy:  0.6226273055059851 0.6916666666666667\n",
      "484 번째 loss, accuracy:  0.6220966966651792 0.6916666666666667\n",
      "485 번째 loss, accuracy:  0.6215687555276929 0.6916666666666667\n",
      "486 번째 loss, accuracy:  0.6210431544652029 0.6916666666666667\n",
      "487 번째 loss, accuracy:  0.6205203202358865 0.6916666666666667\n",
      "488 번째 loss, accuracy:  0.620000266103975 0.6916666666666667\n",
      "489 번째 loss, accuracy:  0.6194828533387186 0.6916666666666667\n",
      "490 번째 loss, accuracy:  0.6189676745122746 0.6916666666666667\n",
      "491 번째 loss, accuracy:  0.6184553435129814 0.6916666666666667\n",
      "492 번째 loss, accuracy:  0.6179456388235489 0.6916666666666667\n",
      "493 번째 loss, accuracy:  0.6174383965318637 0.6916666666666667\n",
      "494 번째 loss, accuracy:  0.6169335464642032 0.6916666666666667\n",
      "495 번째 loss, accuracy:  0.6164313382546468 0.6916666666666667\n",
      "496 번째 loss, accuracy:  0.615931468866009 0.6916666666666667\n",
      "497 번째 loss, accuracy:  0.6154340378154575 0.6916666666666667\n",
      "498 번째 loss, accuracy:  0.6149391224504552 0.6916666666666667\n",
      "499 번째 loss, accuracy:  0.6144466897846083 0.6916666666666667\n",
      "500 번째 loss, accuracy:  0.6139567588269604 0.6916666666666667\n",
      "501 번째 loss, accuracy:  0.6134691938490258 0.6916666666666667\n",
      "502 번째 loss, accuracy:  0.6129838653546852 0.6916666666666667\n",
      "503 번째 loss, accuracy:  0.6125010540916349 0.6916666666666667\n",
      "504 번째 loss, accuracy:  0.6120205988408406 0.6916666666666667\n",
      "505 번째 loss, accuracy:  0.6115423274545642 0.6916666666666667\n",
      "506 번째 loss, accuracy:  0.6110665232363357 0.6916666666666667\n",
      "507 번째 loss, accuracy:  0.6105930445635105 0.6916666666666667\n",
      "508 번째 loss, accuracy:  0.6101218255063725 0.6916666666666667\n",
      "509 번째 loss, accuracy:  0.6096528233231217 0.6916666666666667\n",
      "510 번째 loss, accuracy:  0.6091861315676762 0.6916666666666667\n",
      "511 번째 loss, accuracy:  0.6087217285120708 0.6916666666666667\n",
      "512 번째 loss, accuracy:  0.6082594457718565 0.6916666666666667\n",
      "513 번째 loss, accuracy:  0.6077994872386234 0.6916666666666667\n",
      "514 번째 loss, accuracy:  0.6073416215860716 0.6916666666666667\n",
      "515 번째 loss, accuracy:  0.6068860131585514 0.6916666666666667\n",
      "516 번째 loss, accuracy:  0.6064324717072931 0.6916666666666667\n",
      "517 번째 loss, accuracy:  0.6059812111023838 0.6916666666666667\n",
      "518 번째 loss, accuracy:  0.6055317948874105 0.6916666666666667\n",
      "519 번째 loss, accuracy:  0.6050847921874097 0.6916666666666667\n",
      "520 번째 loss, accuracy:  0.6046399108173044 0.6916666666666667\n",
      "521 번째 loss, accuracy:  0.6041969378068918 0.6916666666666667\n",
      "522 번째 loss, accuracy:  0.6037562100941465 0.6916666666666667\n",
      "523 번째 loss, accuracy:  0.6033176176617978 0.6916666666666667\n",
      "524 번째 loss, accuracy:  0.6028810761205954 0.6916666666666667\n",
      "525 번째 loss, accuracy:  0.6024463417894095 0.6916666666666667\n",
      "526 번째 loss, accuracy:  0.6020138590536919 0.6916666666666667\n",
      "527 번째 loss, accuracy:  0.6015834321265551 0.6916666666666667\n",
      "528 번째 loss, accuracy:  0.6011550480359495 0.6916666666666667\n",
      "529 번째 loss, accuracy:  0.6007284745774285 0.6916666666666667\n",
      "530 번째 loss, accuracy:  0.6003040866750041 0.6916666666666667\n",
      "531 번째 loss, accuracy:  0.5998816059844001 0.6916666666666667\n",
      "532 번째 loss, accuracy:  0.5994611171226313 0.6916666666666667\n",
      "533 번째 loss, accuracy:  0.5990425896405672 0.6916666666666667\n",
      "534 번째 loss, accuracy:  0.5986259852588103 0.6916666666666667\n",
      "535 번째 loss, accuracy:  0.5982113599722515 0.6916666666666667\n",
      "536 번째 loss, accuracy:  0.597798565874664 0.6916666666666667\n",
      "537 번째 loss, accuracy:  0.5973877107125617 0.6916666666666667\n",
      "538 번째 loss, accuracy:  0.5969786413501488 0.6916666666666667\n",
      "539 번째 loss, accuracy:  0.5965714802689805 0.6916666666666667\n",
      "540 번째 loss, accuracy:  0.5961661457658162 0.6916666666666667\n",
      "541 번째 loss, accuracy:  0.5957626089681748 0.6916666666666667\n",
      "542 번째 loss, accuracy:  0.5953610507459303 0.6916666666666667\n",
      "543 번째 loss, accuracy:  0.5949613488954364 0.6916666666666667\n",
      "544 번째 loss, accuracy:  0.5945634642611453 0.6916666666666667\n",
      "545 번째 loss, accuracy:  0.5941672642835196 0.6916666666666667\n",
      "546 번째 loss, accuracy:  0.5937729892283021 0.6916666666666667\n",
      "547 번째 loss, accuracy:  0.5933803741269007 0.6916666666666667\n",
      "548 번째 loss, accuracy:  0.5929895833895583 0.6916666666666667\n",
      "549 번째 loss, accuracy:  0.5926005284835436 0.6916666666666667\n",
      "550 번째 loss, accuracy:  0.592213256472243 0.6916666666666667\n",
      "551 번째 loss, accuracy:  0.5918276555889764 0.6916666666666667\n",
      "552 번째 loss, accuracy:  0.5914438682204455 0.6916666666666667\n",
      "553 번째 loss, accuracy:  0.5910617938167169 0.6916666666666667\n",
      "554 번째 loss, accuracy:  0.5906813434212985 0.6916666666666667\n",
      "555 번째 loss, accuracy:  0.5903026141527441 0.6916666666666667\n",
      "556 번째 loss, accuracy:  0.589925555739971 0.6916666666666667\n",
      "557 번째 loss, accuracy:  0.5895502342834894 0.6916666666666667\n",
      "558 번째 loss, accuracy:  0.5891764941878408 0.6916666666666667\n",
      "559 번째 loss, accuracy:  0.5888042607049819 0.6916666666666667\n",
      "560 번째 loss, accuracy:  0.5884338144190608 0.6916666666666667\n",
      "561 번째 loss, accuracy:  0.588065044623501 0.6916666666666667\n",
      "562 번째 loss, accuracy:  0.5876978937642022 0.6916666666666667\n",
      "563 번째 loss, accuracy:  0.5873323876872119 0.6916666666666667\n",
      "564 번째 loss, accuracy:  0.586968502092158 0.6916666666666667\n",
      "565 번째 loss, accuracy:  0.5866061834446548 0.6916666666666667\n",
      "566 번째 loss, accuracy:  0.5862453957571431 0.6916666666666667\n",
      "567 번째 loss, accuracy:  0.5858862473812735 0.6916666666666667\n",
      "568 번째 loss, accuracy:  0.5855284484511007 0.6916666666666667\n",
      "569 번째 loss, accuracy:  0.5851723606258552 0.6916666666666667\n",
      "570 번째 loss, accuracy:  0.5848177428752191 0.6916666666666667\n",
      "571 번째 loss, accuracy:  0.5844646196700582 0.6916666666666667\n",
      "572 번째 loss, accuracy:  0.5841132055198178 0.6916666666666667\n",
      "573 번째 loss, accuracy:  0.5837631321213476 0.6916666666666667\n",
      "574 번째 loss, accuracy:  0.5834145393509723 0.6916666666666667\n",
      "575 번째 loss, accuracy:  0.5830676454806041 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576 번째 loss, accuracy:  0.5827222439979719 0.6916666666666667\n",
      "577 번째 loss, accuracy:  0.582378224070828 0.6916666666666667\n",
      "578 번째 loss, accuracy:  0.5820357525170861 0.6916666666666667\n",
      "579 번째 loss, accuracy:  0.5816946103284123 0.6916666666666667\n",
      "580 번째 loss, accuracy:  0.5813549158177146 0.6916666666666667\n",
      "581 번째 loss, accuracy:  0.5810168020968817 0.6916666666666667\n",
      "582 번째 loss, accuracy:  0.5806800622413474 0.6916666666666667\n",
      "583 번째 loss, accuracy:  0.5803448107195289 0.6916666666666667\n",
      "584 번째 loss, accuracy:  0.5800109749122788 0.6916666666666667\n",
      "585 번째 loss, accuracy:  0.57967849395889 0.6916666666666667\n",
      "586 번째 loss, accuracy:  0.5793475084739306 0.6916666666666667\n",
      "587 번째 loss, accuracy:  0.5790178206731426 0.6916666666666667\n",
      "588 번째 loss, accuracy:  0.5786895565214303 0.6916666666666667\n",
      "589 번째 loss, accuracy:  0.5783627018456918 0.6916666666666667\n",
      "590 번째 loss, accuracy:  0.5780372385635721 0.6916666666666667\n",
      "591 번째 loss, accuracy:  0.5777130283014877 0.6916666666666667\n",
      "592 번째 loss, accuracy:  0.5773902452082449 0.6916666666666667\n",
      "593 번째 loss, accuracy:  0.5770688229772691 0.6916666666666667\n",
      "594 번째 loss, accuracy:  0.5767487255251156 0.6916666666666667\n",
      "595 번째 loss, accuracy:  0.5764299391192798 0.6916666666666667\n",
      "596 번째 loss, accuracy:  0.5761125103037759 0.6916666666666667\n",
      "597 번째 loss, accuracy:  0.5757962377310369 0.6916666666666667\n",
      "598 번째 loss, accuracy:  0.5754813773710913 0.6916666666666667\n",
      "599 번째 loss, accuracy:  0.5751678235973561 0.6916666666666667\n",
      "600 번째 loss, accuracy:  0.5748554786368264 0.6916666666666667\n",
      "601 번째 loss, accuracy:  0.5745444970753132 0.6916666666666667\n",
      "602 번째 loss, accuracy:  0.5742347985405958 0.6916666666666667\n",
      "603 번째 loss, accuracy:  0.5739263439389825 0.6916666666666667\n",
      "604 번째 loss, accuracy:  0.5736191558785946 0.6916666666666667\n",
      "605 번째 loss, accuracy:  0.5733132435457594 0.6916666666666667\n",
      "606 번째 loss, accuracy:  0.5730085081267279 0.6916666666666667\n",
      "607 번째 loss, accuracy:  0.5727050908537353 0.6916666666666667\n",
      "608 번째 loss, accuracy:  0.5724028492544819 0.6916666666666667\n",
      "609 번째 loss, accuracy:  0.5721018996267624 0.6916666666666667\n",
      "610 번째 loss, accuracy:  0.571802107890376 0.6916666666666667\n",
      "611 번째 loss, accuracy:  0.5715035582125249 0.6916666666666667\n",
      "612 번째 loss, accuracy:  0.5712062417018478 0.6916666666666667\n",
      "613 번째 loss, accuracy:  0.5709100906222273 0.6916666666666667\n",
      "614 번째 loss, accuracy:  0.5706151659658614 0.6916666666666667\n",
      "615 번째 loss, accuracy:  0.5703214500932304 0.6916666666666667\n",
      "616 번째 loss, accuracy:  0.5700288417590197 0.6916666666666667\n",
      "617 번째 loss, accuracy:  0.5697373915356885 0.6916666666666667\n",
      "618 번째 loss, accuracy:  0.5694471177370627 0.6916666666666667\n",
      "619 번째 loss, accuracy:  0.5691580055561068 0.6916666666666667\n",
      "620 번째 loss, accuracy:  0.5688700310587114 0.6916666666666667\n",
      "621 번째 loss, accuracy:  0.568583208750636 0.6916666666666667\n",
      "622 번째 loss, accuracy:  0.5682975192660145 0.6916666666666667\n",
      "623 번째 loss, accuracy:  0.5680129844388547 0.6916666666666667\n",
      "624 번째 loss, accuracy:  0.5677295798974943 0.6916666666666667\n",
      "625 번째 loss, accuracy:  0.5674472280004376 0.6916666666666667\n",
      "626 번째 loss, accuracy:  0.5671660522101469 0.6916666666666667\n",
      "627 번째 loss, accuracy:  0.5668859332528478 0.6916666666666667\n",
      "628 번째 loss, accuracy:  0.5666069187282311 0.6916666666666667\n",
      "629 번째 loss, accuracy:  0.5663289571598559 0.6916666666666667\n",
      "630 번째 loss, accuracy:  0.5660520621560574 0.6916666666666667\n",
      "631 번째 loss, accuracy:  0.5657762436569984 0.6916666666666667\n",
      "632 번째 loss, accuracy:  0.5655015950898723 0.6916666666666667\n",
      "633 번째 loss, accuracy:  0.5652280321949553 0.6916666666666667\n",
      "634 번째 loss, accuracy:  0.5649554632673573 0.6916666666666667\n",
      "635 번째 loss, accuracy:  0.5646839679093835 0.6916666666666667\n",
      "636 번째 loss, accuracy:  0.5644134695968724 0.6916666666666667\n",
      "637 번째 loss, accuracy:  0.5641440575128148 0.6916666666666667\n",
      "638 번째 loss, accuracy:  0.5638756868160842 0.6916666666666667\n",
      "639 번째 loss, accuracy:  0.5636083645327017 0.6916666666666667\n",
      "640 번째 loss, accuracy:  0.5633420817160506 0.6916666666666667\n",
      "641 번째 loss, accuracy:  0.5630767755337821 0.6916666666666667\n",
      "642 번째 loss, accuracy:  0.5628124829918655 0.6916666666666667\n",
      "643 번째 loss, accuracy:  0.562549170302826 0.6916666666666667\n",
      "644 번째 loss, accuracy:  0.562286883204811 0.6916666666666667\n",
      "645 번째 loss, accuracy:  0.5620256396533444 0.6916666666666667\n",
      "646 번째 loss, accuracy:  0.5617653764545969 0.6916666666666667\n",
      "647 번째 loss, accuracy:  0.5615060926587664 0.6916666666666667\n",
      "648 번째 loss, accuracy:  0.5612477470909241 0.6916666666666667\n",
      "649 번째 loss, accuracy:  0.5609904666943747 0.6916666666666667\n",
      "650 번째 loss, accuracy:  0.5607340598354634 0.6916666666666667\n",
      "651 번째 loss, accuracy:  0.5604785981830418 0.6916666666666667\n",
      "652 번째 loss, accuracy:  0.5602241553927343 0.6916666666666667\n",
      "653 번째 loss, accuracy:  0.559970743575247 0.6916666666666667\n",
      "654 번째 loss, accuracy:  0.5597182095579206 0.6916666666666667\n",
      "655 번째 loss, accuracy:  0.5594665990495017 0.6916666666666667\n",
      "656 번째 loss, accuracy:  0.5592159523056022 0.6916666666666667\n",
      "657 번째 loss, accuracy:  0.558966240184949 0.6916666666666667\n",
      "658 번째 loss, accuracy:  0.5587173901973185 0.6916666666666667\n",
      "659 번째 loss, accuracy:  0.5584695437486544 0.6916666666666667\n",
      "660 번째 loss, accuracy:  0.5582226288843943 0.6916666666666667\n",
      "661 번째 loss, accuracy:  0.5579766132064657 0.6916666666666667\n",
      "662 번째 loss, accuracy:  0.5577314914617733 0.6916666666666667\n",
      "663 번째 loss, accuracy:  0.5574872407041443 0.6916666666666667\n",
      "664 번째 loss, accuracy:  0.5572438969525763 0.6916666666666667\n",
      "665 번째 loss, accuracy:  0.5570014963135321 0.6916666666666667\n",
      "666 번째 loss, accuracy:  0.5567599784082243 0.6916666666666667\n",
      "667 번째 loss, accuracy:  0.5565193503188866 0.6916666666666667\n",
      "668 번째 loss, accuracy:  0.5562796217100808 0.6916666666666667\n",
      "669 번째 loss, accuracy:  0.5560406199155346 0.6916666666666667\n",
      "670 번째 loss, accuracy:  0.5558026203591101 0.6916666666666667\n",
      "671 번째 loss, accuracy:  0.5555654642571581 0.6916666666666667\n",
      "672 번째 loss, accuracy:  0.5553291632595998 0.6916666666666667\n",
      "673 번째 loss, accuracy:  0.555093714005166 0.6916666666666667\n",
      "674 번째 loss, accuracy:  0.5548591010894025 0.6916666666666667\n",
      "675 번째 loss, accuracy:  0.5546253588080287 0.6916666666666667\n",
      "676 번째 loss, accuracy:  0.554392433082059 0.6916666666666667\n",
      "677 번째 loss, accuracy:  0.5541603757315267 0.6916666666666667\n",
      "678 번째 loss, accuracy:  0.553929171453204 0.6916666666666667\n",
      "679 번째 loss, accuracy:  0.5536987918768811 0.6916666666666667\n",
      "680 번째 loss, accuracy:  0.5534691747224135 0.6916666666666667\n",
      "681 번째 loss, accuracy:  0.5532404113216965 0.6916666666666667\n",
      "682 번째 loss, accuracy:  0.5530124927620614 0.6916666666666667\n",
      "683 번째 loss, accuracy:  0.5527853587984154 0.6916666666666667\n",
      "684 번째 loss, accuracy:  0.5525590277253543 0.6916666666666667\n",
      "685 번째 loss, accuracy:  0.5523334910974197 0.6916666666666667\n",
      "686 번째 loss, accuracy:  0.5521088002971143 0.6916666666666667\n",
      "687 번째 loss, accuracy:  0.5518848989584966 0.6916666666666667\n",
      "688 번째 loss, accuracy:  0.5516617221211154 0.7\n",
      "689 번째 loss, accuracy:  0.5514394004298412 0.7\n",
      "690 번째 loss, accuracy:  0.5512178429742048 0.7\n",
      "691 번째 loss, accuracy:  0.5509970519071826 0.7\n",
      "692 번째 loss, accuracy:  0.5507770608864528 0.7\n",
      "693 번째 loss, accuracy:  0.5505578110714215 0.7\n",
      "694 번째 loss, accuracy:  0.550339354854439 0.7\n",
      "695 번째 loss, accuracy:  0.5501216651493323 0.7\n",
      "696 번째 loss, accuracy:  0.5499047528910304 0.7\n",
      "697 번째 loss, accuracy:  0.5496885783355533 0.7\n",
      "698 번째 loss, accuracy:  0.549473132289674 0.7\n",
      "699 번째 loss, accuracy:  0.5492584507849739 0.7\n",
      "700 번째 loss, accuracy:  0.5490445228319903 0.7\n",
      "701 번째 loss, accuracy:  0.5488312891632446 0.7\n",
      "702 번째 loss, accuracy:  0.5486188251615758 0.7\n",
      "703 번째 loss, accuracy:  0.5484071004995614 0.7\n",
      "704 번째 loss, accuracy:  0.5481960838915346 0.7\n",
      "705 번째 loss, accuracy:  0.5479857734312523 0.7\n",
      "706 번째 loss, accuracy:  0.5477761714878383 0.7\n",
      "707 번째 loss, accuracy:  0.5475673146842588 0.7\n",
      "708 번째 loss, accuracy:  0.5473591914659096 0.7\n",
      "709 번째 loss, accuracy:  0.5471517493932093 0.7\n",
      "710 번째 loss, accuracy:  0.5469449899621324 0.7\n",
      "711 번째 loss, accuracy:  0.5467389809273567 0.7\n",
      "712 번째 loss, accuracy:  0.5465336490031064 0.7\n",
      "713 번째 loss, accuracy:  0.5463290476199961 0.7\n",
      "714 번째 loss, accuracy:  0.5461251120809796 0.7\n",
      "715 번째 loss, accuracy:  0.5459218341651072 0.7\n",
      "716 번째 loss, accuracy:  0.5457192720329566 0.7\n",
      "717 번째 loss, accuracy:  0.5455174106282359 0.7\n",
      "718 번째 loss, accuracy:  0.5453162288214604 0.7\n",
      "719 번째 loss, accuracy:  0.5451156869955154 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 번째 loss, accuracy:  0.5449157101904234 0.7\n",
      "721 번째 loss, accuracy:  0.5447165733626416 0.7\n",
      "722 번째 loss, accuracy:  0.5445180374038028 0.7\n",
      "723 번째 loss, accuracy:  0.5443202239143005 0.7\n",
      "724 번째 loss, accuracy:  0.5441229673736266 0.7\n",
      "725 번째 loss, accuracy:  0.5439264080588465 0.7\n",
      "726 번째 loss, accuracy:  0.5437304899710241 0.7\n",
      "727 번째 loss, accuracy:  0.5435352162018785 0.7\n",
      "728 번째 loss, accuracy:  0.5433406731074484 0.7\n",
      "729 번째 loss, accuracy:  0.5431466194864237 0.7\n",
      "730 번째 loss, accuracy:  0.5429532754663738 0.7\n",
      "731 번째 loss, accuracy:  0.5427605362724109 0.7\n",
      "732 번째 loss, accuracy:  0.5425684075723342 0.7\n",
      "733 번째 loss, accuracy:  0.542376879554812 0.7\n",
      "734 번째 loss, accuracy:  0.5421860142137133 0.7\n",
      "735 번째 loss, accuracy:  0.5419958020958345 0.7\n",
      "736 번째 loss, accuracy:  0.5418062046035043 0.7\n",
      "737 번째 loss, accuracy:  0.5416172033725382 0.7\n",
      "738 번째 loss, accuracy:  0.5414288378044824 0.7\n",
      "739 번째 loss, accuracy:  0.5412410825995124 0.7\n",
      "740 번째 loss, accuracy:  0.5410539443620691 0.7\n",
      "741 번째 loss, accuracy:  0.5408673391310107 0.7\n",
      "742 번째 loss, accuracy:  0.5406813510562465 0.7\n",
      "743 번째 loss, accuracy:  0.5404960149608636 0.7\n",
      "744 번째 loss, accuracy:  0.5403112978294854 0.7\n",
      "745 번째 loss, accuracy:  0.5401270766513145 0.7\n",
      "746 번째 loss, accuracy:  0.539943478905924 0.7\n",
      "747 번째 loss, accuracy:  0.5397604801080834 0.7\n",
      "748 번째 loss, accuracy:  0.5395780895869297 0.7\n",
      "749 번째 loss, accuracy:  0.5393962464909701 0.7\n",
      "750 번째 loss, accuracy:  0.5392150459996146 0.7\n",
      "751 번째 loss, accuracy:  0.5390343358189047 0.7\n",
      "752 번째 loss, accuracy:  0.5388541695312897 0.7\n",
      "753 번째 loss, accuracy:  0.5386745687826079 0.7\n",
      "754 번째 loss, accuracy:  0.538495565994984 0.7\n",
      "755 번째 loss, accuracy:  0.5383171717117871 0.7\n",
      "756 번째 loss, accuracy:  0.5381393297662357 0.7\n",
      "757 번째 loss, accuracy:  0.5379620842839483 0.7\n",
      "758 번째 loss, accuracy:  0.5377852843904507 0.7\n",
      "759 번째 loss, accuracy:  0.537609026737734 0.7\n",
      "760 번째 loss, accuracy:  0.5374333450970543 0.7\n",
      "761 번째 loss, accuracy:  0.5372582431861476 0.7\n",
      "762 번째 loss, accuracy:  0.5370836178594298 0.7\n",
      "763 번째 loss, accuracy:  0.5369095624936019 0.7\n",
      "764 번째 loss, accuracy:  0.5367360651815732 0.7\n",
      "765 번째 loss, accuracy:  0.5365631230801654 0.7\n",
      "766 번째 loss, accuracy:  0.5363907122587023 0.7\n",
      "767 번째 loss, accuracy:  0.536218829404507 0.7\n",
      "768 번째 loss, accuracy:  0.5360474737451184 0.7\n",
      "769 번째 loss, accuracy:  0.5358765719920769 0.7\n",
      "770 번째 loss, accuracy:  0.535706265843492 0.7\n",
      "771 번째 loss, accuracy:  0.535536496896986 0.7\n",
      "772 번째 loss, accuracy:  0.5353672329396907 0.7\n",
      "773 번째 loss, accuracy:  0.5351984354132295 0.7\n",
      "774 번째 loss, accuracy:  0.5350301913199601 0.7\n",
      "775 번째 loss, accuracy:  0.5348624641107265 0.7\n",
      "776 번째 loss, accuracy:  0.5346952263975763 0.7\n",
      "777 번째 loss, accuracy:  0.5345284934865616 0.7\n",
      "778 번째 loss, accuracy:  0.534362269262472 0.7\n",
      "779 번째 loss, accuracy:  0.5341965355990439 0.7\n",
      "780 번째 loss, accuracy:  0.5340313012214061 0.7\n",
      "781 번째 loss, accuracy:  0.5338665493948971 0.7\n",
      "782 번째 loss, accuracy:  0.5337023260617575 0.7\n",
      "783 번째 loss, accuracy:  0.5335385721028406 0.7\n",
      "784 번째 loss, accuracy:  0.5333753078180692 0.7\n",
      "785 번째 loss, accuracy:  0.533212550214941 0.7\n",
      "786 번째 loss, accuracy:  0.5330502507689504 0.7\n",
      "787 번째 loss, accuracy:  0.5328884704570689 0.7\n",
      "788 번째 loss, accuracy:  0.5327271643017937 0.7\n",
      "789 번째 loss, accuracy:  0.5325663555310955 0.7\n",
      "790 번째 loss, accuracy:  0.5324059847408819 0.7\n",
      "791 번째 loss, accuracy:  0.5322460764030995 0.7\n",
      "792 번째 loss, accuracy:  0.5320866146395765 0.7\n",
      "793 번째 loss, accuracy:  0.5319276547932854 0.7\n",
      "794 번째 loss, accuracy:  0.5317691392162932 0.7\n",
      "795 번째 loss, accuracy:  0.5316111589876811 0.7\n",
      "796 번째 loss, accuracy:  0.531453519805898 0.7\n",
      "797 번째 loss, accuracy:  0.5312963915240323 0.7\n",
      "798 번째 loss, accuracy:  0.5311397045378449 0.7\n",
      "799 번째 loss, accuracy:  0.5309835278241996 0.7\n",
      "800 번째 loss, accuracy:  0.5308277442771746 0.7\n",
      "801 번째 loss, accuracy:  0.5306724148032601 0.7\n",
      "802 번째 loss, accuracy:  0.5305175362141189 0.7\n",
      "803 번째 loss, accuracy:  0.530363126363474 0.7\n",
      "804 번째 loss, accuracy:  0.5302091241300158 0.7\n",
      "805 번째 loss, accuracy:  0.5300556074575564 0.7\n",
      "806 번째 loss, accuracy:  0.5299025315717479 0.7\n",
      "807 번째 loss, accuracy:  0.5297498913887186 0.7\n",
      "808 번째 loss, accuracy:  0.5295976591774306 0.7\n",
      "809 번째 loss, accuracy:  0.5294459132650002 0.7\n",
      "810 번째 loss, accuracy:  0.5292945510592105 0.7\n",
      "811 번째 loss, accuracy:  0.529143635581626 0.7\n",
      "812 번째 loss, accuracy:  0.528993143057037 0.7\n",
      "813 번째 loss, accuracy:  0.5288431077128505 0.7\n",
      "814 번째 loss, accuracy:  0.5286934440908734 0.7\n",
      "815 번째 loss, accuracy:  0.528544236021176 0.7\n",
      "816 번째 loss, accuracy:  0.5283954248368251 0.7\n",
      "817 번째 loss, accuracy:  0.5282470150613183 0.7\n",
      "818 번째 loss, accuracy:  0.5280990407067104 0.7\n",
      "819 번째 loss, accuracy:  0.5279514905046938 0.7\n",
      "820 번째 loss, accuracy:  0.5278043628855943 0.7\n",
      "821 번째 loss, accuracy:  0.5276576220252366 0.7\n",
      "822 번째 loss, accuracy:  0.5275112757183146 0.7\n",
      "823 번째 loss, accuracy:  0.527365347921625 0.7\n",
      "824 번째 loss, accuracy:  0.527219841102294 0.7\n",
      "825 번째 loss, accuracy:  0.5270747320336368 0.7\n",
      "826 번째 loss, accuracy:  0.5269300234181207 0.7\n",
      "827 번째 loss, accuracy:  0.5267857158321508 0.7\n",
      "828 번째 loss, accuracy:  0.5266418139043747 0.7\n",
      "829 번째 loss, accuracy:  0.5264982951979864 0.7\n",
      "830 번째 loss, accuracy:  0.5263551600904328 0.7\n",
      "831 번째 loss, accuracy:  0.5262123878611354 0.7\n",
      "832 번째 loss, accuracy:  0.5260700397888417 0.7\n",
      "833 번째 loss, accuracy:  0.5259280585049799 0.7\n",
      "834 번째 loss, accuracy:  0.5257864664132093 0.7\n",
      "835 번째 loss, accuracy:  0.5256452661192866 0.7\n",
      "836 번째 loss, accuracy:  0.5255044575570041 0.7\n",
      "837 번째 loss, accuracy:  0.5253640273454043 0.7\n",
      "838 번째 loss, accuracy:  0.525223998364332 0.7\n",
      "839 번째 loss, accuracy:  0.5250843312493705 0.7\n",
      "840 번째 loss, accuracy:  0.5249449964376833 0.7\n",
      "841 번째 loss, accuracy:  0.5248060722232687 0.7083333333333334\n",
      "842 번째 loss, accuracy:  0.5246675293100795 0.7083333333333334\n",
      "843 번째 loss, accuracy:  0.5245293609470129 0.7083333333333334\n",
      "844 번째 loss, accuracy:  0.5243914745529457 0.7083333333333334\n",
      "845 번째 loss, accuracy:  0.524253972299817 0.7083333333333334\n",
      "846 번째 loss, accuracy:  0.524116824284095 0.7083333333333334\n",
      "847 번째 loss, accuracy:  0.523980090974756 0.7083333333333334\n",
      "848 번째 loss, accuracy:  0.5238437151142588 0.7083333333333334\n",
      "849 번째 loss, accuracy:  0.5237076739444698 0.7083333333333334\n",
      "850 번째 loss, accuracy:  0.5235720199685416 0.7083333333333334\n",
      "851 번째 loss, accuracy:  0.5234366889136669 0.7083333333333334\n",
      "852 번째 loss, accuracy:  0.5233017226837987 0.7083333333333334\n",
      "853 번째 loss, accuracy:  0.5231671145042726 0.7083333333333334\n",
      "854 번째 loss, accuracy:  0.5230328686464596 0.7083333333333334\n",
      "855 번째 loss, accuracy:  0.5228989742826204 0.7083333333333334\n",
      "856 번째 loss, accuracy:  0.5227654342576311 0.7083333333333334\n",
      "857 번째 loss, accuracy:  0.522632237031655 0.7083333333333334\n",
      "858 번째 loss, accuracy:  0.522499385751289 0.7083333333333334\n",
      "859 번째 loss, accuracy:  0.522366879602599 0.7083333333333334\n",
      "860 번째 loss, accuracy:  0.5222347115278356 0.7083333333333334\n",
      "861 번째 loss, accuracy:  0.5221028828334356 0.7083333333333334\n",
      "862 번째 loss, accuracy:  0.5219713892602238 0.7083333333333334\n",
      "863 번째 loss, accuracy:  0.52184023686678 0.7083333333333334\n",
      "864 번째 loss, accuracy:  0.5217094119188163 0.7083333333333334\n",
      "865 번째 loss, accuracy:  0.5215789250502695 0.7083333333333334\n",
      "866 번째 loss, accuracy:  0.5214487655335039 0.7083333333333334\n",
      "867 번째 loss, accuracy:  0.5213189275699955 0.7083333333333334\n",
      "868 번째 loss, accuracy:  0.5211894344671372 0.7083333333333334\n",
      "869 번째 loss, accuracy:  0.5210602761923667 0.7083333333333334\n",
      "870 번째 loss, accuracy:  0.5209314190444052 0.7083333333333334\n",
      "871 번째 loss, accuracy:  0.5208028658779807 0.7083333333333334\n",
      "872 번째 loss, accuracy:  0.520674653029937 0.7083333333333334\n",
      "873 번째 loss, accuracy:  0.5205467967378683 0.7083333333333334\n",
      "874 번째 loss, accuracy:  0.5204192492447915 0.7083333333333334\n",
      "875 번째 loss, accuracy:  0.5202920004968369 0.7083333333333334\n",
      "876 번째 loss, accuracy:  0.5201650740162028 0.7083333333333334\n",
      "877 번째 loss, accuracy:  0.5200384622342535 0.7083333333333334\n",
      "878 번째 loss, accuracy:  0.5199121426539037 0.7083333333333334\n",
      "879 번째 loss, accuracy:  0.5197861701123428 0.7083333333333334\n",
      "880 번째 loss, accuracy:  0.5196605116244912 0.7083333333333334\n",
      "881 번째 loss, accuracy:  0.5195351568782113 0.7083333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882 번째 loss, accuracy:  0.5194101116947624 0.7083333333333334\n",
      "883 번째 loss, accuracy:  0.5192853632998562 0.7083333333333334\n",
      "884 번째 loss, accuracy:  0.5191609277653568 0.7083333333333334\n",
      "885 번째 loss, accuracy:  0.5190367653362138 0.7083333333333334\n",
      "886 번째 loss, accuracy:  0.5189129393827028 0.7083333333333334\n",
      "887 번째 loss, accuracy:  0.5187894508784795 0.7083333333333334\n",
      "888 번째 loss, accuracy:  0.5186662039338286 0.7083333333333334\n",
      "889 번째 loss, accuracy:  0.5185432615293142 0.7083333333333334\n",
      "890 번째 loss, accuracy:  0.5184206112212913 0.7083333333333334\n",
      "891 번째 loss, accuracy:  0.5182982734846286 0.7083333333333334\n",
      "892 번째 loss, accuracy:  0.5181762269434042 0.7083333333333334\n",
      "893 번째 loss, accuracy:  0.5180544885579382 0.7083333333333334\n",
      "894 번째 loss, accuracy:  0.5179330229740492 0.7083333333333334\n",
      "895 번째 loss, accuracy:  0.5178118421638787 0.7083333333333334\n",
      "896 번째 loss, accuracy:  0.5176909496268537 0.7083333333333334\n",
      "897 번째 loss, accuracy:  0.5175703707681495 0.7083333333333334\n",
      "898 번째 loss, accuracy:  0.5174500755101761 0.7083333333333334\n",
      "899 번째 loss, accuracy:  0.5173300339534027 0.7083333333333334\n",
      "900 번째 loss, accuracy:  0.51721029163244 0.7083333333333334\n",
      "901 번째 loss, accuracy:  0.5170908389680466 0.7083333333333334\n",
      "902 번째 loss, accuracy:  0.5169716363551023 0.7083333333333334\n",
      "903 번째 loss, accuracy:  0.5168527187780824 0.7083333333333334\n",
      "904 번째 loss, accuracy:  0.5167340626956868 0.7083333333333334\n",
      "905 번째 loss, accuracy:  0.5166157513598477 0.7083333333333334\n",
      "906 번째 loss, accuracy:  0.5164976909272144 0.7083333333333334\n",
      "907 번째 loss, accuracy:  0.5163798464561847 0.7083333333333334\n",
      "908 번째 loss, accuracy:  0.5162623270254301 0.7083333333333334\n",
      "909 번째 loss, accuracy:  0.5161450659819431 0.7083333333333334\n",
      "910 번째 loss, accuracy:  0.5160280781139975 0.7083333333333334\n",
      "911 번째 loss, accuracy:  0.5159113641324113 0.7083333333333334\n",
      "912 번째 loss, accuracy:  0.5157949347276986 0.7083333333333334\n",
      "913 번째 loss, accuracy:  0.5156787792521837 0.7083333333333334\n",
      "914 번째 loss, accuracy:  0.5155628649873308 0.7083333333333334\n",
      "915 번째 loss, accuracy:  0.5154472813584385 0.7083333333333334\n",
      "916 번째 loss, accuracy:  0.515331799127753 0.7083333333333334\n",
      "917 번째 loss, accuracy:  0.5152166159937814 0.7083333333333334\n",
      "918 번째 loss, accuracy:  0.515101733191647 0.7083333333333334\n",
      "919 번째 loss, accuracy:  0.5149871357428972 0.7083333333333334\n",
      "920 번째 loss, accuracy:  0.5148727554997602 0.7083333333333334\n",
      "921 번째 loss, accuracy:  0.5147586681627521 0.7083333333333334\n",
      "922 번째 loss, accuracy:  0.5146448311518114 0.7083333333333334\n",
      "923 번째 loss, accuracy:  0.5145312577207515 0.7083333333333334\n",
      "924 번째 loss, accuracy:  0.5144179382871269 0.7083333333333334\n",
      "925 번째 loss, accuracy:  0.5143048595868577 0.7083333333333334\n",
      "926 번째 loss, accuracy:  0.5141920411599863 0.7083333333333334\n",
      "927 번째 loss, accuracy:  0.5140794767268015 0.7083333333333334\n",
      "928 번째 loss, accuracy:  0.513967117912221 0.7083333333333334\n",
      "929 번째 loss, accuracy:  0.5138550261103415 0.7083333333333334\n",
      "930 번째 loss, accuracy:  0.5137431477859284 0.7083333333333334\n",
      "931 번째 loss, accuracy:  0.5136315421441384 0.7083333333333334\n",
      "932 번째 loss, accuracy:  0.5135201985215644 0.7083333333333334\n",
      "933 번째 loss, accuracy:  0.5134090927406515 0.7083333333333334\n",
      "934 번째 loss, accuracy:  0.5132982412484907 0.7083333333333334\n",
      "935 번째 loss, accuracy:  0.5131876216605143 0.7083333333333334\n",
      "936 번째 loss, accuracy:  0.5130772531916549 0.7083333333333334\n",
      "937 번째 loss, accuracy:  0.5129670809707436 0.7083333333333334\n",
      "938 번째 loss, accuracy:  0.5128571781222865 0.7083333333333334\n",
      "939 번째 loss, accuracy:  0.5127475013478872 0.7083333333333334\n",
      "940 번째 loss, accuracy:  0.5126380597678195 0.7083333333333334\n",
      "941 번째 loss, accuracy:  0.5125288378112375 0.7083333333333334\n",
      "942 번째 loss, accuracy:  0.5124198582724692 0.7083333333333334\n",
      "943 번째 loss, accuracy:  0.5123111189484114 0.7083333333333334\n",
      "944 번째 loss, accuracy:  0.5122025996837475 0.7083333333333334\n",
      "945 번째 loss, accuracy:  0.5120943167708943 0.7083333333333334\n",
      "946 번째 loss, accuracy:  0.5119862488319564 0.7083333333333334\n",
      "947 번째 loss, accuracy:  0.5118784270347958 0.7083333333333334\n",
      "948 번째 loss, accuracy:  0.5117708173770114 0.7166666666666667\n",
      "949 번째 loss, accuracy:  0.511663404260974 0.7166666666666667\n",
      "950 번째 loss, accuracy:  0.5115562389849317 0.7166666666666667\n",
      "951 번째 loss, accuracy:  0.5114493024982293 0.7166666666666667\n",
      "952 번째 loss, accuracy:  0.511342586487333 0.7166666666666667\n",
      "953 번째 loss, accuracy:  0.5112360835169178 0.7166666666666667\n",
      "954 번째 loss, accuracy:  0.5111297840899474 0.7166666666666667\n",
      "955 번째 loss, accuracy:  0.5110237006380917 0.7166666666666667\n",
      "956 번째 loss, accuracy:  0.510917842110723 0.7166666666666667\n",
      "957 번째 loss, accuracy:  0.5108122190576815 0.7166666666666667\n",
      "958 번째 loss, accuracy:  0.5107068044081846 0.7166666666666667\n",
      "959 번째 loss, accuracy:  0.5106016005053046 0.7166666666666667\n",
      "960 번째 loss, accuracy:  0.5104966162202224 0.7166666666666667\n",
      "961 번째 loss, accuracy:  0.5103918591265239 0.7166666666666667\n",
      "962 번째 loss, accuracy:  0.5102872602898021 0.7166666666666667\n",
      "963 번째 loss, accuracy:  0.5101828801578864 0.7166666666666667\n",
      "964 번째 loss, accuracy:  0.5100787413764556 0.7166666666666667\n",
      "965 번째 loss, accuracy:  0.5099747959757952 0.7166666666666667\n",
      "966 번째 loss, accuracy:  0.5098710697345222 0.7166666666666667\n",
      "967 번째 loss, accuracy:  0.5097675497180171 0.7166666666666667\n",
      "968 번째 loss, accuracy:  0.5096641911475522 0.7166666666666667\n",
      "969 번째 loss, accuracy:  0.509561068267251 0.7166666666666667\n",
      "970 번째 loss, accuracy:  0.5094581640102284 0.7166666666666667\n",
      "971 번째 loss, accuracy:  0.509355435524577 0.7166666666666667\n",
      "972 번째 loss, accuracy:  0.509252917088899 0.7166666666666667\n",
      "973 번째 loss, accuracy:  0.5091506077228566 0.7166666666666667\n",
      "974 번째 loss, accuracy:  0.5090485034745914 0.7166666666666667\n",
      "975 번째 loss, accuracy:  0.5089465761686619 0.7166666666666667\n",
      "976 번째 loss, accuracy:  0.5088448612521109 0.7166666666666667\n",
      "977 번째 loss, accuracy:  0.5087433302295589 0.7166666666666667\n",
      "978 번째 loss, accuracy:  0.508642025509903 0.7166666666666667\n",
      "979 번째 loss, accuracy:  0.5085408784967463 0.7166666666666667\n",
      "980 번째 loss, accuracy:  0.5084399064978712 0.7166666666666667\n",
      "981 번째 loss, accuracy:  0.50833914283685 0.7166666666666667\n",
      "982 번째 loss, accuracy:  0.5082385801145998 0.7166666666666667\n",
      "983 번째 loss, accuracy:  0.5081382076775073 0.7166666666666667\n",
      "984 번째 loss, accuracy:  0.5080380212304392 0.7166666666666667\n",
      "985 번째 loss, accuracy:  0.507937998167063 0.7166666666666667\n",
      "986 번째 loss, accuracy:  0.5078381686635919 0.7166666666666667\n",
      "987 번째 loss, accuracy:  0.5077385360747773 0.7166666666666667\n",
      "988 번째 loss, accuracy:  0.507639092924022 0.7166666666666667\n",
      "989 번째 loss, accuracy:  0.507539834405491 0.7166666666666667\n",
      "990 번째 loss, accuracy:  0.5074407637528054 0.7166666666666667\n",
      "991 번째 loss, accuracy:  0.5073418629007457 0.7166666666666667\n",
      "992 번째 loss, accuracy:  0.507243150769297 0.7166666666666667\n",
      "993 번째 loss, accuracy:  0.5071446183227394 0.7166666666666667\n",
      "994 번째 loss, accuracy:  0.5070462449745337 0.7166666666666667\n",
      "995 번째 loss, accuracy:  0.5069480764601837 0.7166666666666667\n",
      "996 번째 loss, accuracy:  0.5068500756946709 0.7166666666666667\n",
      "997 번째 loss, accuracy:  0.5067522434208201 0.725\n",
      "998 번째 loss, accuracy:  0.5066546111814008 0.725\n",
      "999 번째 loss, accuracy:  0.506557115610954 0.725\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 loss, accuracy:  1.1764474906793354 0.35833333333333334\n",
      "1 번째 loss, accuracy:  1.1567106890928271 0.35833333333333334\n",
      "2 번째 loss, accuracy:  1.1410084548139565 0.35833333333333334\n",
      "3 번째 loss, accuracy:  1.128443100691734 0.35833333333333334\n",
      "4 번째 loss, accuracy:  1.1182133039015436 0.35833333333333334\n",
      "5 번째 loss, accuracy:  1.1097911420252307 0.35833333333333334\n",
      "6 번째 loss, accuracy:  1.1027152395012996 0.35833333333333334\n",
      "7 번째 loss, accuracy:  1.0966397511756076 0.35833333333333334\n",
      "8 번째 loss, accuracy:  1.0913118244092639 0.35833333333333334\n",
      "9 번째 loss, accuracy:  1.0866063583459384 0.35833333333333334\n",
      "10 번째 loss, accuracy:  1.0823313996064863 0.35833333333333334\n",
      "11 번째 loss, accuracy:  1.0784568566127823 0.35833333333333334\n",
      "12 번째 loss, accuracy:  1.0748780647129768 0.35833333333333334\n",
      "13 번째 loss, accuracy:  1.0715302724472593 0.35833333333333334\n",
      "14 번째 loss, accuracy:  1.0683785456805976 0.35833333333333334\n",
      "15 번째 loss, accuracy:  1.0654000650210456 0.35833333333333334\n",
      "16 번째 loss, accuracy:  1.0625812027132853 0.35833333333333334\n",
      "17 번째 loss, accuracy:  1.059870991724897 0.35833333333333334\n",
      "18 번째 loss, accuracy:  1.0572596682619482 0.35833333333333334\n",
      "19 번째 loss, accuracy:  1.054731546294086 0.35833333333333334\n",
      "20 번째 loss, accuracy:  1.0522966692376305 0.35833333333333334\n",
      "21 번째 loss, accuracy:  1.0499118027846626 0.35833333333333334\n",
      "22 번째 loss, accuracy:  1.0475974957705256 0.35833333333333334\n",
      "23 번째 loss, accuracy:  1.0453409661890989 0.35833333333333334\n",
      "24 번째 loss, accuracy:  1.0431339518667648 0.35833333333333334\n",
      "25 번째 loss, accuracy:  1.040976965498992 0.35833333333333334\n",
      "26 번째 loss, accuracy:  1.0388524672186763 0.35833333333333334\n",
      "27 번째 loss, accuracy:  1.0367580819301407 0.35833333333333334\n",
      "28 번째 loss, accuracy:  1.034699720934255 0.35833333333333334\n",
      "29 번째 loss, accuracy:  1.0326655325959768 0.35833333333333334\n",
      "30 번째 loss, accuracy:  1.0306670371290318 0.35833333333333334\n",
      "31 번째 loss, accuracy:  1.0286940501912127 0.35833333333333334\n",
      "32 번째 loss, accuracy:  1.026745550276824 0.35833333333333334\n",
      "33 번째 loss, accuracy:  1.0248190749979518 0.35833333333333334\n",
      "34 번째 loss, accuracy:  1.02291195897976 0.35833333333333334\n",
      "35 번째 loss, accuracy:  1.0210289899196328 0.35833333333333334\n",
      "36 번째 loss, accuracy:  1.0191622927764925 0.35833333333333334\n",
      "37 번째 loss, accuracy:  1.0173134375350779 0.35833333333333334\n",
      "38 번째 loss, accuracy:  1.0154833407625572 0.35833333333333334\n",
      "39 번째 loss, accuracy:  1.0136671863893334 0.35833333333333334\n",
      "40 번째 loss, accuracy:  1.0118658980165396 0.35833333333333334\n",
      "41 번째 loss, accuracy:  1.010080752014989 0.35833333333333334\n",
      "42 번째 loss, accuracy:  1.0083105000442256 0.36666666666666664\n",
      "43 번째 loss, accuracy:  1.0065548114937823 0.36666666666666664\n",
      "44 번째 loss, accuracy:  1.0048076265552273 0.36666666666666664\n",
      "45 번째 loss, accuracy:  1.003076569580069 0.375\n",
      "46 번째 loss, accuracy:  1.0013572297578688 0.38333333333333336\n",
      "47 번째 loss, accuracy:  0.9996494699617142 0.38333333333333336\n",
      "48 번째 loss, accuracy:  0.9979532270698495 0.38333333333333336\n",
      "49 번째 loss, accuracy:  0.9962688867801004 0.38333333333333336\n",
      "50 번째 loss, accuracy:  0.994594257076099 0.38333333333333336\n",
      "51 번째 loss, accuracy:  0.9929320301169572 0.38333333333333336\n",
      "52 번째 loss, accuracy:  0.9912782115429466 0.38333333333333336\n",
      "53 번째 loss, accuracy:  0.9896351269954896 0.38333333333333336\n",
      "54 번째 loss, accuracy:  0.9880026790101355 0.38333333333333336\n",
      "55 번째 loss, accuracy:  0.9863792772804056 0.38333333333333336\n",
      "56 번째 loss, accuracy:  0.9847622798322025 0.38333333333333336\n",
      "57 번째 loss, accuracy:  0.9831548754501216 0.39166666666666666\n",
      "58 번째 loss, accuracy:  0.9815569639260793 0.39166666666666666\n",
      "59 번째 loss, accuracy:  0.9799667880745971 0.4083333333333333\n",
      "60 번째 loss, accuracy:  0.9783839670771927 0.4083333333333333\n",
      "61 번째 loss, accuracy:  0.976809134662504 0.4083333333333333\n",
      "62 번째 loss, accuracy:  0.9752403674230663 0.4166666666666667\n",
      "63 번째 loss, accuracy:  0.9736808467324886 0.4166666666666667\n",
      "64 번째 loss, accuracy:  0.9721282244953016 0.4166666666666667\n",
      "65 번째 loss, accuracy:  0.9705814887429809 0.4166666666666667\n",
      "66 번째 loss, accuracy:  0.9690418692815206 0.4166666666666667\n",
      "67 번째 loss, accuracy:  0.9675109879159174 0.4166666666666667\n",
      "68 번째 loss, accuracy:  0.9659864905508909 0.425\n",
      "69 번째 loss, accuracy:  0.9644662029835163 0.425\n",
      "70 번째 loss, accuracy:  0.962951046317972 0.43333333333333335\n",
      "71 번째 loss, accuracy:  0.9614428655093388 0.43333333333333335\n",
      "72 번째 loss, accuracy:  0.9599402451652463 0.44166666666666665\n",
      "73 번째 loss, accuracy:  0.9584429905540013 0.44166666666666665\n",
      "74 번째 loss, accuracy:  0.9569510790645294 0.4583333333333333\n",
      "75 번째 loss, accuracy:  0.9554653658522998 0.4583333333333333\n",
      "76 번째 loss, accuracy:  0.9539849123908337 0.475\n",
      "77 번째 loss, accuracy:  0.9525080231795634 0.475\n",
      "78 번째 loss, accuracy:  0.9510364371351522 0.48333333333333334\n",
      "79 번째 loss, accuracy:  0.9495698360107608 0.48333333333333334\n",
      "80 번째 loss, accuracy:  0.9481067792307969 0.5\n",
      "81 번째 loss, accuracy:  0.9466482071651673 0.5333333333333333\n",
      "82 번째 loss, accuracy:  0.9451953734755366 0.5916666666666667\n",
      "83 번째 loss, accuracy:  0.9437469125915056 0.5833333333333334\n",
      "84 번째 loss, accuracy:  0.9423020149007219 0.6083333333333333\n",
      "85 번째 loss, accuracy:  0.9408615846354955 0.6416666666666667\n",
      "86 번째 loss, accuracy:  0.9394256278304124 0.6333333333333333\n",
      "87 번째 loss, accuracy:  0.9379936825592953 0.625\n",
      "88 번째 loss, accuracy:  0.9365671318314193 0.625\n",
      "89 번째 loss, accuracy:  0.9351430975072428 0.625\n",
      "90 번째 loss, accuracy:  0.9337236157509622 0.625\n",
      "91 번째 loss, accuracy:  0.9323078234859961 0.6333333333333333\n",
      "92 번째 loss, accuracy:  0.9308959959065212 0.6416666666666667\n",
      "93 번째 loss, accuracy:  0.92948743205123 0.6416666666666667\n",
      "94 번째 loss, accuracy:  0.9280831509675913 0.6416666666666667\n",
      "95 번째 loss, accuracy:  0.9266821860443748 0.65\n",
      "96 번째 loss, accuracy:  0.9252842459373962 0.65\n",
      "97 번째 loss, accuracy:  0.9238910637068493 0.6583333333333333\n",
      "98 번째 loss, accuracy:  0.9224998675318365 0.675\n",
      "99 번째 loss, accuracy:  0.9211137000668503 0.675\n",
      "100 번째 loss, accuracy:  0.9197306609132494 0.675\n",
      "101 번째 loss, accuracy:  0.9183519635906673 0.6833333333333333\n",
      "102 번째 loss, accuracy:  0.9169764304749637 0.6833333333333333\n",
      "103 번째 loss, accuracy:  0.9156036687912168 0.6833333333333333\n",
      "104 번째 loss, accuracy:  0.9142346235210788 0.6833333333333333\n",
      "105 번째 loss, accuracy:  0.9128687449301628 0.6833333333333333\n",
      "106 번째 loss, accuracy:  0.9115064842446282 0.6833333333333333\n",
      "107 번째 loss, accuracy:  0.9101477861119759 0.6833333333333333\n",
      "108 번째 loss, accuracy:  0.9087929890576955 0.6833333333333333\n",
      "109 번째 loss, accuracy:  0.907441378517984 0.6833333333333333\n",
      "110 번째 loss, accuracy:  0.9060926785693938 0.6833333333333333\n",
      "111 번째 loss, accuracy:  0.9047479947688396 0.6833333333333333\n",
      "112 번째 loss, accuracy:  0.9034067810874118 0.6833333333333333\n",
      "113 번째 loss, accuracy:  0.9020687427535435 0.6916666666666667\n",
      "114 번째 loss, accuracy:  0.9007339262178303 0.6916666666666667\n",
      "115 번째 loss, accuracy:  0.8994030882441045 0.6916666666666667\n",
      "116 번째 loss, accuracy:  0.8980760719700683 0.6916666666666667\n",
      "117 번째 loss, accuracy:  0.8967516644164426 0.6916666666666667\n",
      "118 번째 loss, accuracy:  0.8954309869398813 0.6916666666666667\n",
      "119 번째 loss, accuracy:  0.8941131239988739 0.6916666666666667\n",
      "120 번째 loss, accuracy:  0.892799653167463 0.6916666666666667\n",
      "121 번째 loss, accuracy:  0.8914890620591908 0.6916666666666667\n",
      "122 번째 loss, accuracy:  0.890181554470804 0.6916666666666667\n",
      "123 번째 loss, accuracy:  0.8888777902844711 0.6916666666666667\n",
      "124 번째 loss, accuracy:  0.8875778646582115 0.6916666666666667\n",
      "125 번째 loss, accuracy:  0.8862812218802015 0.6916666666666667\n",
      "126 번째 loss, accuracy:  0.884987296651622 0.6916666666666667\n",
      "127 번째 loss, accuracy:  0.8836977284965869 0.6916666666666667\n",
      "128 번째 loss, accuracy:  0.8824115397706431 0.6916666666666667\n",
      "129 번째 loss, accuracy:  0.8811288676618431 0.6916666666666667\n",
      "130 번째 loss, accuracy:  0.8798494287060278 0.6916666666666667\n",
      "131 번째 loss, accuracy:  0.8785739795243128 0.6916666666666667\n",
      "132 번째 loss, accuracy:  0.8773019361067896 0.6916666666666667\n",
      "133 번째 loss, accuracy:  0.8760335857475847 0.6916666666666667\n",
      "134 번째 loss, accuracy:  0.8747680070222794 0.6916666666666667\n",
      "135 번째 loss, accuracy:  0.8735064458994678 0.6916666666666667\n",
      "136 번째 loss, accuracy:  0.8722488770984722 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 번째 loss, accuracy:  0.8709947257679029 0.6916666666666667\n",
      "138 번째 loss, accuracy:  0.8697444039195013 0.6916666666666667\n",
      "139 번째 loss, accuracy:  0.8684975665127049 0.6916666666666667\n",
      "140 번째 loss, accuracy:  0.867253708199837 0.6916666666666667\n",
      "141 번째 loss, accuracy:  0.8660139672941038 0.6916666666666667\n",
      "142 번째 loss, accuracy:  0.8647779462791498 0.6916666666666667\n",
      "143 번째 loss, accuracy:  0.863545646512973 0.6916666666666667\n",
      "144 번째 loss, accuracy:  0.8623169322873917 0.6916666666666667\n",
      "145 번째 loss, accuracy:  0.8610921122343658 0.6916666666666667\n",
      "146 번째 loss, accuracy:  0.8598707623528189 0.6916666666666667\n",
      "147 번째 loss, accuracy:  0.8586532518756951 0.6916666666666667\n",
      "148 번째 loss, accuracy:  0.8574391104450264 0.6916666666666667\n",
      "149 번째 loss, accuracy:  0.856228739804713 0.6916666666666667\n",
      "150 번째 loss, accuracy:  0.8550220482381062 0.6916666666666667\n",
      "151 번째 loss, accuracy:  0.8538191383389059 0.6916666666666667\n",
      "152 번째 loss, accuracy:  0.8526199437200374 0.6916666666666667\n",
      "153 번째 loss, accuracy:  0.8514246819898957 0.6916666666666667\n",
      "154 번째 loss, accuracy:  0.8502332522312609 0.6916666666666667\n",
      "155 번째 loss, accuracy:  0.8490455164538049 0.6916666666666667\n",
      "156 번째 loss, accuracy:  0.8478609765886559 0.6916666666666667\n",
      "157 번째 loss, accuracy:  0.8466803667634086 0.6916666666666667\n",
      "158 번째 loss, accuracy:  0.8455038857850368 0.6916666666666667\n",
      "159 번째 loss, accuracy:  0.844330789462898 0.6916666666666667\n",
      "160 번째 loss, accuracy:  0.8431619726265647 0.6916666666666667\n",
      "161 번째 loss, accuracy:  0.8419969605209869 0.6916666666666667\n",
      "162 번째 loss, accuracy:  0.8408354089280787 0.6916666666666667\n",
      "163 번째 loss, accuracy:  0.8396777879857205 0.6916666666666667\n",
      "164 번째 loss, accuracy:  0.83852411369088 0.6916666666666667\n",
      "165 번째 loss, accuracy:  0.8373742928856807 0.6916666666666667\n",
      "166 번째 loss, accuracy:  0.8362282623741284 0.6916666666666667\n",
      "167 번째 loss, accuracy:  0.8350861323350126 0.6916666666666667\n",
      "168 번째 loss, accuracy:  0.8339478606700651 0.6916666666666667\n",
      "169 번째 loss, accuracy:  0.8328135152348781 0.6916666666666667\n",
      "170 번째 loss, accuracy:  0.831682991308002 0.6916666666666667\n",
      "171 번째 loss, accuracy:  0.8305561973586383 0.6916666666666667\n",
      "172 번째 loss, accuracy:  0.829433235247199 0.6916666666666667\n",
      "173 번째 loss, accuracy:  0.8283141356457386 0.6916666666666667\n",
      "174 번째 loss, accuracy:  0.8271989907527216 0.6916666666666667\n",
      "175 번째 loss, accuracy:  0.8260875559480539 0.6916666666666667\n",
      "176 번째 loss, accuracy:  0.8249801573969837 0.6916666666666667\n",
      "177 번째 loss, accuracy:  0.8238765815006747 0.6916666666666667\n",
      "178 번째 loss, accuracy:  0.8227764554302621 0.6916666666666667\n",
      "179 번째 loss, accuracy:  0.8216803987966182 0.6916666666666667\n",
      "180 번째 loss, accuracy:  0.820588257573472 0.6916666666666667\n",
      "181 번째 loss, accuracy:  0.8194999822248511 0.6916666666666667\n",
      "182 번째 loss, accuracy:  0.8184154833487159 0.6916666666666667\n",
      "183 번째 loss, accuracy:  0.8173344495255244 0.6916666666666667\n",
      "184 번째 loss, accuracy:  0.8162577334376304 0.6916666666666667\n",
      "185 번째 loss, accuracy:  0.8151847199177474 0.6916666666666667\n",
      "186 번째 loss, accuracy:  0.8141157610833896 0.6916666666666667\n",
      "187 번째 loss, accuracy:  0.8130506040322211 0.6916666666666667\n",
      "188 번째 loss, accuracy:  0.8119892054167358 0.6916666666666667\n",
      "189 번째 loss, accuracy:  0.8109317042798629 0.6916666666666667\n",
      "190 번째 loss, accuracy:  0.8098777532970833 0.6916666666666667\n",
      "191 번째 loss, accuracy:  0.8088278729078789 0.6916666666666667\n",
      "192 번째 loss, accuracy:  0.8077819195048631 0.6916666666666667\n",
      "193 번째 loss, accuracy:  0.80673971885494 0.6916666666666667\n",
      "194 번째 loss, accuracy:  0.8057010290741784 0.6916666666666667\n",
      "195 번째 loss, accuracy:  0.804666509801263 0.6916666666666667\n",
      "196 번째 loss, accuracy:  0.8036355390457339 0.6916666666666667\n",
      "197 번째 loss, accuracy:  0.8026086589972664 0.6916666666666667\n",
      "198 번째 loss, accuracy:  0.8015850090515746 0.6916666666666667\n",
      "199 번째 loss, accuracy:  0.8005651777981394 0.6916666666666667\n",
      "200 번째 loss, accuracy:  0.7995495523529129 0.6916666666666667\n",
      "201 번째 loss, accuracy:  0.7985377075097141 0.6916666666666667\n",
      "202 번째 loss, accuracy:  0.7975296732521058 0.6916666666666667\n",
      "203 번째 loss, accuracy:  0.7965253990268358 0.6916666666666667\n",
      "204 번째 loss, accuracy:  0.7955245293184772 0.6916666666666667\n",
      "205 번째 loss, accuracy:  0.7945277090578778 0.6916666666666667\n",
      "206 번째 loss, accuracy:  0.7935346764126646 0.6916666666666667\n",
      "207 번째 loss, accuracy:  0.7925454071520377 0.6916666666666667\n",
      "208 번째 loss, accuracy:  0.7915597831859282 0.6916666666666667\n",
      "209 번째 loss, accuracy:  0.790578051139232 0.6916666666666667\n",
      "210 번째 loss, accuracy:  0.7895999874596497 0.6916666666666667\n",
      "211 번째 loss, accuracy:  0.788625562967808 0.6916666666666667\n",
      "212 번째 loss, accuracy:  0.7876548805602127 0.6916666666666667\n",
      "213 번째 loss, accuracy:  0.7866879578001277 0.6916666666666667\n",
      "214 번째 loss, accuracy:  0.7857244824027544 0.6916666666666667\n",
      "215 번째 loss, accuracy:  0.7847647788403617 0.6916666666666667\n",
      "216 번째 loss, accuracy:  0.7838088703093743 0.6916666666666667\n",
      "217 번째 loss, accuracy:  0.7828566412549447 0.6916666666666667\n",
      "218 번째 loss, accuracy:  0.7819080110595681 0.6916666666666667\n",
      "219 번째 loss, accuracy:  0.7809622388598005 0.6916666666666667\n",
      "220 번째 loss, accuracy:  0.7800209152040313 0.6916666666666667\n",
      "221 번째 loss, accuracy:  0.7790830422589579 0.6916666666666667\n",
      "222 번째 loss, accuracy:  0.7781490251595538 0.6916666666666667\n",
      "223 번째 loss, accuracy:  0.777218572167041 0.6916666666666667\n",
      "224 번째 loss, accuracy:  0.7762917235538864 0.6916666666666667\n",
      "225 번째 loss, accuracy:  0.7753684071208535 0.6916666666666667\n",
      "226 번째 loss, accuracy:  0.7744487021337346 0.6916666666666667\n",
      "227 번째 loss, accuracy:  0.7735325098234195 0.6916666666666667\n",
      "228 번째 loss, accuracy:  0.7726199398892233 0.6916666666666667\n",
      "229 번째 loss, accuracy:  0.7717108553955361 0.6916666666666667\n",
      "230 번째 loss, accuracy:  0.7708053033981085 0.6916666666666667\n",
      "231 번째 loss, accuracy:  0.7699028302333542 0.6916666666666667\n",
      "232 번째 loss, accuracy:  0.7690044079638951 0.6916666666666667\n",
      "233 번째 loss, accuracy:  0.7681093884319068 0.6916666666666667\n",
      "234 번째 loss, accuracy:  0.7672179092462454 0.6916666666666667\n",
      "235 번째 loss, accuracy:  0.7663298645680534 0.6916666666666667\n",
      "236 번째 loss, accuracy:  0.7654453433403258 0.6916666666666667\n",
      "237 번째 loss, accuracy:  0.7645643165833821 0.6916666666666667\n",
      "238 번째 loss, accuracy:  0.7636867723871792 0.6916666666666667\n",
      "239 번째 loss, accuracy:  0.7628126484831862 0.6916666666666667\n",
      "240 번째 loss, accuracy:  0.7619419123408858 0.6916666666666667\n",
      "241 번째 loss, accuracy:  0.7610746845610694 0.6916666666666667\n",
      "242 번째 loss, accuracy:  0.7602107917461688 0.6916666666666667\n",
      "243 번째 loss, accuracy:  0.759350161952218 0.6916666666666667\n",
      "244 번째 loss, accuracy:  0.7584928313301611 0.6916666666666667\n",
      "245 번째 loss, accuracy:  0.7576390341521179 0.6916666666666667\n",
      "246 번째 loss, accuracy:  0.7567886195788203 0.6916666666666667\n",
      "247 번째 loss, accuracy:  0.755941538884602 0.6916666666666667\n",
      "248 번째 loss, accuracy:  0.7550979121449086 0.6916666666666667\n",
      "249 번째 loss, accuracy:  0.7542575324979038 0.6916666666666667\n",
      "250 번째 loss, accuracy:  0.7534205677450387 0.6916666666666667\n",
      "251 번째 loss, accuracy:  0.7525868404092738 0.6916666666666667\n",
      "252 번째 loss, accuracy:  0.7517564613019158 0.6916666666666667\n",
      "253 번째 loss, accuracy:  0.7509293360316968 0.6916666666666667\n",
      "254 번째 loss, accuracy:  0.7501053183475466 0.6916666666666667\n",
      "255 번째 loss, accuracy:  0.7492842801493153 0.6916666666666667\n",
      "256 번째 loss, accuracy:  0.7484669776014109 0.6916666666666667\n",
      "257 번째 loss, accuracy:  0.7476527568334108 0.6916666666666667\n",
      "258 번째 loss, accuracy:  0.7468417916159111 0.6916666666666667\n",
      "259 번째 loss, accuracy:  0.7460340990312008 0.6916666666666667\n",
      "260 번째 loss, accuracy:  0.7452294003533199 0.6916666666666667\n",
      "261 번째 loss, accuracy:  0.7444277739833615 0.6916666666666667\n",
      "262 번째 loss, accuracy:  0.743629612897535 0.6916666666666667\n",
      "263 번째 loss, accuracy:  0.7428345063018391 0.6916666666666667\n",
      "264 번째 loss, accuracy:  0.7420424544911614 0.6916666666666667\n",
      "265 번째 loss, accuracy:  0.7412537629500925 0.6916666666666667\n",
      "266 번째 loss, accuracy:  0.7404679608711289 0.6916666666666667\n",
      "267 번째 loss, accuracy:  0.7396854896779572 0.6916666666666667\n",
      "268 번째 loss, accuracy:  0.7389060771482645 0.6916666666666667\n",
      "269 번째 loss, accuracy:  0.7381297575997435 0.6916666666666667\n",
      "270 번째 loss, accuracy:  0.7373565207780999 0.6916666666666667\n",
      "271 번째 loss, accuracy:  0.7365863211617625 0.6916666666666667\n",
      "272 번째 loss, accuracy:  0.7358188472284675 0.6916666666666667\n",
      "273 번째 loss, accuracy:  0.7350547916604768 0.6916666666666667\n",
      "274 번째 loss, accuracy:  0.7342937325992713 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 번째 loss, accuracy:  0.7335355462402201 0.6916666666666667\n",
      "276 번째 loss, accuracy:  0.7327805020922968 0.6916666666666667\n",
      "277 번째 loss, accuracy:  0.7320284636904985 0.6916666666666667\n",
      "278 번째 loss, accuracy:  0.7312792999362085 0.6916666666666667\n",
      "279 번째 loss, accuracy:  0.7305330513512079 0.6916666666666667\n",
      "280 번째 loss, accuracy:  0.729789928973408 0.6916666666666667\n",
      "281 번째 loss, accuracy:  0.7290495996171005 0.6916666666666667\n",
      "282 번째 loss, accuracy:  0.7283123340627977 0.6916666666666667\n",
      "283 번째 loss, accuracy:  0.7275779618123593 0.6916666666666667\n",
      "284 번째 loss, accuracy:  0.726846507150357 0.6916666666666667\n",
      "285 번째 loss, accuracy:  0.7261179603380754 0.6916666666666667\n",
      "286 번째 loss, accuracy:  0.7253921301587666 0.6916666666666667\n",
      "287 번째 loss, accuracy:  0.7246692977982725 0.6916666666666667\n",
      "288 번째 loss, accuracy:  0.7239493122380216 0.6916666666666667\n",
      "289 번째 loss, accuracy:  0.7232321399826611 0.6916666666666667\n",
      "290 번째 loss, accuracy:  0.722517860148875 0.6916666666666667\n",
      "291 번째 loss, accuracy:  0.7218062534442656 0.6916666666666667\n",
      "292 번째 loss, accuracy:  0.7210975140674262 0.6916666666666667\n",
      "293 번째 loss, accuracy:  0.720391654686789 0.6916666666666667\n",
      "294 번째 loss, accuracy:  0.7196882941447718 0.6916666666666667\n",
      "295 번째 loss, accuracy:  0.7189879763689421 0.6916666666666667\n",
      "296 번째 loss, accuracy:  0.7182903904804154 0.6916666666666667\n",
      "297 번째 loss, accuracy:  0.7175954312988554 0.6916666666666667\n",
      "298 번째 loss, accuracy:  0.7169033572354953 0.6916666666666667\n",
      "299 번째 loss, accuracy:  0.7162140032831045 0.6916666666666667\n",
      "300 번째 loss, accuracy:  0.7155273660401588 0.6916666666666667\n",
      "301 번째 loss, accuracy:  0.714843384396414 0.6916666666666667\n",
      "302 번째 loss, accuracy:  0.7141619757721939 0.6916666666666667\n",
      "303 번째 loss, accuracy:  0.7134833987210302 0.6916666666666667\n",
      "304 번째 loss, accuracy:  0.7128073765826016 0.6916666666666667\n",
      "305 번째 loss, accuracy:  0.7121340573664181 0.6916666666666667\n",
      "306 번째 loss, accuracy:  0.7114633503236313 0.6916666666666667\n",
      "307 번째 loss, accuracy:  0.7107952515001313 0.6916666666666667\n",
      "308 번째 loss, accuracy:  0.7101298231343961 0.6916666666666667\n",
      "309 번째 loss, accuracy:  0.7094669912181808 0.6916666666666667\n",
      "310 번째 loss, accuracy:  0.7088067586625253 0.6916666666666667\n",
      "311 번째 loss, accuracy:  0.7081491670913571 0.6916666666666667\n",
      "312 번째 loss, accuracy:  0.7074941578421335 0.6916666666666667\n",
      "313 번째 loss, accuracy:  0.7068416796074101 0.6916666666666667\n",
      "314 번째 loss, accuracy:  0.7061917005411874 0.6916666666666667\n",
      "315 번째 loss, accuracy:  0.7055443080332492 0.6916666666666667\n",
      "316 번째 loss, accuracy:  0.7048994226778525 0.6916666666666667\n",
      "317 번째 loss, accuracy:  0.7042570868916225 0.6916666666666667\n",
      "318 번째 loss, accuracy:  0.7036171762312665 0.6916666666666667\n",
      "319 번째 loss, accuracy:  0.7029798468100511 0.6916666666666667\n",
      "320 번째 loss, accuracy:  0.702344907326365 0.6916666666666667\n",
      "321 번째 loss, accuracy:  0.7017125103424784 0.6916666666666667\n",
      "322 번째 loss, accuracy:  0.7010825641985678 0.6916666666666667\n",
      "323 번째 loss, accuracy:  0.7004550173474214 0.6916666666666667\n",
      "324 번째 loss, accuracy:  0.6998298914396632 0.6916666666666667\n",
      "325 번째 loss, accuracy:  0.6992072188994688 0.6916666666666667\n",
      "326 번째 loss, accuracy:  0.6985864681517158 0.6916666666666667\n",
      "327 번째 loss, accuracy:  0.6979686257093072 0.6916666666666667\n",
      "328 번째 loss, accuracy:  0.6973532106714954 0.6916666666666667\n",
      "329 번째 loss, accuracy:  0.6967400072201763 0.6916666666666667\n",
      "330 번째 loss, accuracy:  0.6961292306482098 0.6916666666666667\n",
      "331 번째 loss, accuracy:  0.6955208657065272 0.6916666666666667\n",
      "332 번째 loss, accuracy:  0.6949149081580649 0.6916666666666667\n",
      "333 번째 loss, accuracy:  0.6943111989812344 0.6916666666666667\n",
      "334 번째 loss, accuracy:  0.6937099044034055 0.6916666666666667\n",
      "335 번째 loss, accuracy:  0.6931109268396501 0.6916666666666667\n",
      "336 번째 loss, accuracy:  0.6925142503380537 0.6916666666666667\n",
      "337 번째 loss, accuracy:  0.6919198901007663 0.6916666666666667\n",
      "338 번째 loss, accuracy:  0.6913278053744653 0.6916666666666667\n",
      "339 번째 loss, accuracy:  0.6907379913664771 0.6916666666666667\n",
      "340 번째 loss, accuracy:  0.6901504224826496 0.6916666666666667\n",
      "341 번째 loss, accuracy:  0.689565110635903 0.6916666666666667\n",
      "342 번째 loss, accuracy:  0.6889820177693707 0.6916666666666667\n",
      "343 번째 loss, accuracy:  0.6884011911500036 0.6916666666666667\n",
      "344 번째 loss, accuracy:  0.6878225720547176 0.6916666666666667\n",
      "345 번째 loss, accuracy:  0.6872461554853029 0.6916666666666667\n",
      "346 번째 loss, accuracy:  0.6866718633950808 0.6916666666666667\n",
      "347 번째 loss, accuracy:  0.6860998479996406 0.6916666666666667\n",
      "348 번째 loss, accuracy:  0.6855300361468789 0.6916666666666667\n",
      "349 번째 loss, accuracy:  0.6849623632227858 0.6916666666666667\n",
      "350 번째 loss, accuracy:  0.6843968185720392 0.6916666666666667\n",
      "351 번째 loss, accuracy:  0.6838329897440141 0.6916666666666667\n",
      "352 번째 loss, accuracy:  0.6832717800821643 0.6916666666666667\n",
      "353 번째 loss, accuracy:  0.6827127032941003 0.6916666666666667\n",
      "354 번째 loss, accuracy:  0.6821557608314787 0.6916666666666667\n",
      "355 번째 loss, accuracy:  0.6816009306295494 0.6916666666666667\n",
      "356 번째 loss, accuracy:  0.681048204395823 0.6916666666666667\n",
      "357 번째 loss, accuracy:  0.6804975642441489 0.6916666666666667\n",
      "358 번째 loss, accuracy:  0.6799489917068573 0.6916666666666667\n",
      "359 번째 loss, accuracy:  0.6794025162351252 0.6916666666666667\n",
      "360 번째 loss, accuracy:  0.6788580876985385 0.6916666666666667\n",
      "361 번째 loss, accuracy:  0.6783157081445258 0.6916666666666667\n",
      "362 번째 loss, accuracy:  0.6777753818134241 0.6916666666666667\n",
      "363 번째 loss, accuracy:  0.6772370860207102 0.6916666666666667\n",
      "364 번째 loss, accuracy:  0.676700718464921 0.6916666666666667\n",
      "365 번째 loss, accuracy:  0.6761664475618835 0.6916666666666667\n",
      "366 번째 loss, accuracy:  0.6756341321843619 0.6916666666666667\n",
      "367 번째 loss, accuracy:  0.6751038562834348 0.6916666666666667\n",
      "368 번째 loss, accuracy:  0.6745755710331713 0.6916666666666667\n",
      "369 번째 loss, accuracy:  0.6740492001546795 0.6916666666666667\n",
      "370 번째 loss, accuracy:  0.6735246812399145 0.6916666666666667\n",
      "371 번째 loss, accuracy:  0.673002319421873 0.6916666666666667\n",
      "372 번째 loss, accuracy:  0.6724818880010207 0.6916666666666667\n",
      "373 번째 loss, accuracy:  0.671963421415393 0.6916666666666667\n",
      "374 번째 loss, accuracy:  0.671446800951203 0.6916666666666667\n",
      "375 번째 loss, accuracy:  0.6709320344128137 0.6916666666666667\n",
      "376 번째 loss, accuracy:  0.6704193335242602 0.6916666666666667\n",
      "377 번째 loss, accuracy:  0.6699085473250971 0.6916666666666667\n",
      "378 번째 loss, accuracy:  0.6693996413482405 0.6916666666666667\n",
      "379 번째 loss, accuracy:  0.6688926402236002 0.6916666666666667\n",
      "380 번째 loss, accuracy:  0.668387439485962 0.6916666666666667\n",
      "381 번째 loss, accuracy:  0.6678841829117156 0.6916666666666667\n",
      "382 번째 loss, accuracy:  0.6673827842183125 0.6916666666666667\n",
      "383 번째 loss, accuracy:  0.6668832408327787 0.6916666666666667\n",
      "384 번째 loss, accuracy:  0.6663855260023587 0.6916666666666667\n",
      "385 번째 loss, accuracy:  0.6658896548186213 0.6916666666666667\n",
      "386 번째 loss, accuracy:  0.6653955638031164 0.6916666666666667\n",
      "387 번째 loss, accuracy:  0.6649033488515563 0.6916666666666667\n",
      "388 번째 loss, accuracy:  0.6644129260543774 0.6916666666666667\n",
      "389 번째 loss, accuracy:  0.663924298224979 0.6916666666666667\n",
      "390 번째 loss, accuracy:  0.6634374197081558 0.6916666666666667\n",
      "391 번째 loss, accuracy:  0.6629523876766302 0.6916666666666667\n",
      "392 번째 loss, accuracy:  0.662468890843122 0.6916666666666667\n",
      "393 번째 loss, accuracy:  0.6619873999326503 0.6916666666666667\n",
      "394 번째 loss, accuracy:  0.6615076612587575 0.6916666666666667\n",
      "395 번째 loss, accuracy:  0.6610296801257252 0.6916666666666667\n",
      "396 번째 loss, accuracy:  0.6605534109817236 0.6916666666666667\n",
      "397 번째 loss, accuracy:  0.6600789053786031 0.6916666666666667\n",
      "398 번째 loss, accuracy:  0.6596061300816267 0.6916666666666667\n",
      "399 번째 loss, accuracy:  0.6591349457932111 0.6916666666666667\n",
      "400 번째 loss, accuracy:  0.6586655908199219 0.6916666666666667\n",
      "401 번째 loss, accuracy:  0.6581979127869853 0.6916666666666667\n",
      "402 번째 loss, accuracy:  0.6577319319551118 0.6916666666666667\n",
      "403 번째 loss, accuracy:  0.6572676735388969 0.6916666666666667\n",
      "404 번째 loss, accuracy:  0.6568050969612247 0.6916666666666667\n",
      "405 번째 loss, accuracy:  0.6563440876965372 0.6916666666666667\n",
      "406 번째 loss, accuracy:  0.6558848394081583 0.6916666666666667\n",
      "407 번째 loss, accuracy:  0.6554272528415618 0.6916666666666667\n",
      "408 번째 loss, accuracy:  0.6549713090576561 0.6916666666666667\n",
      "409 번째 loss, accuracy:  0.6545167826301932 0.6916666666666667\n",
      "410 번째 loss, accuracy:  0.654064076058679 0.6916666666666667\n",
      "411 번째 loss, accuracy:  0.653612964388982 0.6916666666666667\n",
      "412 번째 loss, accuracy:  0.6531635386697894 0.6916666666666667\n",
      "413 번째 loss, accuracy:  0.652715712566111 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414 번째 loss, accuracy:  0.6522694872161455 0.6916666666666667\n",
      "415 번째 loss, accuracy:  0.651824874918007 0.6916666666666667\n",
      "416 번째 loss, accuracy:  0.6513818533575491 0.6916666666666667\n",
      "417 번째 loss, accuracy:  0.6509404053538131 0.6916666666666667\n",
      "418 번째 loss, accuracy:  0.6505004645095892 0.6916666666666667\n",
      "419 번째 loss, accuracy:  0.6500621597363104 0.6916666666666667\n",
      "420 번째 loss, accuracy:  0.6496252569195747 0.6916666666666667\n",
      "421 번째 loss, accuracy:  0.6491900641188021 0.6916666666666667\n",
      "422 번째 loss, accuracy:  0.6487564175436482 0.6916666666666667\n",
      "423 번째 loss, accuracy:  0.6483240354834224 0.6916666666666667\n",
      "424 번째 loss, accuracy:  0.6478934250760756 0.6916666666666667\n",
      "425 번째 loss, accuracy:  0.64746433178753 0.6916666666666667\n",
      "426 번째 loss, accuracy:  0.647036772125125 0.6916666666666667\n",
      "427 번째 loss, accuracy:  0.6466106576083842 0.6916666666666667\n",
      "428 번째 loss, accuracy:  0.6461860949701982 0.6916666666666667\n",
      "429 번째 loss, accuracy:  0.6457630023505349 0.6916666666666667\n",
      "430 번째 loss, accuracy:  0.6453414373683622 0.6916666666666667\n",
      "431 번째 loss, accuracy:  0.6449213736611638 0.6916666666666667\n",
      "432 번째 loss, accuracy:  0.644502766222081 0.6916666666666667\n",
      "433 번째 loss, accuracy:  0.644085626742686 0.6916666666666667\n",
      "434 번째 loss, accuracy:  0.6436699575152967 0.6916666666666667\n",
      "435 번째 loss, accuracy:  0.6432557057158586 0.6916666666666667\n",
      "436 번째 loss, accuracy:  0.6428429295970446 0.6916666666666667\n",
      "437 번째 loss, accuracy:  0.6424315435920706 0.6916666666666667\n",
      "438 번째 loss, accuracy:  0.6420216323853821 0.6916666666666667\n",
      "439 번째 loss, accuracy:  0.6416131413559681 0.6916666666666667\n",
      "440 번째 loss, accuracy:  0.6412059535749445 0.6916666666666667\n",
      "441 번째 loss, accuracy:  0.6408003057127997 0.6916666666666667\n",
      "442 번째 loss, accuracy:  0.6403960837122235 0.6916666666666667\n",
      "443 번째 loss, accuracy:  0.6399932231678264 0.6916666666666667\n",
      "444 번째 loss, accuracy:  0.6395918024569196 0.6916666666666667\n",
      "445 번째 loss, accuracy:  0.6391916908724738 0.6916666666666667\n",
      "446 번째 loss, accuracy:  0.6387930420610528 0.6916666666666667\n",
      "447 번째 loss, accuracy:  0.6383957635939377 0.6916666666666667\n",
      "448 번째 loss, accuracy:  0.6379997896951842 0.6916666666666667\n",
      "449 번째 loss, accuracy:  0.6376052438402964 0.6916666666666667\n",
      "450 번째 loss, accuracy:  0.6372120373898137 0.6916666666666667\n",
      "451 번째 loss, accuracy:  0.6368202057861296 0.6916666666666667\n",
      "452 번째 loss, accuracy:  0.6364297093721656 0.6916666666666667\n",
      "453 번째 loss, accuracy:  0.6360405632083498 0.6916666666666667\n",
      "454 번째 loss, accuracy:  0.6356527115209222 0.6916666666666667\n",
      "455 번째 loss, accuracy:  0.6352662237127402 0.6916666666666667\n",
      "456 번째 loss, accuracy:  0.6348810461725108 0.6916666666666667\n",
      "457 번째 loss, accuracy:  0.634497180029724 0.6916666666666667\n",
      "458 번째 loss, accuracy:  0.6341146421073993 0.6916666666666667\n",
      "459 번째 loss, accuracy:  0.6337333764286343 0.6916666666666667\n",
      "460 번째 loss, accuracy:  0.6333534368757213 0.6916666666666667\n",
      "461 번째 loss, accuracy:  0.6329747199762875 0.6916666666666667\n",
      "462 번째 loss, accuracy:  0.6325973380520388 0.6916666666666667\n",
      "463 번째 loss, accuracy:  0.6322212500698247 0.6916666666666667\n",
      "464 번째 loss, accuracy:  0.6318463858783069 0.6916666666666667\n",
      "465 번째 loss, accuracy:  0.6314728125400378 0.6916666666666667\n",
      "466 번째 loss, accuracy:  0.6311005235173088 0.6916666666666667\n",
      "467 번째 loss, accuracy:  0.6307293501919742 0.6916666666666667\n",
      "468 번째 loss, accuracy:  0.6303595654990056 0.6916666666666667\n",
      "469 번째 loss, accuracy:  0.6299909146105123 0.6916666666666667\n",
      "470 번째 loss, accuracy:  0.6296235199760034 0.6916666666666667\n",
      "471 번째 loss, accuracy:  0.6292574580848608 0.6916666666666667\n",
      "472 번째 loss, accuracy:  0.6288926296334574 0.6916666666666667\n",
      "473 번째 loss, accuracy:  0.6285290173859749 0.6916666666666667\n",
      "474 번째 loss, accuracy:  0.628166629024191 0.6916666666666667\n",
      "475 번째 loss, accuracy:  0.6278054471901396 0.6916666666666667\n",
      "476 번째 loss, accuracy:  0.6274454674182343 0.6916666666666667\n",
      "477 번째 loss, accuracy:  0.6270866948657726 0.6916666666666667\n",
      "478 번째 loss, accuracy:  0.6267290928032904 0.6916666666666667\n",
      "479 번째 loss, accuracy:  0.6263726413956902 0.6916666666666667\n",
      "480 번째 loss, accuracy:  0.6260174099196315 0.6916666666666667\n",
      "481 번째 loss, accuracy:  0.6256633930343374 0.6916666666666667\n",
      "482 번째 loss, accuracy:  0.6253105453838637 0.6916666666666667\n",
      "483 번째 loss, accuracy:  0.6249588649388386 0.6916666666666667\n",
      "484 번째 loss, accuracy:  0.6246083502460072 0.6916666666666667\n",
      "485 번째 loss, accuracy:  0.624258982895776 0.6916666666666667\n",
      "486 번째 loss, accuracy:  0.6239105725160528 0.6916666666666667\n",
      "487 번째 loss, accuracy:  0.6235634548875937 0.6916666666666667\n",
      "488 번째 loss, accuracy:  0.6232175488210524 0.6916666666666667\n",
      "489 번째 loss, accuracy:  0.6228727822988812 0.6916666666666667\n",
      "490 번째 loss, accuracy:  0.6225291429299619 0.6916666666666667\n",
      "491 번째 loss, accuracy:  0.6221866370915816 0.6916666666666667\n",
      "492 번째 loss, accuracy:  0.6218451934998813 0.6916666666666667\n",
      "493 번째 loss, accuracy:  0.621504919562676 0.6916666666666667\n",
      "494 번째 loss, accuracy:  0.6211657456964379 0.6916666666666667\n",
      "495 번째 loss, accuracy:  0.6208277039692478 0.6916666666666667\n",
      "496 번째 loss, accuracy:  0.6204907576588677 0.6916666666666667\n",
      "497 번째 loss, accuracy:  0.6201549031858297 0.6916666666666667\n",
      "498 번째 loss, accuracy:  0.6198201269250784 0.6916666666666667\n",
      "499 번째 loss, accuracy:  0.6194864427561704 0.6916666666666667\n",
      "500 번째 loss, accuracy:  0.6191538478768559 0.6916666666666667\n",
      "501 번째 loss, accuracy:  0.6188223495612161 0.6916666666666667\n",
      "502 번째 loss, accuracy:  0.6184919038191743 0.6916666666666667\n",
      "503 번째 loss, accuracy:  0.6181625459799249 0.6916666666666667\n",
      "504 번째 loss, accuracy:  0.6178342591533464 0.6916666666666667\n",
      "505 번째 loss, accuracy:  0.6175070173169606 0.6916666666666667\n",
      "506 번째 loss, accuracy:  0.6171808299927373 0.6916666666666667\n",
      "507 번째 loss, accuracy:  0.6168557121375419 0.6916666666666667\n",
      "508 번째 loss, accuracy:  0.6165316418405911 0.6916666666666667\n",
      "509 번째 loss, accuracy:  0.6162085294616203 0.6916666666666667\n",
      "510 번째 loss, accuracy:  0.6158865196994451 0.6916666666666667\n",
      "511 번째 loss, accuracy:  0.615565519184501 0.6916666666666667\n",
      "512 번째 loss, accuracy:  0.6152455844096839 0.6916666666666667\n",
      "513 번째 loss, accuracy:  0.6149266702321833 0.6916666666666667\n",
      "514 번째 loss, accuracy:  0.6146087723451195 0.6916666666666667\n",
      "515 번째 loss, accuracy:  0.6142918826050037 0.6916666666666667\n",
      "516 번째 loss, accuracy:  0.6139760163018688 0.6916666666666667\n",
      "517 번째 loss, accuracy:  0.6136611327797122 0.6916666666666667\n",
      "518 번째 loss, accuracy:  0.6133472585053829 0.6916666666666667\n",
      "519 번째 loss, accuracy:  0.613034398041026 0.6916666666666667\n",
      "520 번째 loss, accuracy:  0.6127224468434069 0.6916666666666667\n",
      "521 번째 loss, accuracy:  0.612411562713208 0.6916666666666667\n",
      "522 번째 loss, accuracy:  0.6121016436025748 0.6916666666666667\n",
      "523 번째 loss, accuracy:  0.6117927274503109 0.6916666666666667\n",
      "524 번째 loss, accuracy:  0.6114847846139777 0.6916666666666667\n",
      "525 번째 loss, accuracy:  0.6111777971529707 0.6916666666666667\n",
      "526 번째 loss, accuracy:  0.6108717777275471 0.6916666666666667\n",
      "527 번째 loss, accuracy:  0.6105667307482981 0.6916666666666667\n",
      "528 번째 loss, accuracy:  0.6102625974878525 0.6916666666666667\n",
      "529 번째 loss, accuracy:  0.609959430466286 0.6916666666666667\n",
      "530 번째 loss, accuracy:  0.609657237881665 0.6916666666666667\n",
      "531 번째 loss, accuracy:  0.6093560102553952 0.6916666666666667\n",
      "532 번째 loss, accuracy:  0.6090557189506803 0.6916666666666667\n",
      "533 번째 loss, accuracy:  0.6087563693372038 0.6916666666666667\n",
      "534 번째 loss, accuracy:  0.6084579018404145 0.6916666666666667\n",
      "535 번째 loss, accuracy:  0.6081604170931771 0.6916666666666667\n",
      "536 번째 loss, accuracy:  0.6078638600145367 0.6916666666666667\n",
      "537 번째 loss, accuracy:  0.6075682024071866 0.6916666666666667\n",
      "538 번째 loss, accuracy:  0.6072734745721078 0.6916666666666667\n",
      "539 번째 loss, accuracy:  0.606979657090884 0.6916666666666667\n",
      "540 번째 loss, accuracy:  0.6066867572786954 0.6916666666666667\n",
      "541 번째 loss, accuracy:  0.6063947684022809 0.6916666666666667\n",
      "542 번째 loss, accuracy:  0.6061036383787862 0.6916666666666667\n",
      "543 번째 loss, accuracy:  0.6058134355538817 0.6916666666666667\n",
      "544 번째 loss, accuracy:  0.6055240787050202 0.6916666666666667\n",
      "545 번째 loss, accuracy:  0.6052356817384185 0.6916666666666667\n",
      "546 번째 loss, accuracy:  0.6049481623774824 0.6916666666666667\n",
      "547 번째 loss, accuracy:  0.6046615358188127 0.6916666666666667\n",
      "548 번째 loss, accuracy:  0.6043757860665391 0.6916666666666667\n",
      "549 번째 loss, accuracy:  0.604090912933862 0.6916666666666667\n",
      "550 번째 loss, accuracy:  0.6038068784305598 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551 번째 loss, accuracy:  0.6035237459893387 0.6916666666666667\n",
      "552 번째 loss, accuracy:  0.6032414779968173 0.6916666666666667\n",
      "553 번째 loss, accuracy:  0.6029600707223063 0.6916666666666667\n",
      "554 번째 loss, accuracy:  0.602679513310831 0.6916666666666667\n",
      "555 번째 loss, accuracy:  0.6023998179716084 0.6916666666666667\n",
      "556 번째 loss, accuracy:  0.6021209565044857 0.6916666666666667\n",
      "557 번째 loss, accuracy:  0.6018429592165956 0.6916666666666667\n",
      "558 번째 loss, accuracy:  0.6015657989279056 0.6916666666666667\n",
      "559 번째 loss, accuracy:  0.6012893921122138 0.6916666666666667\n",
      "560 번째 loss, accuracy:  0.6010139086901033 0.6916666666666667\n",
      "561 번째 loss, accuracy:  0.600739255686802 0.6916666666666667\n",
      "562 번째 loss, accuracy:  0.6004654378848837 0.6916666666666667\n",
      "563 번째 loss, accuracy:  0.6001924473462342 0.6916666666666667\n",
      "564 번째 loss, accuracy:  0.5999202546273461 0.6916666666666667\n",
      "565 번째 loss, accuracy:  0.5996489045247936 0.6916666666666667\n",
      "566 번째 loss, accuracy:  0.5993783731517515 0.6916666666666667\n",
      "567 번째 loss, accuracy:  0.5991086563171315 0.6916666666666667\n",
      "568 번째 loss, accuracy:  0.5988397391622933 0.6916666666666667\n",
      "569 번째 loss, accuracy:  0.5985716345461957 0.6916666666666667\n",
      "570 번째 loss, accuracy:  0.5983043122147184 0.6916666666666667\n",
      "571 번째 loss, accuracy:  0.598037803983974 0.6916666666666667\n",
      "572 번째 loss, accuracy:  0.5977720880188717 0.6916666666666667\n",
      "573 번째 loss, accuracy:  0.5975071718204122 0.6916666666666667\n",
      "574 번째 loss, accuracy:  0.5972430323441036 0.6916666666666667\n",
      "575 번째 loss, accuracy:  0.5969796858941602 0.6916666666666667\n",
      "576 번째 loss, accuracy:  0.5967170722447649 0.6916666666666667\n",
      "577 번째 loss, accuracy:  0.5964552866939644 0.6916666666666667\n",
      "578 번째 loss, accuracy:  0.5961942739771847 0.6916666666666667\n",
      "579 번째 loss, accuracy:  0.5959340362492095 0.6916666666666667\n",
      "580 번째 loss, accuracy:  0.5956745608620376 0.6916666666666667\n",
      "581 번째 loss, accuracy:  0.5954158460860948 0.6916666666666667\n",
      "582 번째 loss, accuracy:  0.5951579006855252 0.6916666666666667\n",
      "583 번째 loss, accuracy:  0.5949007163756419 0.6916666666666667\n",
      "584 번째 loss, accuracy:  0.5946442896362527 0.6916666666666667\n",
      "585 번째 loss, accuracy:  0.5943886206020764 0.6916666666666667\n",
      "586 번째 loss, accuracy:  0.594133688217097 0.6916666666666667\n",
      "587 번째 loss, accuracy:  0.5938794993130636 0.6916666666666667\n",
      "588 번째 loss, accuracy:  0.5936260665213212 0.6916666666666667\n",
      "589 번째 loss, accuracy:  0.5933733671093281 0.6916666666666667\n",
      "590 번째 loss, accuracy:  0.5931214146958654 0.6916666666666667\n",
      "591 번째 loss, accuracy:  0.5928701924902621 0.6916666666666667\n",
      "592 번째 loss, accuracy:  0.592619709904699 0.6916666666666667\n",
      "593 번째 loss, accuracy:  0.5923699192482734 0.6916666666666667\n",
      "594 번째 loss, accuracy:  0.5921208901161222 0.6916666666666667\n",
      "595 번째 loss, accuracy:  0.5918725830199972 0.6916666666666667\n",
      "596 번째 loss, accuracy:  0.5916249906906629 0.6916666666666667\n",
      "597 번째 loss, accuracy:  0.5913781225202915 0.6916666666666667\n",
      "598 번째 loss, accuracy:  0.5911319454365856 0.6916666666666667\n",
      "599 번째 loss, accuracy:  0.5908864994886028 0.6916666666666667\n",
      "600 번째 loss, accuracy:  0.5906417630254369 0.6916666666666667\n",
      "601 번째 loss, accuracy:  0.5903977206075152 0.6916666666666667\n",
      "602 번째 loss, accuracy:  0.5901543530325492 0.6916666666666667\n",
      "603 번째 loss, accuracy:  0.5899117210666067 0.6916666666666667\n",
      "604 번째 loss, accuracy:  0.5896697821886995 0.6916666666666667\n",
      "605 번째 loss, accuracy:  0.5894285263279462 0.6916666666666667\n",
      "606 번째 loss, accuracy:  0.5891879342748964 0.6916666666666667\n",
      "607 번째 loss, accuracy:  0.5889480685521876 0.6916666666666667\n",
      "608 번째 loss, accuracy:  0.5887088959706493 0.6916666666666667\n",
      "609 번째 loss, accuracy:  0.5884704000082361 0.6916666666666667\n",
      "610 번째 loss, accuracy:  0.5882325744839823 0.6916666666666667\n",
      "611 번째 loss, accuracy:  0.5879954359318292 0.6916666666666667\n",
      "612 번째 loss, accuracy:  0.5877589757241736 0.6916666666666667\n",
      "613 번째 loss, accuracy:  0.5875230662411149 0.6916666666666667\n",
      "614 번째 loss, accuracy:  0.5872879021222285 0.6916666666666667\n",
      "615 번째 loss, accuracy:  0.5870533915380156 0.6916666666666667\n",
      "616 번째 loss, accuracy:  0.5868195992678469 0.6916666666666667\n",
      "617 번째 loss, accuracy:  0.5865864652170267 0.6916666666666667\n",
      "618 번째 loss, accuracy:  0.5863539810650339 0.6916666666666667\n",
      "619 번째 loss, accuracy:  0.5861221250512022 0.6916666666666667\n",
      "620 번째 loss, accuracy:  0.585890953766935 0.6916666666666667\n",
      "621 번째 loss, accuracy:  0.5856604378418827 0.6916666666666667\n",
      "622 번째 loss, accuracy:  0.58543056579892 0.6916666666666667\n",
      "623 번째 loss, accuracy:  0.5852013101916422 0.6916666666666667\n",
      "624 번째 loss, accuracy:  0.584972725907998 0.6916666666666667\n",
      "625 번째 loss, accuracy:  0.5847447806818489 0.6916666666666667\n",
      "626 번째 loss, accuracy:  0.5845174828452121 0.6916666666666667\n",
      "627 번째 loss, accuracy:  0.5842908158084427 0.6916666666666667\n",
      "628 번째 loss, accuracy:  0.5840647784800402 0.6916666666666667\n",
      "629 번째 loss, accuracy:  0.5838393751048782 0.6916666666666667\n",
      "630 번째 loss, accuracy:  0.5836145982352157 0.6916666666666667\n",
      "631 번째 loss, accuracy:  0.5833904428407279 0.6916666666666667\n",
      "632 번째 loss, accuracy:  0.5831668916921434 0.6916666666666667\n",
      "633 번째 loss, accuracy:  0.5829439844308442 0.6916666666666667\n",
      "634 번째 loss, accuracy:  0.5827216672505872 0.6916666666666667\n",
      "635 번째 loss, accuracy:  0.5824999542102205 0.6916666666666667\n",
      "636 번째 loss, accuracy:  0.5822788820721336 0.6916666666666667\n",
      "637 번째 loss, accuracy:  0.5820584254237602 0.6916666666666667\n",
      "638 번째 loss, accuracy:  0.5818385343158009 0.6916666666666667\n",
      "639 번째 loss, accuracy:  0.5816192919380564 0.6916666666666667\n",
      "640 번째 loss, accuracy:  0.5814006477075101 0.6916666666666667\n",
      "641 번째 loss, accuracy:  0.5811825951564363 0.6916666666666667\n",
      "642 번째 loss, accuracy:  0.5809651449027542 0.6916666666666667\n",
      "643 번째 loss, accuracy:  0.5807482943423977 0.6916666666666667\n",
      "644 번째 loss, accuracy:  0.5805320073233058 0.6916666666666667\n",
      "645 번째 loss, accuracy:  0.5803163440331289 0.6916666666666667\n",
      "646 번째 loss, accuracy:  0.580101265068738 0.6916666666666667\n",
      "647 번째 loss, accuracy:  0.5798867777681769 0.6916666666666667\n",
      "648 번째 loss, accuracy:  0.5796728706866668 0.6916666666666667\n",
      "649 번째 loss, accuracy:  0.579459543773442 0.6916666666666667\n",
      "650 번째 loss, accuracy:  0.5792468024171403 0.6916666666666667\n",
      "651 번째 loss, accuracy:  0.5790346394644029 0.6916666666666667\n",
      "652 번째 loss, accuracy:  0.5788230330311617 0.6916666666666667\n",
      "653 번째 loss, accuracy:  0.57861202109 0.6916666666666667\n",
      "654 번째 loss, accuracy:  0.578401570917284 0.6916666666666667\n",
      "655 번째 loss, accuracy:  0.5781916915784333 0.6916666666666667\n",
      "656 번째 loss, accuracy:  0.5779823411603063 0.6916666666666667\n",
      "657 번째 loss, accuracy:  0.5777735949523142 0.6916666666666667\n",
      "658 번째 loss, accuracy:  0.5775654029655922 0.6916666666666667\n",
      "659 번째 loss, accuracy:  0.5773577616402629 0.6916666666666667\n",
      "660 번째 loss, accuracy:  0.5771506942205562 0.6916666666666667\n",
      "661 번째 loss, accuracy:  0.5769441778480682 0.6916666666666667\n",
      "662 번째 loss, accuracy:  0.5767382084775952 0.6916666666666667\n",
      "663 번째 loss, accuracy:  0.5765327943432891 0.6916666666666667\n",
      "664 번째 loss, accuracy:  0.5763279308070316 0.6916666666666667\n",
      "665 번째 loss, accuracy:  0.5761235851033578 0.6916666666666667\n",
      "666 번째 loss, accuracy:  0.5759198165208103 0.6916666666666667\n",
      "667 번째 loss, accuracy:  0.5757165814529076 0.6916666666666667\n",
      "668 번째 loss, accuracy:  0.5755138971952763 0.6916666666666667\n",
      "669 번째 loss, accuracy:  0.5753117484687997 0.6916666666666667\n",
      "670 번째 loss, accuracy:  0.5751101294122091 0.6916666666666667\n",
      "671 번째 loss, accuracy:  0.5749090432086742 0.6916666666666667\n",
      "672 번째 loss, accuracy:  0.5747084802579275 0.6916666666666667\n",
      "673 번째 loss, accuracy:  0.5745084590446284 0.6916666666666667\n",
      "674 번째 loss, accuracy:  0.5743089669092046 0.6916666666666667\n",
      "675 번째 loss, accuracy:  0.5741100020796123 0.6916666666666667\n",
      "676 번째 loss, accuracy:  0.573911562897981 0.6916666666666667\n",
      "677 번째 loss, accuracy:  0.573713638403575 0.6916666666666667\n",
      "678 번째 loss, accuracy:  0.5735162339568248 0.6916666666666667\n",
      "679 번째 loss, accuracy:  0.5733193139054755 0.6916666666666667\n",
      "680 번째 loss, accuracy:  0.5731229501285147 0.6916666666666667\n",
      "681 번째 loss, accuracy:  0.5729271006459771 0.6916666666666667\n",
      "682 번째 loss, accuracy:  0.5727317623637006 0.6916666666666667\n",
      "683 번째 loss, accuracy:  0.5725369335928294 0.6916666666666667\n",
      "684 번째 loss, accuracy:  0.5723426155896495 0.6916666666666667\n",
      "685 번째 loss, accuracy:  0.5721487911545271 0.6916666666666667\n",
      "686 번째 loss, accuracy:  0.5719554774667867 0.6916666666666667\n",
      "687 번째 loss, accuracy:  0.571762670672736 0.6916666666666667\n",
      "688 번째 loss, accuracy:  0.5715703564850981 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "689 번째 loss, accuracy:  0.5713785454006144 0.6916666666666667\n",
      "690 번째 loss, accuracy:  0.5711872179307266 0.6916666666666667\n",
      "691 번째 loss, accuracy:  0.5709963863096582 0.6916666666666667\n",
      "692 번째 loss, accuracy:  0.570806034381611 0.6916666666666667\n",
      "693 번째 loss, accuracy:  0.5706161755863105 0.6916666666666667\n",
      "694 번째 loss, accuracy:  0.5704268302089404 0.6916666666666667\n",
      "695 번째 loss, accuracy:  0.570237961981772 0.6916666666666667\n",
      "696 번째 loss, accuracy:  0.570049565495763 0.6916666666666667\n",
      "697 번째 loss, accuracy:  0.569861640045008 0.6916666666666667\n",
      "698 번째 loss, accuracy:  0.5696742209054746 0.6916666666666667\n",
      "699 번째 loss, accuracy:  0.5694872791165059 0.6916666666666667\n",
      "700 번째 loss, accuracy:  0.5693008036431046 0.6916666666666667\n",
      "701 번째 loss, accuracy:  0.5691148282740868 0.6916666666666667\n",
      "702 번째 loss, accuracy:  0.5689293221447455 0.6916666666666667\n",
      "703 번째 loss, accuracy:  0.5687442932533694 0.6916666666666667\n",
      "704 번째 loss, accuracy:  0.5685597105491295 0.6916666666666667\n",
      "705 번째 loss, accuracy:  0.568375613796973 0.6916666666666667\n",
      "706 번째 loss, accuracy:  0.5681919792939885 0.6916666666666667\n",
      "707 번째 loss, accuracy:  0.5680088165271665 0.6916666666666667\n",
      "708 번째 loss, accuracy:  0.5678260999077207 0.6916666666666667\n",
      "709 번째 loss, accuracy:  0.5676438589867818 0.6916666666666667\n",
      "710 번째 loss, accuracy:  0.5674620584825969 0.6916666666666667\n",
      "711 번째 loss, accuracy:  0.5672807454075995 0.6916666666666667\n",
      "712 번째 loss, accuracy:  0.5670998884229237 0.6916666666666667\n",
      "713 번째 loss, accuracy:  0.5669194764030636 0.6916666666666667\n",
      "714 번째 loss, accuracy:  0.5667395228721219 0.6916666666666667\n",
      "715 번째 loss, accuracy:  0.5665600247303433 0.6916666666666667\n",
      "716 번째 loss, accuracy:  0.5663809767733542 0.6916666666666667\n",
      "717 번째 loss, accuracy:  0.566202377669396 0.6916666666666667\n",
      "718 번째 loss, accuracy:  0.566024223201572 0.6916666666666667\n",
      "719 번째 loss, accuracy:  0.5658465049645238 0.6916666666666667\n",
      "720 번째 loss, accuracy:  0.5656692122085093 0.6916666666666667\n",
      "721 번째 loss, accuracy:  0.5654923880944338 0.6916666666666667\n",
      "722 번째 loss, accuracy:  0.5653160037934372 0.6916666666666667\n",
      "723 번째 loss, accuracy:  0.5651400568205297 0.6916666666666667\n",
      "724 번째 loss, accuracy:  0.5649645412895309 0.6916666666666667\n",
      "725 번째 loss, accuracy:  0.5647894451730765 0.6916666666666667\n",
      "726 번째 loss, accuracy:  0.5646148048116753 0.6916666666666667\n",
      "727 번째 loss, accuracy:  0.5644405780303018 0.6916666666666667\n",
      "728 번째 loss, accuracy:  0.5642667853170749 0.6916666666666667\n",
      "729 번째 loss, accuracy:  0.5640934277738376 0.6916666666666667\n",
      "730 번째 loss, accuracy:  0.5639205047466985 0.6916666666666667\n",
      "731 번째 loss, accuracy:  0.563748001003781 0.6916666666666667\n",
      "732 번째 loss, accuracy:  0.5635759057640236 0.6916666666666667\n",
      "733 번째 loss, accuracy:  0.5634042344369649 0.6916666666666667\n",
      "734 번째 loss, accuracy:  0.5632329935703335 0.6916666666666667\n",
      "735 번째 loss, accuracy:  0.5630621680167368 0.6916666666666667\n",
      "736 번째 loss, accuracy:  0.5628917735703922 0.6916666666666667\n",
      "737 번째 loss, accuracy:  0.5627217845890968 0.6916666666666667\n",
      "738 번째 loss, accuracy:  0.5625522202995483 0.6916666666666667\n",
      "739 번째 loss, accuracy:  0.5623830688212892 0.6916666666666667\n",
      "740 번째 loss, accuracy:  0.5622143235733691 0.6916666666666667\n",
      "741 번째 loss, accuracy:  0.562045995653974 0.6916666666666667\n",
      "742 번째 loss, accuracy:  0.5618780741056414 0.6916666666666667\n",
      "743 번째 loss, accuracy:  0.5617105577869634 0.6916666666666667\n",
      "744 번째 loss, accuracy:  0.5615434468027551 0.6916666666666667\n",
      "745 번째 loss, accuracy:  0.5613767125101842 0.6916666666666667\n",
      "746 번째 loss, accuracy:  0.5612103969115393 0.6916666666666667\n",
      "747 번째 loss, accuracy:  0.5610444939128271 0.6916666666666667\n",
      "748 번째 loss, accuracy:  0.5608789929501573 0.6916666666666667\n",
      "749 번째 loss, accuracy:  0.5607138796995278 0.6916666666666667\n",
      "750 번째 loss, accuracy:  0.5605491592139482 0.6916666666666667\n",
      "751 번째 loss, accuracy:  0.560384844947949 0.6916666666666667\n",
      "752 번째 loss, accuracy:  0.5602209061363339 0.6916666666666667\n",
      "753 번째 loss, accuracy:  0.5600573502271656 0.6916666666666667\n",
      "754 번째 loss, accuracy:  0.5598942111720728 0.6916666666666667\n",
      "755 번째 loss, accuracy:  0.5597314637173502 0.6916666666666667\n",
      "756 번째 loss, accuracy:  0.5595691056553496 0.6916666666666667\n",
      "757 번째 loss, accuracy:  0.5594071294489368 0.6916666666666667\n",
      "758 번째 loss, accuracy:  0.5592455373342344 0.6916666666666667\n",
      "759 번째 loss, accuracy:  0.5590843287254347 0.6916666666666667\n",
      "760 번째 loss, accuracy:  0.5589235042244598 0.6916666666666667\n",
      "761 번째 loss, accuracy:  0.5587630563669629 0.6916666666666667\n",
      "762 번째 loss, accuracy:  0.5586029955559084 0.6916666666666667\n",
      "763 번째 loss, accuracy:  0.5584433095724574 0.6916666666666667\n",
      "764 번째 loss, accuracy:  0.5582840003471616 0.6916666666666667\n",
      "765 번째 loss, accuracy:  0.558125067737287 0.6916666666666667\n",
      "766 번째 loss, accuracy:  0.5579665065861988 0.6916666666666667\n",
      "767 번째 loss, accuracy:  0.557808315087535 0.6916666666666667\n",
      "768 번째 loss, accuracy:  0.5576505000301181 0.6916666666666667\n",
      "769 번째 loss, accuracy:  0.5574930539772655 0.6916666666666667\n",
      "770 번째 loss, accuracy:  0.557335951069332 0.6916666666666667\n",
      "771 번째 loss, accuracy:  0.5571792206312066 0.6916666666666667\n",
      "772 번째 loss, accuracy:  0.5570228738302733 0.6916666666666667\n",
      "773 번째 loss, accuracy:  0.5568668936375079 0.6916666666666667\n",
      "774 번째 loss, accuracy:  0.5567112704691105 0.6916666666666667\n",
      "775 번째 loss, accuracy:  0.5565559924478735 0.6916666666666667\n",
      "776 번째 loss, accuracy:  0.5564010817538129 0.6916666666666667\n",
      "777 번째 loss, accuracy:  0.5562465471280703 0.6916666666666667\n",
      "778 번째 loss, accuracy:  0.5560923618363922 0.6916666666666667\n",
      "779 번째 loss, accuracy:  0.5559385204094139 0.6916666666666667\n",
      "780 번째 loss, accuracy:  0.5557850549189415 0.6916666666666667\n",
      "781 번째 loss, accuracy:  0.555631938371368 0.6916666666666667\n",
      "782 번째 loss, accuracy:  0.5554791795384961 0.6916666666666667\n",
      "783 번째 loss, accuracy:  0.5553267709071522 0.6916666666666667\n",
      "784 번째 loss, accuracy:  0.5551747047528218 0.6916666666666667\n",
      "785 번째 loss, accuracy:  0.5550229875698673 0.6916666666666667\n",
      "786 번째 loss, accuracy:  0.5548716243483285 0.6916666666666667\n",
      "787 번째 loss, accuracy:  0.5547205811515669 0.6916666666666667\n",
      "788 번째 loss, accuracy:  0.5545699091887312 0.6916666666666667\n",
      "789 번째 loss, accuracy:  0.554419580852539 0.6916666666666667\n",
      "790 번째 loss, accuracy:  0.5542695928734612 0.6916666666666667\n",
      "791 번째 loss, accuracy:  0.5541199424561647 0.6916666666666667\n",
      "792 번째 loss, accuracy:  0.5539706418435483 0.6916666666666667\n",
      "793 번째 loss, accuracy:  0.5538216613248129 0.6916666666666667\n",
      "794 번째 loss, accuracy:  0.5536730372124355 0.6916666666666667\n",
      "795 번째 loss, accuracy:  0.5535247463428387 0.6916666666666667\n",
      "796 번째 loss, accuracy:  0.5533767930452449 0.6916666666666667\n",
      "797 번째 loss, accuracy:  0.5532291728041233 0.6916666666666667\n",
      "798 번째 loss, accuracy:  0.5530818747484022 0.6916666666666667\n",
      "799 번째 loss, accuracy:  0.5529349161412919 0.6916666666666667\n",
      "800 번째 loss, accuracy:  0.5527882891580618 0.6916666666666667\n",
      "801 번째 loss, accuracy:  0.552641987592538 0.6916666666666667\n",
      "802 번째 loss, accuracy:  0.5524960136216658 0.6916666666666667\n",
      "803 번째 loss, accuracy:  0.552350366035095 0.6916666666666667\n",
      "804 번째 loss, accuracy:  0.5522050498468815 0.6916666666666667\n",
      "805 번째 loss, accuracy:  0.552060050568667 0.6916666666666667\n",
      "806 번째 loss, accuracy:  0.5519153859154791 0.6916666666666667\n",
      "807 번째 loss, accuracy:  0.5517710353558969 0.6916666666666667\n",
      "808 번째 loss, accuracy:  0.5516269974687609 0.6916666666666667\n",
      "809 번째 loss, accuracy:  0.5514832925160671 0.6916666666666667\n",
      "810 번째 loss, accuracy:  0.5513398955356957 0.6916666666666667\n",
      "811 번째 loss, accuracy:  0.5511968260282541 0.6916666666666667\n",
      "812 번째 loss, accuracy:  0.5510540730312992 0.6916666666666667\n",
      "813 번째 loss, accuracy:  0.5509116294044413 0.6916666666666667\n",
      "814 번째 loss, accuracy:  0.5507695071113282 0.6916666666666667\n",
      "815 번째 loss, accuracy:  0.5506276942922924 0.6916666666666667\n",
      "816 번째 loss, accuracy:  0.5504861951338074 0.6916666666666667\n",
      "817 번째 loss, accuracy:  0.5503450037555148 0.6916666666666667\n",
      "818 번째 loss, accuracy:  0.5502041227918892 0.6916666666666667\n",
      "819 번째 loss, accuracy:  0.5500635552413372 0.6916666666666667\n",
      "820 번째 loss, accuracy:  0.5499232937472769 0.6916666666666667\n",
      "821 번째 loss, accuracy:  0.5497833333391923 0.6916666666666667\n",
      "822 번째 loss, accuracy:  0.5496436821786703 0.6916666666666667\n",
      "823 번째 loss, accuracy:  0.549504335657044 0.6916666666666667\n",
      "824 번째 loss, accuracy:  0.549365291969005 0.6916666666666667\n",
      "825 번째 loss, accuracy:  0.5492265527233797 0.6916666666666667\n",
      "826 번째 loss, accuracy:  0.5490880927054269 0.6916666666666667\n",
      "827 번째 loss, accuracy:  0.5489499456410882 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "828 번째 loss, accuracy:  0.548812100963847 0.6916666666666667\n",
      "829 번째 loss, accuracy:  0.5486745579833132 0.6916666666666667\n",
      "830 번째 loss, accuracy:  0.5485372883494065 0.6916666666666667\n",
      "831 번째 loss, accuracy:  0.5484003127136099 0.6916666666666667\n",
      "832 번째 loss, accuracy:  0.5482636262110722 0.6916666666666667\n",
      "833 번째 loss, accuracy:  0.5481272460752211 0.6916666666666667\n",
      "834 번째 loss, accuracy:  0.5479911693216413 0.6916666666666667\n",
      "835 번째 loss, accuracy:  0.5478553796842739 0.6916666666666667\n",
      "836 번째 loss, accuracy:  0.5477198759775174 0.6916666666666667\n",
      "837 번째 loss, accuracy:  0.547584671083786 0.6916666666666667\n",
      "838 번째 loss, accuracy:  0.5474497551154952 0.6916666666666667\n",
      "839 번째 loss, accuracy:  0.5473151220718037 0.6916666666666667\n",
      "840 번째 loss, accuracy:  0.5471807795308166 0.6916666666666667\n",
      "841 번째 loss, accuracy:  0.5470467157419231 0.6916666666666667\n",
      "842 번째 loss, accuracy:  0.5469129161403874 0.6916666666666667\n",
      "843 번째 loss, accuracy:  0.5467794214811341 0.6916666666666667\n",
      "844 번째 loss, accuracy:  0.5466462107446315 0.6916666666666667\n",
      "845 번째 loss, accuracy:  0.5465132783492213 0.6916666666666667\n",
      "846 번째 loss, accuracy:  0.5463806275968479 0.6916666666666667\n",
      "847 번째 loss, accuracy:  0.5462482511735482 0.6916666666666667\n",
      "848 번째 loss, accuracy:  0.5461161501593332 0.6916666666666667\n",
      "849 번째 loss, accuracy:  0.545984330863868 0.6916666666666667\n",
      "850 번째 loss, accuracy:  0.5458527893559431 0.6916666666666667\n",
      "851 번째 loss, accuracy:  0.5457215264547078 0.6916666666666667\n",
      "852 번째 loss, accuracy:  0.5455905395421816 0.6916666666666667\n",
      "853 번째 loss, accuracy:  0.5454598117359348 0.6916666666666667\n",
      "854 번째 loss, accuracy:  0.545329362025257 0.6916666666666667\n",
      "855 번째 loss, accuracy:  0.5451991852341755 0.6916666666666667\n",
      "856 번째 loss, accuracy:  0.545069275533951 0.6916666666666667\n",
      "857 번째 loss, accuracy:  0.544939623842191 0.6916666666666667\n",
      "858 번째 loss, accuracy:  0.5448102351228907 0.6916666666666667\n",
      "859 번째 loss, accuracy:  0.5446811264962569 0.6916666666666667\n",
      "860 번째 loss, accuracy:  0.5445522844192995 0.6916666666666667\n",
      "861 번째 loss, accuracy:  0.5444237018328816 0.6916666666666667\n",
      "862 번째 loss, accuracy:  0.5442953918609319 0.6916666666666667\n",
      "863 번째 loss, accuracy:  0.5441673455456626 0.6916666666666667\n",
      "864 번째 loss, accuracy:  0.5440395546217135 0.6916666666666667\n",
      "865 번째 loss, accuracy:  0.5439120327559087 0.6916666666666667\n",
      "866 번째 loss, accuracy:  0.5437847642994399 0.6916666666666667\n",
      "867 번째 loss, accuracy:  0.5436577498390761 0.6916666666666667\n",
      "868 번째 loss, accuracy:  0.5435310025496922 0.6916666666666667\n",
      "869 번째 loss, accuracy:  0.5434045098139517 0.6916666666666667\n",
      "870 번째 loss, accuracy:  0.5432782664673002 0.6916666666666667\n",
      "871 번째 loss, accuracy:  0.5431522893783917 0.6916666666666667\n",
      "872 번째 loss, accuracy:  0.5430265618013358 0.6916666666666667\n",
      "873 번째 loss, accuracy:  0.5429010950791511 0.6916666666666667\n",
      "874 번째 loss, accuracy:  0.5427758608064951 0.6916666666666667\n",
      "875 번째 loss, accuracy:  0.5426508803995986 0.6916666666666667\n",
      "876 번째 loss, accuracy:  0.5425261663892671 0.6916666666666667\n",
      "877 번째 loss, accuracy:  0.5424016915155085 0.6916666666666667\n",
      "878 번째 loss, accuracy:  0.5422774751996989 0.6916666666666667\n",
      "879 번째 loss, accuracy:  0.5421535037259751 0.6916666666666667\n",
      "880 번째 loss, accuracy:  0.5420297816337293 0.6916666666666667\n",
      "881 번째 loss, accuracy:  0.5419063072114209 0.6916666666666667\n",
      "882 번째 loss, accuracy:  0.5417830794313035 0.6916666666666667\n",
      "883 번째 loss, accuracy:  0.5416600956021201 0.6916666666666667\n",
      "884 번째 loss, accuracy:  0.5415373561880867 0.6916666666666667\n",
      "885 번째 loss, accuracy:  0.5414148593610979 0.6916666666666667\n",
      "886 번째 loss, accuracy:  0.5412925944634313 0.6916666666666667\n",
      "887 번째 loss, accuracy:  0.5411705826233809 0.6916666666666667\n",
      "888 번째 loss, accuracy:  0.5410487890206942 0.6916666666666667\n",
      "889 번째 loss, accuracy:  0.5409272353821177 0.6916666666666667\n",
      "890 번째 loss, accuracy:  0.5408059381407982 0.6916666666666667\n",
      "891 번째 loss, accuracy:  0.5406848796153713 0.6916666666666667\n",
      "892 번째 loss, accuracy:  0.5405640516342821 0.7\n",
      "893 번째 loss, accuracy:  0.5404434598407651 0.7\n",
      "894 번째 loss, accuracy:  0.540323105942456 0.7\n",
      "895 번째 loss, accuracy:  0.5402029810601376 0.7\n",
      "896 번째 loss, accuracy:  0.5400830954555919 0.7\n",
      "897 번째 loss, accuracy:  0.5399634309180205 0.7\n",
      "898 번째 loss, accuracy:  0.5398439794435258 0.7\n",
      "899 번째 loss, accuracy:  0.5397247888691314 0.7\n",
      "900 번째 loss, accuracy:  0.5396058245006069 0.7\n",
      "901 번째 loss, accuracy:  0.539487088981489 0.7\n",
      "902 번째 loss, accuracy:  0.5393685827367207 0.7\n",
      "903 번째 loss, accuracy:  0.5392503023587959 0.7\n",
      "904 번째 loss, accuracy:  0.5391322476822328 0.7\n",
      "905 번째 loss, accuracy:  0.5390144149386819 0.7\n",
      "906 번째 loss, accuracy:  0.5388967999345914 0.7\n",
      "907 번째 loss, accuracy:  0.5387794140412832 0.7\n",
      "908 번째 loss, accuracy:  0.5386622466494766 0.7\n",
      "909 번째 loss, accuracy:  0.5385453100755113 0.7\n",
      "910 번째 loss, accuracy:  0.5384285956171114 0.7\n",
      "911 번째 loss, accuracy:  0.5383121007874428 0.7\n",
      "912 번째 loss, accuracy:  0.538195810861518 0.7\n",
      "913 번째 loss, accuracy:  0.5380797495392682 0.7\n",
      "914 번째 loss, accuracy:  0.5379639100795287 0.7\n",
      "915 번째 loss, accuracy:  0.5378482805799912 0.7\n",
      "916 번째 loss, accuracy:  0.5377328736058261 0.7\n",
      "917 번째 loss, accuracy:  0.5376176722540599 0.7\n",
      "918 번째 loss, accuracy:  0.5375026885666723 0.7\n",
      "919 번째 loss, accuracy:  0.537387911901402 0.7\n",
      "920 번째 loss, accuracy:  0.5372733609406308 0.7\n",
      "921 번째 loss, accuracy:  0.5371590209488571 0.7\n",
      "922 번째 loss, accuracy:  0.5370448885861746 0.7\n",
      "923 번째 loss, accuracy:  0.5369309695013313 0.7\n",
      "924 번째 loss, accuracy:  0.536817258882786 0.7\n",
      "925 번째 loss, accuracy:  0.5367037481948279 0.7\n",
      "926 번째 loss, accuracy:  0.5365904568388143 0.7\n",
      "927 번째 loss, accuracy:  0.5364773720239268 0.7\n",
      "928 번째 loss, accuracy:  0.5363644861796265 0.7\n",
      "929 번째 loss, accuracy:  0.536251800379411 0.7\n",
      "930 번째 loss, accuracy:  0.5361393212818893 0.7\n",
      "931 번째 loss, accuracy:  0.5360270528513982 0.7\n",
      "932 번째 loss, accuracy:  0.5359149876818946 0.7\n",
      "933 번째 loss, accuracy:  0.5358031242323344 0.7\n",
      "934 번째 loss, accuracy:  0.5356914604850165 0.7\n",
      "935 번째 loss, accuracy:  0.5355799943844912 0.7\n",
      "936 번째 loss, accuracy:  0.5354687342951047 0.7\n",
      "937 번째 loss, accuracy:  0.5353576696500767 0.7\n",
      "938 번째 loss, accuracy:  0.5352467983555093 0.7\n",
      "939 번째 loss, accuracy:  0.5351361326196594 0.7\n",
      "940 번째 loss, accuracy:  0.5350256617639229 0.7\n",
      "941 번째 loss, accuracy:  0.5349153847748263 0.7\n",
      "942 번째 loss, accuracy:  0.5348053058595424 0.7\n",
      "943 번째 loss, accuracy:  0.5346954204181389 0.7\n",
      "944 번째 loss, accuracy:  0.5345857281394404 0.7\n",
      "945 번째 loss, accuracy:  0.5344762296656375 0.7\n",
      "946 번째 loss, accuracy:  0.5343669130803769 0.7\n",
      "947 번째 loss, accuracy:  0.5342577761053285 0.7\n",
      "948 번째 loss, accuracy:  0.5341488472798473 0.7\n",
      "949 번째 loss, accuracy:  0.5340401117800496 0.7\n",
      "950 번째 loss, accuracy:  0.5339315606627174 0.7\n",
      "951 번째 loss, accuracy:  0.533823194211878 0.7\n",
      "952 번째 loss, accuracy:  0.5337150219936241 0.7\n",
      "953 번째 loss, accuracy:  0.5336070255850272 0.7\n",
      "954 번째 loss, accuracy:  0.533499215326718 0.7\n",
      "955 번째 loss, accuracy:  0.5333915979258568 0.7\n",
      "956 번째 loss, accuracy:  0.5332841486301765 0.7\n",
      "957 번째 loss, accuracy:  0.533176898699042 0.7\n",
      "958 번째 loss, accuracy:  0.5330698320669772 0.7\n",
      "959 번째 loss, accuracy:  0.5329629425208385 0.7\n",
      "960 번째 loss, accuracy:  0.5328562330748507 0.7\n",
      "961 번째 loss, accuracy:  0.5327497125484967 0.7\n",
      "962 번째 loss, accuracy:  0.5326433706388587 0.7\n",
      "963 번째 loss, accuracy:  0.532537208013993 0.7\n",
      "964 번째 loss, accuracy:  0.5324312080612527 0.7\n",
      "965 번째 loss, accuracy:  0.5323253930802152 0.7\n",
      "966 번째 loss, accuracy:  0.5322197602640869 0.7\n",
      "967 번째 loss, accuracy:  0.5321143041414496 0.7\n",
      "968 번째 loss, accuracy:  0.5320090234320791 0.7\n",
      "969 번째 loss, accuracy:  0.5319039078788841 0.7\n",
      "970 번째 loss, accuracy:  0.531798970298923 0.7\n",
      "971 번째 loss, accuracy:  0.5316942075875737 0.7\n",
      "972 번째 loss, accuracy:  0.5315896019825902 0.7\n",
      "973 번째 loss, accuracy:  0.531485183789624 0.7\n",
      "974 번째 loss, accuracy:  0.5313809339168062 0.7\n",
      "975 번째 loss, accuracy:  0.5312768562200725 0.7\n",
      "976 번째 loss, accuracy:  0.5311729402846624 0.7\n",
      "977 번째 loss, accuracy:  0.5310692026111816 0.7\n",
      "978 번째 loss, accuracy:  0.5309656189339097 0.7\n",
      "979 번째 loss, accuracy:  0.5308622030133312 0.7\n",
      "980 번째 loss, accuracy:  0.5307589580728526 0.7\n",
      "981 번째 loss, accuracy:  0.5306558879307214 0.7\n",
      "982 번째 loss, accuracy:  0.5305529778403206 0.7\n",
      "983 번째 loss, accuracy:  0.5304502199233864 0.7\n",
      "984 번째 loss, accuracy:  0.5303476294379559 0.7\n",
      "985 번째 loss, accuracy:  0.5302451961114047 0.7\n",
      "986 번째 loss, accuracy:  0.5301429362721956 0.7\n",
      "987 번째 loss, accuracy:  0.5300408388103955 0.7\n",
      "988 번째 loss, accuracy:  0.5299388966696689 0.7\n",
      "989 번째 loss, accuracy:  0.5298371125493259 0.7\n",
      "990 번째 loss, accuracy:  0.5297354927943183 0.7\n",
      "991 번째 loss, accuracy:  0.5296340154602629 0.7\n",
      "992 번째 loss, accuracy:  0.5295327079937562 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993 번째 loss, accuracy:  0.5294315529656898 0.7\n",
      "994 번째 loss, accuracy:  0.5293305529070254 0.7\n",
      "995 번째 loss, accuracy:  0.5292297127797606 0.7\n",
      "996 번째 loss, accuracy:  0.5291290213643286 0.7\n",
      "997 번째 loss, accuracy:  0.5290284856112878 0.7\n",
      "998 번째 loss, accuracy:  0.5289281072802118 0.7\n",
      "999 번째 loss, accuracy:  0.528827876745985 0.7\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEXCAYAAAC9A7+nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO2dd3xVRfr/3096QnpCDyGgIL0GxAKiLIiuil2xIFhYddVt+v3p7n7FVfer67rrrq4NFbGsimtbe0FFRWmhSpUOCS0QCCUEUub3x0zCJdwkN+EmN/fmeb9e53XvmZlz5jln5nzOnJk5zxFjDIqiKErwExZoAxRFURT/oIKuKIoSIqigK4qihAgq6IqiKCGCCrqiKEqIoIKuKIoSIgS1oIvIBhH5WTVxQ0VkVQ3bThWRB2uINyJyoj/srCGPTBHZLyLhDZnP8dAY5yFUCcVzJyIzROTGRshnvIjMrOe2NV77oUxQC3pNGGO+M8acFGg7RCRDRN4WkZ0iUigiP4rIeABjzCZjTLwxpqwB8+8hIjkistst00WkR0Pl5yX/aBGZIiJ7RWSbiPy2lvS/cekK3XbRLryViLwuIltc3PcicnLjHEXjIyK9ROQzV2+OeVlERFJF5F0ROSAiG0XkqirxV7nwAyLynoik+rptAx5TjY2oOu6rp4h87ur0HhGZLyLnQuNd+yLSUkRec/nvFpF/e8TVqd77i5AV9CbEK8BmoCOQBowDtjdi/luAS4FUIB14H3jDHzv28cniPqAL9vjPBP5HREZXs7+zgbuBEUAW0Bn4k4uOB+YBA7HH8hLwkYjE1/8ImjQlwJvADdXEPwkcBloDVwNPi0hPsGIHPAtc6+KLgKd82TaI+AD4AnsMrYA7gL2NbMM7wDZs3W4FPOoRdx8+1nu/YowJ2gXYANwJLAEKgWlAjIsbDuR6pO0PLAD2uXRvAA96xN8FbMUK4PWAAU50cdHYwtqEFeNngFjPfIDfATvcPiZ47Hc/0K8a+7NcPhHAKS5txVIMbHDpwrBCtxbYhb3QU+txviKAXwJFddjG8zxMBZ4GPgYOAD/zYfs8YJTH+gPAG9WkfQ34P4/1EcC2Gva9Fxjo43EkAS+48skDHgTCXdx44HvgCVePVgIjPLZth70RFgBrgJs84sKB37uy2QfMBzp4nLubgdXAbqyQSh3L7ETAVAlrgRXkrh5hrwAPu///B7zmEXeCS59Q27Y+2DMDeAiY687Vfz3rIvAfrMgVAt8CPV34ROxN6rCr3x+48A5YYcx3dftfHmUyE3vd7QbWA+e4uHR3bpOrsXE47toHruDo6+oQMKO269qH8zAKqz/hx1vv/bmEQgv9cmA00Anog60IRyEiUcB72Iqbiq10l3jEj8beGEZi76pV++X/AnQF+mEvsPbAvR7xbbCC0R7bonpSRFJc3Gy3fqWIZFZ3EMaYWcZ2v8QDKW671130HcCFwBlYcakQhwr7l9T22Cwie7A3iSewF3x9uQr4M1YcZrpH+yXV5Jni7F3sEbwYqK412NNL2tYikuZl3/2AKKzA+sJLQCm2/PpjL0jPvuCTgXVYsZgEvOPRTfE69qbdDvu0838iMsLF/RYYC5wLJGIbA0Ue+z0PGAT0xdbVs539me5Rvdo6UQNdgTJjzE8eYZ7n9ajzaIxZixNxH7b1hXHY42yHPaePe8R9gr2GWmEbUP92Nkx2/x9x9fx894T3IbAR27hpz9FPjycDq7Bl8gjwgogIVvjXAK+KyIUi0ro6Q40x0zyuq3bYMq64rmq8rl35nF7Nroc4214SkV0iMk9EznDb1bXe+4+GvmM05IK9Q17jsf4I8IyXu/QwbMtbPNL+gGuhA1PwaKFgC9lgC1mwrdETPOJPAdZ75HMQiPCI3wEMcf9TgIeBZUAZsAgY5OKyXD4RVY7raeAjIMytr+DoFmNbbGsnwpfz5LFdC+BW4Od12KZqC/3lOmzbwW0f4xE2Evfk4SX9WmC0x3qk2z6rSrpE4EfgHh/taI1tmcV6hI0Fvnb/x3upH3OxXRYdXLkleMQ9BEx1/1cBY2o4d6d7rL8J3F3HMvPWQh9KlScX4CaOtDy/BG6uEp/n6mqN2/pgz4wq10oP7M3imJYqkOzOQZJH/fF8Kj4F2zI/ph67MlnjsR7n9tXGrWcA/3J1phz7NNDFHLkmc6vsLwx783jardd4XftwHiY7e25w9fRKYA/25lOneu/PJRRa6Ns8/hdh+1qr0g7IM+7MOjZWid9cTVxLbGWa7+7Ye4BPXXgFu4wxpd7sMMbsNsbcbYzpiRWWRcB7rqVxDCLyC2yFvMoYU+6COwLveuS/Aisy1bZMvGGMOYB9rHxZRFrVZVsPNteepJL97jfRIywR2zVRXfqqafFMLyKx2P7T2caYh3y0oyP2otvqcQ6fxbYiK/BWP9q5pcAYs69KXHv3vwNWVKrDl/pZV6qeJzj6vNYUX9u2vlD1WokE0kUkXEQeFpG1IrIX2+ACK3Le6ABsrHLteFJ57owxFU89FddVrjHmNmPMCdjyPQC8XIPNFU+Vd7h1X67rmjiIFegXjDElxpg3sOflNOpe7/1GKAi6L2wF2lcR0cwq8R2qiduJLbyexphktyQZ+whXJ4wxO7F9du2wXT9HISJDsX1tY4wxhR5Rm7H9h8keS4wxJq+uNmDLPI4jglRXfHbPaYzZjT23fT2C+2KfVryxzEva7caYXWBnDmC7zvKAX9TB5s3YFnq6x/lLdDfZCrzVjy1uSRWRhCpxFed+M7aPujH5CYgQkS4eYZ7n9ajzKCKdsf3FP/mwrS9UvVZKsNfJVcAYbJdlEvYJFGxrGI6tO5uBTBGJqEPex2CM2YztguzlLV5ErsQ+kV1qjClxwcd7XS+hmmuhHvXebzQXQZ+F7eu7Q0QiRORiYLBH/JvAeLFT/OKwfagAuFbyc8BjFa1aEWnvZmTUioj8xU1Bi3CicAv2UXJXlXQdsIO148zR/ZtgW9V/FpGOLm1LERnjY/4jRaS/az0lAn/H9sGvcPHjRWSDL/uqJy8DfxSRFBHphn28n1pD2htcOaQAf6xIKyKRwFvYi3Ccx9MLLj5L7LzvrKo7NcZsBT4H/iYiiSISJiInVPR5Olph60ekiFwGdAc+dmLxA/CQiMSISB/sY3bFFLXngQdEpItY+njr868rbl8x2HECXN7R7ngOYAcS7xeRFiJyGlZIX3Gb/xs4X+x87BbA/cA7xph9tW1b03n04BqPa+V+4C1jp94mYG+cu7CNhqpjNduxM5cqmIsVvoedLTHOntrOTYqI/ElETnRlmY7t05/tJW1/7LjRhcaY/Irw472ugXeBFBG5zl1bl2IbSd+7+LrUe7/RLATdGHMYuBjbL7cbO/L9jkf8J8A/gK+wgy1fVdnF/3Phs92j5HTA13mucdjC34MdkOkIXOAl3Qjs4OpbYl822i8iFXf0f2JnWXwuIvuwFbdyDraILBORq6vJPxk7CFSI7Ro4EdtPXeziO3CkEtYZEbnaw05vTHL5bgS+Af5qjPnUbVvxYlUmgAt/BPjapd/IkZvrqdgBxlHAHo9zNNTjODZypOVclXFYcVyOrQNvYcciKpiDHczbiX08v9TjpjsW29rcgi3LScaYL1zc37ENgs+xs25eAGJrOB94O3YvdMTevCrO7UFsf30Ft7p8dmDL9xZjzDIA93szVth3YIX2Vl+2pfbzCFb8p2K7RGI40o3xsse2yzlWYF8AergujvfcTeB8bJ3chB14vqKGfCs4jC2P6dhzvhR7IxnvJe0Y7DjWTI8684mLq/G6rlK/jsIYU4C9ju/EXlt3Y5+sd7ok1db7hkSO7jZUmhsi8jnwK2PMikDbcjyIyB+BfGPMs/XYdjxwozGmuhkNzYbjOY9K4FFBV5o9KuhKqNAsulwURVGaA9pCVxRFCRG0ha4oihIiqKAriqKECCEt6BKC/qhDGRFZVd00MUVRaiekBT2QSB38IbuXe8o85snuF5HhLq5WP+AicruIrHd55YiHQyER+aTKfg+LyI8e8V+LSL7bdrH4/sLSMo99lolIscf67+t8wgBjzEnGmO/qs60X+2aK9SUe5Y/91ZJXjFhf33tFZKuI/KqGtM9XKY9DIrLbhzyu89jmoIiUe6zvOQ7be4lIce0pj9rmPNdYurX21MePiPxcRFaL9d/+uYi0qyHtThEp8jg371aJP0msn/n9rt5PqhI/QUR+cnmtFpFsj7hEEXlORArctfiJR9y3Vcq1RERm+fM8+ERDO4sJ5IKHY6kA5P0Q8B32pYbu2JcwRleTdjwws5q4zliPfm2xrlonYl9+iXfxJ2P9WAzEvmJ9C9bhUXVuPWcA93qs98E5R3L72ge0reOxzsBO+6spTZ0ciR3nuT8B6+tmN3BRI+T3V3cOkrGvn2/HB9fCbttXgcl1zO9n+MnRk7O3uI7b/Af7NuicRji3Ga5Onod9GepJYHoN6XfiHON5iYvDuhu4xe0rDujlEX8h9kWjimspE+cMzMW/h3Xkl+quxWpdNwM5wG8b+vwck29jZ9ioB3e0p8Ak7Jts+di3t/7IEW+GJ2Lf5ip0FWKaCxfgMewbdYVY/w29fMy7Ln7AqxX0atJX+gHHvlk31yOuhTvuY0QZ+3ZdGdCpmv0OxrrYHVzH83yMoGNd036Lda1awBGH/187MdiJfeMwyWObXGC4+/8g9i3GV90FvRQY4KM997vyfBx4r0pcnCvTTRzx2R3t4oZh324sdBf+tT7mtx04y2P9IeBVH7ZLwN6MT6vj+fYq6Ni3Sz9w53YtR/ttH4p1DLcX+7p9hafRAldfKvyF964l7xRXRy51dalblfgRHPGVvhG4woXHY70jbsa+NT0Dd/3Vkt9vgc891tOwvmMyqklfk6D/FvikhryWVNjrJW6Aq7e1+kvHusktAVrXpVz9sTRqZo1+cEcL+stYZ/wJWGH7CbjBxb0O/AHbBRWDc3mK9V09H9vyEmxLu62LuwpYUk2+KS7v1h5hlwI/VpN+vLuwdzq7/pdqWrRY383FHHFJmuhsPBnbargdWIiXDylgfT3P8BL+odunwXqcq/VCq7L9DLwLeim2NRSObRF1dRd8FNZ3yvfAox7bVBX0g64MwrGt4Fpveq6c1mOfZE7Gviae7hH/LNa9bMUTz+lYb4GdsDeOy7EfAknHfZgE60Z3QTX5tXTnLc0j7EpgoQ+2Xg+srke9PkbQnc3LsR9aiQS6ufNZUZd/xD2tuDoz2P2vUwvdleda9/9rjv4gyUnYm8JFzp5WQB8X9xLWV3prFzeMI9Om1wIXVJPfC9jX5j3DNgBnV5N+J/ZpeAf2QyzdPeLexPre+dKlmw6c5OIq3PPeiXXRsQn4GxDl4m/Fuod4Givsi6jGDTXWfcWHdS1XfyyNnmGjHtwRn+bhWF8PPTzifsER/9EvY/0bZ1TZ/iyswA6hDiJH3f2Ad3aCEgb0dhfmMb6+8eIHHCtgv8e2CEpdRR1UTT5rgPHVxEUC5wC/qcd5noF3QV9Xy3aXAvM81qsK+qcecX2A/T7YMhwr4qkex3y7+19RD3p62e5/gf/U49g7UcWnvTuPa3zY9hvgj/XI05ugjwBWVAn7M/CE+78A67sktUqaugr6bI607m/CtrjDPPJ7xcs2MdjW/Am+5uOx7bSq5wj7sYhLq0k/FOtZMt7VoY1ACxf3A7bhMhzbqLjPXWthHPkGwkzsTboNtqF0j9v2/1z8XW7b0dibV6cq+Ydjn8692tfQS3MZFE3HFoKnn3NPn9b/gxXGuW6w73oAY8xX2MfEJ4HtIjJZrMfC2qiTP2RjzDpjzHpjTLkx5kdsl8Glnmmkej/gN2Jbej3dMV4DfFh14MgNlLbBOqXyZkOJsU7KzhYRb87D6sNRvtNFpI2IvCkiec4Z0lSq95UNx/oSb+FDntdhH6sL3PprLgxs6zAK7/7La/NrXh318n0tIp2wTwev1JSuDnQEThTn29sNlN6BLXOwTxnZwGoRmS0iI+uagYichH3qqfA0+Ra2FX6WW6/uHLbjyJNTXamT/3ZjPxB9yBiz3xjzR6zAVnhWPQh8YYyZYazDvgexDb5OLg7gMWNMvjFmG7bL7lyPbfdinygPG+toax5Hjr2Ckdin0Q/qcazHTXMR9J3YFmxHj7BKn9bGmG3GmJuMMe2wLfenKqY7GmMeN8YMxApmV+wdukbM8ftDNhzxIV2bH/C+2O8z/uRuCJ+6vE+tku46rAvV/dRMBP7z722qrP8F20LubYxJxHY1ef3QR30Q6yr2UmCEm1m0DdsFNVDsR5C3Y1vv3o6vXn7NjXXJmk/dy3oc8I0xZmMt6XxlM7DUHO0zP8EYc5mzc5n73wrbbfCOWD/kVcuoJipujF+7c7sCK5jjPGzwdg7zXD6d6nxUx/p2T8UOlPp6LZVzpI5V68Mc+3S4u4Z4r59Z9MJ12LGyQz6m9y+BeCxorIWj+9Bfxbo+TcAK+0pcNwFwGa67BSvcB7GVbxC2RRKJbR1+CtznY94PYx+pU7D9mVupfpbLObj+dpd2KdZFKy7vD7CC7u1TXddhu4U6YyvuSGxrtptHmljsQNRZVbbt5vKOdflcgxW8AS4+Cy+fgPNiwwy8d7nMqBL2Dta3ezi2NTcbj64Dju1ymeoRd8yn2LzYcS325p2BbZlWLD8Af3FpnsV+Lb6Ns+M0ju5Dv4Qjfeh9fSzrR7Eul5Oxn2TbRi2zXLAt2XFewl8Fnq9lW29dLhWugW/DdjlEYIWwYhxgHEe6oS7EDlqGc2QMoH0teYZhBfuuKud2LLYVHY9t8OzDupUNx948ervtX8J+VrGVi6vsQ68l34pZLue6evoE1cxywd5MKq7XWKwL2zzc5wOx35I9gH0yisBOjFjKkS6jv2MHyVPdeZkH/D+PaygXO7Aa7spgHx7XBnbiRRF1nFTgzyUgmTbawR0t6CnuYsl3FfNej4J8xBX8fnehTXThI7B35v1Yofg3R6YLXg0sqyHvaOwUp73YluFvPeIy3T4z3fqjLs0B7IDM/UCkizvDHUcRR3+9fKiLF5d+k6tgK6gyO8NddBurXkDYQd45brs9rgJf5BE/FDsAFVnLeZ6Bb4LeG9uXux87cHsX/hX06TjhrhJ+lSvfcOzg1+NuvRB7060Y+BqOnaGx153Pa1z4dcDiGvKNxQrWPqyY/8ojrrM73nZVzut+XN9ulX19A0yo5ThrmuXylqtLBdj+4IpB0bexdXgftg/a89utj7q4PVQzy4UjApZYJTwM25Uy3iNdjjuHG4DLXXg89slgq8vnK45cfxuwH6Co7njPw46FFGH9znuey1eAv7n/A7Et94rr9VPcoGyVurDBlf10oKtHXAx2ELYQ6/v+r3jUfewNYZ7b/xLsV8Q8930TVcYxGntR51xKtYj6xm5UxH6haCFWVKv7zqaiVIsKuqIoSojQXAZFlRBBRDpXecXac6n2lXDFN0TklWrO7d8CbZtSO9pCVxRFCREiApVxenq6ycrKClT2iqIoQcn8+fN3GmNaeourVdBFZAp2lHmHMaaXl/irsW+ggR39vcUYs7i2/WZlZZGTk1NbMkVRFMUDEan23QVf+tCnYl9zrY71wBnGmD5YB1ST62SdoiiK4hdqbaEbY74Vkawa4n/wWJ2NfRFAURRFaWT8PcvlBqxHNUVRFKWR8dugqIiciRX002tIMxHr1pTMzEx/Za0ozYqSkhJyc3MpLq7Th4aUICMmJoaMjAwiIyN93sYvgi4ifbB+hs8xxuyqLp0xZjKujz07O1vnSypKPcjNzSUhIYGsrCxE/ObbTGlCGGPYtWsXubm5dOrku0+z4+5yEZFMrNOla40xPx3v/hRFqZni4mLS0tJUzEMYESEtLa3OT2G+TFt8Heu0KF1EcrEezCIBjDHPYJ1cpWFdzgKUGmOyve9NURR/oGIe+tSnjH2Z5TK2lvgbsZ71GoVV2/bxweIt3HB6J1JaNPgH3RVFUYKGoPPlsn7nAf719Rq2FB6sPbGiKA1CfHx8oE1QvBB0gp4cZ0d8C4tKAmyJoihK0yJoBX3PQRV0RQk0xhjuuusuevXqRe/evZk2bRoAW7duZdiwYfTr149evXrx3XffUVZWxvjx4yvTPvbYYwG2PvQImHOu+pIca/vN92gLXVH40wfLWL5lr1/32aNdIpPO7+lT2nfeeYdFixaxePFidu7cyaBBgxg2bBivvfYaZ599Nn/4wx8oKyujqKiIRYsWkZeXx9KlSwHYs2ePX+1WgriFvrvocIAtURRl5syZjB07lvDwcFq3bs0ZZ5zBvHnzGDRoEC+++CL33XcfP/74IwkJCXTu3Jl169Zx++238+mnn5KYmBho80OOoGuhx0SGExMZRqF2uSiKzy3phqK67ykMGzaMb7/9lo8++ohrr72Wu+66i3HjxrF48WI+++wznnzySd58802mTJnSyBaHNkHXQgfb7bJHW+iKEnCGDRvGtGnTKCsrIz8/n2+//ZbBgwezceNGWrVqxU033cQNN9zAggUL2LlzJ+Xl5VxyySU88MADLFiwINDmhxxB10IH2+2ifeiKEnguuugiZs2aRd++fRERHnnkEdq0acNLL73EX//6VyIjI4mPj+fll18mLy+PCRMmUF5eDsBDDz0UYOtDj4B9gi47O9vU9wMXV06eRXk5vHnzKX62SlGaPitWrKB79+6BNkNpBLyVtYjMr+5t/KDscmmZEMOOfeppTlEUxZOgFPS2STFsLSyudkBGURSlORK0gn6otJzd2o+uKIpSSdAKOsBW9eeiKIpSSVAKepukWAC27tF+dEVRlAqCUtDbJ1tBz91dFGBLFEVRmg5BKejp8VEkxUayJn9/oE1RlGbHnj17eOqpp+q17bnnnlurD5d7772X6dOn12v/zZ1aBV1EpojIDhFZWk18NxGZJSKHRORO/5voNU9ObBXP6u0q6IrS2NQk6GVlZTVu+/HHH5OcnFxjmvvvv5+f/exn9bYvEJSWlgbaBMC3FvpUYHQN8QXAHcCj/jDIV7q0imfNDhV0RWls7r77btauXUu/fv246667mDFjBmeeeSZXXXUVvXv3BuDCCy9k4MCB9OzZk8mTJ1dum5WVxc6dO9mwYQPdu3fnpptuomfPnowaNYqDB+0kh/Hjx/PWW29Vpp80aRIDBgygd+/erFy5EoD8/HxGjhzJgAED+MUvfkHHjh3ZuXPnMbbecsstZGdn07NnTyZNmlQZPm/ePE499VT69u3L4MGD2bdvH2VlZdx555307t2bPn368MQTTxxlM0BOTg7Dhw8H4L777mPixImMGjWKcePGsWHDBoYOHcqAAQMYMGAAP/zwQ2V+jzzyCL1796Zv376V52/AgAGV8atXr2bgwIHHXTa+fILuWxHJqiF+B7BDRH5+3NbUge5tE3lj3mby9hys7FNXlGbHJ3fDth/9u882veGch6uNfvjhh1m6dCmLFi0CYMaMGcydO5elS5dWfqF+ypQppKamcvDgQQYNGsQll1xCWlraUftZvXo1r7/+Os899xyXX345b7/9Ntdcc80x+aWnp7NgwQKeeuopHn30UZ5//nn+9Kc/cdZZZ3HPPffw6aefHnXT8OTPf/4zqamplJWVMWLECJYsWUK3bt244oormDZtGoMGDWLv3r3ExsYyefJk1q9fz8KFC4mIiKCgoKDWUzV//nxmzpxJbGwsRUVFfPHFF8TExLB69WrGjh1LTk4On3zyCe+99x5z5swhLi6OgoICUlNTSUpKYtGiRfTr148XX3yR8ePH15pfbTRqH7qITBSRHBHJyc/PP659ZWelAJCzofaTrihKwzJ48OBKMQd4/PHH6du3L0OGDGHz5s2sXr36mG06depEv379ABg4cCAbNmzwuu+LL774mDQzZ87kyiuvBGD06NGkpKR43fbNN99kwIAB9O/fn2XLlrF8+XJWrVpF27ZtGTRoEACJiYlEREQwffp0br75ZiIibDs3NTW11uO+4IILiI21DcqSkhJuuukmevfuzWWXXcby5csBmD59OhMmTCAuLu6o/d544428+OKLlJWVMW3aNK666qpa86uNRnXOZYyZDEwG68vlePbVrU0i8dERzNtQwJh+7f1in6IEHTW0pBuTFi1aVP6fMWMG06dPZ9asWcTFxTF8+HCKi4+dYhwdHV35Pzw8vLLLpbp04eHhlX3Vvrwlvn79eh599FHmzZtHSkoK48ePp7jYvmEuIsekry48IiKi0qFY1ePwPO7HHnuM1q1bs3jxYsrLy4mJialxv5dccknlk8bAgQOPeYKpD0E5ywUgPEwY0DGFueu1ha4ojUlCQgL79u2rNr6wsJCUlBTi4uJYuXIls2fP9rsNp59+Om+++SYAn3/+Obt37z4mzd69e2nRogVJSUls376dTz75BIBu3bqxZcsW5s2bB8C+ffsoLS1l1KhRPPPMM5U3jYoul6ysLObPnw/A22+/Xa1NhYWFtG3blrCwMF555ZXKAeJRo0YxZcoUioqKjtpvTEwMZ599NrfccgsTJkw47nMCQSzoAKefmMZP2/ezuUDnoytKY5GWlsZpp51Gr169uOuuu46JHz16NKWlpfTp04f//d//ZciQIX63YdKkSXz++ecMGDCATz75hLZt25KQkHBUmr59+9K/f3969uzJ9ddfz2mnnQZAVFQU06ZN4/bbb6dv376MHDmS4uJibrzxRjIzM+nTpw99+/bltddeq8zrV7/6FUOHDiU8PLxam2699VZeeuklhgwZwk8//VTZeh89ejQXXHAB2dnZ9OvXj0cfPTJ/5Oqrr0ZEGDVqlF/OS63uc0XkdWA4kA5sByYBkQDGmGdEpA2QAyQC5cB+oIcxpsYPHR6P+9wKNu0qYthfv+aPP+/OjUM7H9e+FCVYUPe5cOjQIcLDw4mIiGDWrFnccsstlYO0wcSjjz5KYWEhDzzwgNf4urrP9WWWy9ha4rcBGbXtpyHITIujR9tEPvpxqwq6ojQjNm3axOWXX055eTlRUVE899xzgTapzlx00UWsXbuWr776ym/7DMovFnly8YD2PPjRClZu20u3NvrRWUVpDnTp0oWFCxcG2ozj4t133/X7PoO6Dx3g0oEZRHsP/qgAACAASURBVEeE8ersjYE2RVEaDf0WQOhTnzIOekFPjovivD7teHdBHoXqH11pBsTExLBr1y4V9RDGGMOuXbsqpz76StB3uQDcOLQTby/IZcr36/nNyK6BNkdRGpSMjAxyc3M53pfzlKZNTEwMGRl1G54MCUHv3jaRs3u2Zsr367n+9E4kxUYG2iRFaTAiIyOPeitTUSoI+i6XCu4Y0YV9xaW88N26QJuiKIoSEEJG0Hu2S+K8Pm2Z/N06tuzRT9MpitL8CBlBB7j7nG4YAw9/sjLQpiiKojQ6ISXoGSlx/GJYZ95fvIV56oVRUZRmRkgJOsDNw0+gfXIs97zzI4dKa/56iqIoSigRcoIeFxXBgxf1Ys2O/Tw9Y22gzVEURWk0Qk7QAc48qRVj+rXjya/XsHp79W4+FUVRQomQFHSAe8/rQXx0BHe9tYTSsvJAm6MoitLghKygp8VH86cxvVi0eQ9PadeLoijNgJAVdIAL+rbjwn7t+OeXq1m0eU+gzVEURWlQahV0EZkiIjtEZGk18SIij4vIGhFZIiID/G9m/fnTmF60SYzh128s5MCh0kCboyiK0mD40kKfCoyuIf4coItbJgJPH79Z/iMpNpK/Xd6XjQVFPPDh8kCboyiK0mDUKujGmG+Bmt7SGQO8bCyzgWQRaesvA/3BkM5p3HzGCbwxbzP/XZQXaHMURVEaBH/0obcHNnus57qwYxCRiSKSIyI5je3687cjuzIoK4V73vmRNTv2N2reiqIojYE/BF28hHn1vG+MmWyMyTbGZLds2dIPWftOZHgYT4wdQGxkOLf+ez5Fh7U/XVGU0MIfgp4LdPBYzwC2+GG/fqdNUgz/uLIfq3fs54/vLdUvviiKElL4Q9DfB8a52S5DgEJjzFY/7LdBGNqlJXec1YV3FuTxxrzNtW+gKIoSJNT6xSIReR0YDqSLSC4wCYgEMMY8A3wMnAusAYqACQ1lrL+4Y0QXFmzazb3/XUrX1vEM7JgaaJMURVGOGwlUt0N2drbJyckJSN4AhUUljHlyJvsPlfHB7afRNik2YLYoiqL4iojMN8Zke4sL6TdFayIpLpLnxmVz8HApv3hlPsUl6mpXUZTgptkKOkCX1gn848r+LMkt5O63l+ggqaIoQU2zFnSAkT1a87uRXXlv0RYmf6sfmFYUJXipdVC0OXDbWSeyYtteHv50JR3TWjC6V5tAm6QoilJnmn0LHUBE+Ntl/eiTkcyvpy1ksXpmVBQlCFFBd8RGhfP8uGzS46O54aUcNhcUBdokRVGUOqGC7kHLhGimThjE4dIyrp86j8KDJYE2SVEUxWdU0KtwYqsEnrl2IOt3HuCX/15AiX6+TlGUIEEF3QunnpDOw5f0Yeaanfzh3R91OqOiKEGBznKphksHZrCpoIjHv1xNm8QYfjvqpECbpCiKUiMq6DXwm591YXthMY9/tYb0hGjGnZIVaJMURVGqRQW9BkSEP1/Ui10HDjPp/WWktYjm532a1MeYFEVRKtE+9FqICA/jX1f1J7tjCr+Ztogf1uwMtEmKoiheUUH3gZjIcJ4fN4hO6S2Y+Mp8luYVBtokRVGUY1BB95GkuEheun4wSbGRjH9xHht3HQi0SYqiKEfhk6CLyGgRWSUia0Tkbi/xHUXkSxFZIiIzRCTD/6YGnjZJMbx0/WDKysu55oU5bCssDrRJiqIoldQq6CISDjwJnAP0AMaKSI8qyR4FXjbG9AHuBx7yt6FNhRNbxTN1wmB2Hyjh6udns3P/oUCbpCiKAvjWQh8MrDHGrDPGHAbeAMZUSdMD+NL9/9pLfEjRt0MyU8YPIm/PQa59YS6FReoiQFGUwOOLoLcHPL+mnOvCPFkMXOL+XwQkiEha1R2JyEQRyRGRnPz8/PrY22QY3CmVyddms3bHfq57cS77D5UG2iRFUZo5vgi6eAmr+i78ncAZIrIQOAPIA45ROGPMZGNMtjEmu2XLlnU2tqkxrGtL/nVVf37MK+SGqfM4eFg/Y6coSuDwRdBzgQ4e6xnAFs8ExpgtxpiLjTH9gT+4sGYxt29Uzzb8/fK+zN1QwM2vzudQqYq6oiiBwRdBnwd0EZFOIhIFXAm875lARNJFpGJf9wBT/Gtm02ZMv/Y8fHFvvvkpn9tfW6geGhVFCQi1CroxphS4DfgMWAG8aYxZJiL3i8gFLtlwYJWI/AS0Bv7cQPY2Wa4YlMmk83vw+fLt3PbaAg6XqqgritK4SKBcw2ZnZ5ucnJyA5N2QvPj9ev70wXJG9mjNk1cNICpC391SFMV/iMh8Y0y2tzhVGz8z4bRO3Hd+D75Yvp1faktdUZRGRAW9ARh/WifuH9OTL5Zv59Z/60CpoiiNgwp6AzHulCweGNOT6St2cOurC1TUFUVpcFTQG5BrT8niwQt78eXKHfzilfk6T11RlAZFBb2BuWZIx8opjddNmcu+YnUToChKw6CC3ghcOTiTx6/sz4JNu7nquTkUHDgcaJMURQlBVNAbifP7tuO5cdn8tH0flz87S13vKorid1TQG5Ezu7XipesHs62wmEuf+UE/kqEoil9RQW9khnRO47WbTubAoVIufWYWK7ftDbRJiqKECCroAaBPRjJv/uIUwgQue3qWfnhaURS/oIIeILq0TuDdW0+jbXIM1704l/cW5gXaJEVRghwV9ADSLjmW/9x8KgM7pvDraYt4asYaAuVbR1GU4EcFPcAkxUby0vWDOb9vOx75dBX3/ncZZeUq6oqi1J2IQBugQHREOP+8oh/tkmN49pt1bC0s5p9X9qNFtBaPoii+oy30JkJYmHDPOd25f0xPvlq5nUue/oHc3UWBNktRlCDCJ0EXkdEiskpE1ojI3V7iM0XkaxFZKCJLRORc/5vaPBh3ShZTJwwmb89Bxvzre+ZtKAi0SYqiBAm1CrqIhANPAucAPYCxItKjSrI/Yr9k1B/7ibqn/G1oc2JY15a898vTSIyN5KrnZvNmzuZAm6QoShDgSwt9MLDGGLPOGHMYeAMYUyWNARLd/ySqfERaqTsntIznvVtP4+ROafzPW0t48MPlOliqKEqN+CLo7QHPJmKuC/PkPuAaEckFPgZu97YjEZkoIjkikpOfn18Pc5sXSXGRTJ0wiPGnZvH8zPWMmzKHXfsPBdosRVGaKL4IungJq9pUHAtMNcZkAOcCr4jIMfs2xkw2xmQbY7JbtmxZd2ubIRHhYdx3QU8euaQP8zbs5rwnZrJg0+5Am6UoShPEF0HPBTp4rGdwbJfKDcCbAMaYWUAMkO4PAxXL5YM68M4tpxIRLlzx7CxenrVBX0JSFOUofBH0eUAXEekkIlHYQc/3q6TZBIwAEJHuWEHXPhU/06t9Eh/eNpShXVpy73+X8etpiyg6XBposxRFaSLUKujGmFLgNuAzYAV2NssyEblfRC5wyX4H3CQii4HXgfFGm48NQlJcJM+Py+bOUV15f/EWxvzre/XYqCgKABIo3c3OzjY5OTkByTtUmLl6J7+etoi9xSX84dzujDulIyLehjwURQkVRGS+MSbbW5y+KRrEnN4lnU9/PZTTTkhj0vvLuOnlHP28naI0Y1TQg5z0+GimjB/Evef14NufdjL6H9/yvfpXV5RmiQp6CCAiXH96J9795akkxERwzQtzeODD5RSXlAXaNEVRGhEV9BCiZ7skPrj9dK4+OZMXZq7n3H9+p3PWFaUZoYIeYsRFRfDghb159YaTOVRazqVP/8BDn6zQ1rqiNANU0EOUigHTKwZ14Nlv1nH+EzNZvHlPoM1SFKUBUUEPYRJiInno4j68dP1g9hWXctFT3/OnD5ax/5C+jKQooYgKejPgjK4t+fy3w7j65I5M/WEDI//+DZ8v2xZosxRF8TMq6M2ExJhIHriwF2/fcipJsZFMfGU+E1/OYcueg4E2TVEUP6GC3swYkJnCB7efzt3ndOPb1fmM/Ps3PP/dOg6XlgfaNEVRjhMV9GZIZHgYN59xAl/85gwGd0rlwY9WMPqf3zJj1Y5Am6YoynGggt6M6ZAax4sTBjNlfDbGwPgX53H91Hms33kg0KYpilIPVNAVzurWms9+PYzfn9uNuesLGPXYNzz08Qr2FpcE2jRFUeqACroCQFREGBOHncBXd57BRf3bM/m7dZzxyNc8/906fSlJUYIEFXTlKFolxPDIpX354LbT6dU+iQc/WsGIv33D2/Nz9SPVitLE8UnQRWS0iKwSkTUicreX+MdEZJFbfhIRfSUxyOnVPolXbjiZV284mdQWUfzuP4s595/f8dXK7frpO0VpotT6gQsRCQd+AkZivy86DxhrjFleTfrbgf7GmOtr2q9+4CJ4KC83fLx0K49+tooNu4oYkJnMr3/WlaFd0vWDGorSyNT0gYsIH7YfDKwxxqxzO3sDGAN4FXRgLDCpPoYqTZOwMOG8Pu04u2cbps3bzFNfr2HclLn0z0zmVyO6cEbXlirsSnCwbzus/hwI8FNm657QfqDfd+uLoLcHNnus5wIne0soIh2BTsBX1cRPBCYCZGZm1slQJfBEhodxzZCOXJadwVvzc3nq67WMf3Ee/Tok86ufdWG4CrvS1PnmL5DzQqCtgNN+HTBB93aFVnd7uxJ4yxjjdVqEMWYyMBlsl4tPFjZl8ubDjpWBtqLRiQaujoIrRhpyNhbw5fIdfPTyf1mUHMvwk1rRJyOJcBV2pSmyaRa0GwBXvBJYO6LiG2S3vgh6LtDBYz0D2FJN2iuBXx6vUUGBMfDvy6BoV6AtCRgRwBC3EAkcABa4RVGaKiffAkkZgbaiQfBF0OcBXUSkE5CHFe2rqiYSkZOAFGCWXy1sShzYCWu/smJeUmTFfPg90HdsoC1rEpQbw8w1O3lt7maW5hWSGBPBxf3bc/HADFLjogJtnqJYQlTMwQdBN8aUishtwGdAODDFGLNMRO4Hcowx77ukY4E3TCjPafvmEZj77NFhnc6AlI6BsaeJEQYMG5zFsMHZzN9YwLPfrOPPs7bzyNw1XNC3HdedkkXvjKRAm6koIUut0xYbiqCbtli8Fx7tCuld4LKpNiwyFhLbBdSsps6aHft56YcNvL0gl6LDZQzITOa6U7M4p1dboiL0vTZFqSs1TVvUK8pXcqZA6UHIGARpJ9hFxbxWTmwVzwMX9mL270cw6fwe7C4q4VdvLOLUh7/i71/8xLbC4kCbqCghgy996ArAPveFn3P+Elg7gpTEmEgmnNaJ607J4rs1O3nphw088dVq/vXVas48qRWXD+rAWd1aERmubQxFqS8q6L5ysACSO0J4ZKAtCWrCwoQzurbkjK4t2bjrAG/mbOY/Obl8uXIH6fHRXDowg8uzM+jcsmGmdSlKKKN96L7y6iV2VsvEGYG2JOQoLSvnm5/yeWPeZr5auYOycsPgrFQuy85gdK82JMToTVRRKjjeV/8VgP07IKFNoK0ISSLCwxjRvTUjurdmx75i3p6fx5s5m7nrrSX88b2l/KxHay7s154zurbUgVRFqQEVdF8oPQTblkDHUwNtScjTKiGGW4afwM1ndGbh5j38d2EeHyzZykdLtpIcF8nPe7flov7tGdgxRd0MKEoVVNB9YdFr9rflSYG1oxkhIgzITGFAZgp/PK8HM1fv5N2Feby9IJd/z9lE++RYzu3dhnN6t6VfRjJhYSruiqKCXh2lh2DPJkjtDAtftWEDrgusTc2UyPAwzuzWijO7tWL/oVI+X7aNDxZvYeoPG3juu/W0SYxhdK82nNu7LQM7phCu4q40U1TQq+OLe2HOM3D+45CXA616QFh4oK1q9sRHR3DxgAwuHpBB4cESvlq5nY9/3MZrczcx9YcNpMdHM7pXa87p1ZZBWana5640K1TQq2PLQvu7/L/296ppgbNF8UpSbCQX9c/gov4Z7D9Uytcrd/DJ0q28PT+PV2dvIiE6gmFdWzKieyuGn9SK1BbqT0YJbVTQq6PiLdC1X0JCW0hW/+1NmfjoCM7v247z+7bj4OEyZq7ZyZcrtvPlyh189ONWRGBAZgojurdiRLfWdG0dr4OqSsihgl4dcWlH/qedGDg7lDoTGxXOyB6tGdmjNeXlhqVbCvlyxQ6+WrmDRz5dxSOfrqJ9cizDurZkWJd0Tj0hnaQ4neuuBD8q6NVR6uFjpHWvwNmhHBdhYUKfjGT6ZCTzm5Fd2b63mK9WWnH/cPEWXp+7iTCBPhnJDO2SztAuLemfmawuCJSgRN8UrY7/TIA1X8LVb0KbPhAVF2iLFD9TWlbO4tw9fPvTTr5bnc+izXsoN9AiKpxTTkjj9BPTOblzGie1TtBpkUqTQd8UrQ8lRZCaBZlDAm2J0kBEhIcxsGMqAzum8puRXSk8WMKstbv4bnU+M9fsZPqKHQAkx0UyOCuVkzuncXKnVLq3TdSpkUqTxCdBF5HRwD+xH7h43hjzsJc0lwP3Yb83utgYc8xXjYKKwwcgskWgrVAakaTYSEb3asPoXtbFQ+7uIuasK2D2ul3MWV/A58u3A5AQE+EEPpWTO6XRs10iEdpFozQBahV0EQkHngRGYr8vOk9E3jfGLPdI0wW4BzjNGLNbRFo1lMGNRkkRxKYE2golgGSkxJExMI5LBtpPlm0tPMicdQXMWb+L2esK+HKlbcHHRobTJyOJAR1TGJiZQv/MZNLiowNputJM8aWFPhhYY4xZByAibwBjgOUeaW4CnjTG7AYwxuzwt6GNzuEiSGwfaCuUJkTbpFgu7N+eC/vberF9bzFz1hewYONuFmzazXPfruPpcjsmlZUWx4DMFPp3TGFAZjIntU7QVrzS4Pgi6O2BzR7rucDJVdJ0BRCR77HdMvcZYz71i4WBouQARGmXi1I9rRNjuKBvOy7oa99ZOHi4jB/zClmwaTcLNu7m29X5vLMwD7ADrb3aJ9G7fRK9M5Lok5FMx9Q4HWxV/Iovgu6txlWdGhMBdAGGAxnAdyLSyxiz56gdiUwEJgJkZjbhF3WMsX5cThwZaEuUICI2KpzBnVIZ3CkVAGMMmwsOWoHftJsluYW8PHsjh0vLAUiIjqBX+yT6ZCRV/mamxukLT0q98UXQc4EOHusZwBYvaWYbY0qA9SKyCivw8zwTGWMmA5PBTlusr9ENzoKX7W/BusDaoQQ1IkJmWhyZaXGV3TQlZeWs3r6fH/P2sCS3kKV5hbz4/QYOl1mRT4yJoHdGEt3bJNK9bSLd2iZwYqt4oiPUj5BSO74I+jygi4h0AvKAK4GqM1jeA8YCU0UkHdsFE7xqWOHHpeJXUfxEZHgYPdol0qNdIlcMsmGHS8v5afs+fswrZEluIcu2FPLK7I0cci35iDDhhJbxdG+bQLe2Vui7t0mgZUK0tuaVo6hV0I0xpSJyG/AZtn98ijFmmYjcD+QYY953caNEZDlQBtxljNnVkIY3KBV+WzoMDqwdSrMgKiKMXu1tt8tYV+VKy8rZsKuIFVv3snLbXlZs3cfc9QW8t+jIw3Faiyi6tU3gpNaJdGkdT5dW8ZzYKp7kOHVC1lzRN0W9Mfc5+PhO+PVSSO5Qe3pFaST2FB1m5bZ9Vui37mPFtr2s3r6fgyVllWnS46Mrxb1La/t7Yqt4WsZriz4U0DdF60pZif2N1i/PK02L5LgohnROY0jnI87jyssNeXsOsmbHflbv2Od+9/Pewjz2HSqtTJcUG1kp9J3SW1QumWlx2kcfIqige6PcCXq4ProqTZ+wMKFDahwdUuM4s9uRd/qMMWzfe6hS6Ffv2M+aHfv5fPl2Cg4crkwnAu2TYysFPivtiNhnpMTq/PkgQgXdG2WusoepS1UleBER2iTF0CYphtO7pB8VV1hUwvpdB9iw8wDr3bJh1wHeXXB0qz4iTMhMjSMrvQWZqXFkuhtHh9RYOqTE0SJaJaQpoaXhjTJXocNV0JXQJCkukn5xyfTrkHxUuDGGXQcOHyP06/IPMGfdLg4cLjsqfVqLKDIqhD4llg6V/+NomxyjbogbGRV0b5QdhrAI+yyqKM0IESE9Ppr0+Giys1KPijPGsLuohE0FRWwuKGLzbvdbcJAluXv45MetlJYfmWQRHia0TYqhQ0oc7ZJjaZ8cQ7vkWNq6/22TYrWF72f0bHqjvET7zxWlCiJCaosoUltEHdOyBzvVctveYjYXHDxK8DcVFPHD2p1s31tMeZVJdUmxkZVi3zYplnbJsbRLjqG9E/7WCdHah18HVNC9UVai/eeKUkciwsOsh8qUOE45Ie2Y+JKycrbvLWZrYTFb9hwkb89Btu6x/3N3H2Teht0UHiw5apswsT5z7BJNm8QYWrn1Ni6sVWIMiTEROiUTFXTvlB3W/nNF8TORHoJfHfsPlbK1Quyd8G/ZU8yOfcWsyz/ArLW72Ftcesx2sZHhtE6MPkr8K/63SYqhdUIMrRKjiYkM7emZKujeKCtRQVeUABAfHUGX1gl0aZ1QbZqiw6Xs2HuI7XuL2ba3+Jj/i3P3sK2wuNJ1QtX9t0yIJj0+yv3apeJ/RVx6fHCKvwq6N1TQFaXJEhcVQVZ6BFnp1bu3Nsaw92Ap2/cVs63Qin3+vkPs3H+o8nfVtn18v3/XMd08FSTERNAyPpr0hGj7W+UmkBYfRVqLaFLjo2gRFd4kunxU0L1Rrn3oihLMiAhJcZEkxUXStYbWPsCh0jJ27T9cKfRHRN+G5e8/xIqte8nff4h9Xrp7wPrjSWsRRVp8FKktoklzg8epLaJceHTl/9T4KBKiG6bPXwXdGyXFEFl9P5+iKKFDdES4m10TW2va4pIyJ/qHKThQ8WuXXS6s4MBh1uXvZ9f+w0f52PHkpqGd+MPPe/j7UFTQvVJSBJExgbZCUZQmRkxkeK0Du54cPFzGLifyuw4cpsDdAHq2T2wQ+1TQvVFyECJrv1sriqLURGxUOBlRvt8Ajhedse+N0oPa5aIoStChgu4NbaErihKE+CToIjJaRFaJyBoRudtL/HgRyReRRW650f+mNiIl2kJXFCX4qLUPXUTCgSeBkdiPQc8TkfeNMcurJJ1mjLmtAWxsfEqKIEIHRRVFCS58aaEPBtYYY9YZYw4DbwBjGtasAKNdLoqiBCG+CHp7YLPHeq4Lq8olIrJERN4SEa8f4hSRiSKSIyI5+fn59TC3ESg9ZFvoscd6k1MURWnK+CLo3l5nqvpl6Q+ALGNMH2A68JK3HRljJhtjso0x2S1btqybpY1FUYH9jU2tOZ2iKEoTwxdBzwU8W9wZwBbPBMaYXcaYQ271OWCgf8wLAAedoMcd6/5TURSlKeOLoM8DuohIJxGJAq4E3vdMICJtPVYvAFb4z8RGJudF+9siveZ0iqIoTYxaZ7kYY0pF5DbgMyAcmGKMWSYi9wM5xpj3gTtE5AKgFCgAxjegzQ3Lno32t8OQwNqhKIpSR3x69d8Y8zHwcZWwez3+3wPc41/TAsC+bbD6czjpXAhXrwiKogQX+qaoJ7vW2t/OZwbWDkVRlHqggu5JxYBo5smBtUNRFKUeqKB7UrTL/uoMF0VRghAVdE9mPWl/dQ66oihBiAq6J+VlEN8aotQxl6IowYcKuicHC6D7+YG2QlEUpV6ooFdQXgYH92h3i6IoQYsKegWrPwcMxKmgK4oSnKigV5CbY3+7XxBYOxRFUeqJCjpAeTl89yikdIIkb56BFUVRmj4q6ACFzt17x1MDa4eiKMpxoIIOR+af97sqsHYoiqIcByroADtX2d8O+sq/oijBS/N2Kbh6Orz7Czv/vPflEB4ZaIsURVHqTfMW9LVfwaF9MHgi9Ls60NYoiqIcFz51uYjIaBFZJSJrROTuGtJdKiJGRLL9Z2IDkbcAZj8JaSfCOX+Btn0CbZGiKMpxUaugi0g48CRwDtADGCsiPbykSwDuAOb428gGIW++/R1yS2DtUBRF8RO+tNAHA2uMMeuMMYeBN4AxXtI9ADwCFPvRvobj4G772+eKwNqhKIriJ3wR9PbAZo/1XBdWiYj0BzoYYz6saUciMlFEckQkJz8/v87G+pWiXRCdCBFRgbVDURTFT/gi6OIlzFRGioQBjwG/q21HxpjJxphsY0x2y5YtfbeyIZjzDMQmB9YGRVEUP+KLoOcCHTzWM4AtHusJQC9ghohsAIYA7zfpgdEi96m5VscMBSiKogQtvkxbnAd0EZFOQB5wJVD5SqUxphBIr1gXkRnAncaYHP+aepzsWAnTroHSQ1BeYsMGTgisTYqiKH6kVkE3xpSKyG3AZ0A4MMUYs0xE7gdyjDHvN7SRfmH9t7BrNfS+DMIiIaoFdBoaaKsURVH8hk8vFhljPgY+rhJ2bzVphx+/WQ3ArtUQlQAXPwfibVhAURQluGk+vlx2rob0E1XMFUUJWZqHoK/4ENZ9bd8KVRRFCVGaiaC7bv5BNwbWDkVRlAYk+JxzrZkOn/2hbtvs2QSdh0PmkIawSFEUpUkQfIIenQgtT6rbNi1Pgn7XNIw9iqIoTYTgE/QOg6HDy4G2QlEUpcnRPPrQFUVRmgEq6IqiKCGCCrqiKEqIoIKuKIoSIqigK4qihAgq6IqiKCGCCrqiKEqIoIKuKIoSIogxpvZUDZGxSD6wsZ6bpwM7/WhOMKDH3DzQY24eHM8xdzTGeP2GZ8AE/XgQkRxjTNP9xF0DoMfcPNBjbh401DFrl4uiKEqIoIKuKIoSIgSroE8OtAEBQI+5eaDH3DxokGMOyj50RVEU5ViCtYWuKIqiVEEFXVEUJUQIOkEXkdEiskpE1ojI3YG2x1+ISAcR+VpEVojIMhH5lQtPFZEvRGS1+01x4SIij7vzsEREBgT2COqHiISLyEIR+dCtdxKROe54p4lIlAuPdutrXHxWIO0+HkQkWUTeEpGVrrxPCeVyFpHfuDq9VEReF5GYUCxnEZkiIjtEZKlHWJ3LVUSuc+lXi8h1dbEhqARdRMKBJ4FzgB7AWBHpEVir/EYp8DtjTHdgCPBLd2x3A18aY7oAX7p1sOeg6AEBMQAAAylJREFUi1smAk83vsl+4VfACo/1vwCPuePdDdzgwm8AdhtjTgQec+mClX8CnxpjugF9sccfkuUsIu2BO4BsY0wvIBy4ktAs56nA6CphdSpXEUkFJgEnA4OBSRU3AZ8wxgTNApwCfOaxfg9wT6DtaqBj/S8wElgFtHVhbYFV7v+zwFiP9JXpgmUBMlwlPwv4EBDs23MRVcsb+Aw4xf2PcOkk0MdQj2NOBNZXtT1UyxloD2wGUl25fQicHarlDGQBS+tbrsBY4FmP8KPS1bYEVQudI5WjglwXFlK4x8z+wBygtTFmK4D7beWShcK5+AfwP0C5W08D9hhjSt265zFVHq+LL3Tpg43OQD7woutqel5EWhCi5WyMyQMeBTYBW7HlNp/QL+cK6lqux1XewSbo4iUspOZdikg88Dbwa2PM3pqSegkLmnMhIucBO4wx8z2DvSQ1PsQFExHAAOBpY0x/4ABHHsO9EdTH7boLxgCdgHZAC2x3Q1VCrZxro7rjPK7jDzZBzwU6eKxnAFsCZIvfEZFIrJj/2xjzjgveLiJtXXxbYIcLD/ZzcRpwgYhsAN7Adrv8A0gWkQiXxvOYKo/XxScBBY1psJ/IBXKNMXPc+ltYgQ/Vcv4ZsN4Yk2+MKQHeAU4l9Mu5grqW63GVd7AJ+jygixshj8IOrrwfYJv8gogI8AKwwhjzd4+o94GKke7rsH3rFeHj3Gj5EKCw4tEuGDDG3GOMyTDGZGHL8StjzNXA18ClLlnV4604D5e69EHXcjPGbAM2i8hJLmgEsJwQLWdsV8sQEYlzdbzieEO6nD2oa7l+BowSkRT3dDPKhflGoAcR6jHocC7wE7AW+EOg7fHjcZ2OfbRaAixyy7nY/sMvgdXuN9WlF+yMn7XAj9hZBAE/jnoe+3DgQ/e/MzAXWAP8B4h24TFufY2L7xxou4/jePsBOa6s3wNSQrmcgT8BK4GlwCtAdCiWM/A6dpygBNvSvqE+5Qpc745/DTChLjboq/+KoighQrB1uSiKoijVoIKuKIoSIqigK4qihAgq6IqiKCGCCrqiKEqIoIKuKIoSIqigK4qihAj/H8A4+L3pcX03AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 loss, accuracy:  2.500463172288638 0.35833333333333334\n",
      "1 번째 loss, accuracy:  2.4804363550896604 0.35833333333333334\n",
      "2 번째 loss, accuracy:  2.4606588731654457 0.35833333333333334\n",
      "3 번째 loss, accuracy:  2.4411238285645975 0.35833333333333334\n",
      "4 번째 loss, accuracy:  2.4218252420060784 0.35833333333333334\n",
      "5 번째 loss, accuracy:  2.4027578869706203 0.35833333333333334\n",
      "6 번째 loss, accuracy:  2.3839171491780227 0.35833333333333334\n",
      "7 번째 loss, accuracy:  2.3652989067097137 0.35833333333333334\n",
      "8 번째 loss, accuracy:  2.3468994269587977 0.35833333333333334\n",
      "9 번째 loss, accuracy:  2.328715277313562 0.35833333333333334\n",
      "10 번째 loss, accuracy:  2.3107432470520615 0.35833333333333334\n",
      "11 번째 loss, accuracy:  2.292980278372815 0.35833333333333334\n",
      "12 번째 loss, accuracy:  2.275423404840514 0.35833333333333334\n",
      "13 번째 loss, accuracy:  2.2580696958012725 0.35833333333333334\n",
      "14 번째 loss, accuracy:  2.2409162055387055 0.35833333333333334\n",
      "15 번째 loss, accuracy:  2.2239599261080953 0.35833333333333334\n",
      "16 번째 loss, accuracy:  2.207197742911842 0.35833333333333334\n",
      "17 번째 loss, accuracy:  2.190626392176053 0.35833333333333334\n",
      "18 번째 loss, accuracy:  2.174242419551328 0.35833333333333334\n",
      "19 번째 loss, accuracy:  2.158042139108978 0.35833333333333334\n",
      "20 번째 loss, accuracy:  2.1420215920269152 0.35833333333333334\n",
      "21 번째 loss, accuracy:  2.126176504270833 0.35833333333333334\n",
      "22 번째 loss, accuracy:  2.1105022425726405 0.35833333333333334\n",
      "23 번째 loss, accuracy:  2.0949937679977935 0.35833333333333334\n",
      "24 번째 loss, accuracy:  2.0796455863747614 0.35833333333333334\n",
      "25 번째 loss, accuracy:  2.064451694843781 0.35833333333333334\n",
      "26 번째 loss, accuracy:  2.049405523771423 0.35833333333333334\n",
      "27 번째 loss, accuracy:  2.0344998732845765 0.35833333333333334\n",
      "28 번째 loss, accuracy:  2.019726843716105 0.35833333333333334\n",
      "29 번째 loss, accuracy:  2.0050777593453555 0.35833333333333334\n",
      "30 번째 loss, accuracy:  1.99054308498864 0.35833333333333334\n",
      "31 번째 loss, accuracy:  1.9761123352925363 0.35833333333333334\n",
      "32 번째 loss, accuracy:  1.9617739770622233 0.35833333333333334\n",
      "33 번째 loss, accuracy:  1.947515325702936 0.35833333333333334\n",
      "34 번째 loss, accuracy:  1.9333224379760667 0.35833333333333334\n",
      "35 번째 loss, accuracy:  1.9191800049241015 0.35833333333333334\n",
      "36 번째 loss, accuracy:  1.905071251197849 0.35833333333333334\n",
      "37 번째 loss, accuracy:  1.890977850374136 0.35833333333333334\n",
      "38 번째 loss, accuracy:  1.8768798704806107 0.35833333333333334\n",
      "39 번째 loss, accuracy:  1.8627557701714348 0.35833333333333334\n",
      "40 번째 loss, accuracy:  1.848582474119514 0.35833333333333334\n",
      "41 번째 loss, accuracy:  1.8343355663634153 0.35833333333333334\n",
      "42 번째 loss, accuracy:  1.8199896523581414 0.35833333333333334\n",
      "43 번째 loss, accuracy:  1.8055189534127114 0.35833333333333334\n",
      "44 번째 loss, accuracy:  1.7908982088744 0.35833333333333334\n",
      "45 번째 loss, accuracy:  1.7761039677155046 0.35833333333333334\n",
      "46 번째 loss, accuracy:  1.7611163452938414 0.35833333333333334\n",
      "47 번째 loss, accuracy:  1.7459212931759998 0.35833333333333334\n",
      "48 번째 loss, accuracy:  1.7305133680633242 0.35833333333333334\n",
      "49 번째 loss, accuracy:  1.7148988791729864 0.35833333333333334\n",
      "50 번째 loss, accuracy:  1.6990991390252361 0.35833333333333334\n",
      "51 번째 loss, accuracy:  1.6831533560977168 0.35833333333333334\n",
      "52 번째 loss, accuracy:  1.667120534146577 0.35833333333333334\n",
      "53 번째 loss, accuracy:  1.6510796589897652 0.35833333333333334\n",
      "54 번째 loss, accuracy:  1.6351275505663927 0.35833333333333334\n",
      "55 번째 loss, accuracy:  1.6193740980532212 0.35833333333333334\n",
      "56 번째 loss, accuracy:  1.6039351532908948 0.35833333333333334\n",
      "57 번째 loss, accuracy:  1.5889239862276534 0.35833333333333334\n",
      "58 번째 loss, accuracy:  1.5744426725597134 0.35833333333333334\n",
      "59 번째 loss, accuracy:  1.560574878163085 0.35833333333333334\n",
      "60 번째 loss, accuracy:  1.5473811643728534 0.35833333333333334\n",
      "61 번째 loss, accuracy:  1.5348972944185368 0.35833333333333334\n",
      "62 번째 loss, accuracy:  1.5231353286962392 0.35833333333333334\n",
      "63 번째 loss, accuracy:  1.512086792103015 0.35833333333333334\n",
      "64 번째 loss, accuracy:  1.5017269923869199 0.35833333333333334\n",
      "65 번째 loss, accuracy:  1.4920196355198663 0.35833333333333334\n",
      "66 번째 loss, accuracy:  1.4829211113235723 0.35833333333333334\n",
      "67 번째 loss, accuracy:  1.474384090991488 0.35833333333333334\n",
      "68 번째 loss, accuracy:  1.4663603070459539 0.35833333333333334\n",
      "69 번째 loss, accuracy:  1.4588025424520614 0.35833333333333334\n",
      "70 번째 loss, accuracy:  1.4516659403003864 0.35833333333333334\n",
      "71 번째 loss, accuracy:  1.4449087762444643 0.35833333333333334\n",
      "72 번째 loss, accuracy:  1.4384928332481166 0.35833333333333334\n",
      "73 번째 loss, accuracy:  1.4323834988883766 0.35833333333333334\n",
      "74 번째 loss, accuracy:  1.4265496806156825 0.35833333333333334\n",
      "75 번째 loss, accuracy:  1.4209636102503898 0.35833333333333334\n",
      "76 번째 loss, accuracy:  1.4156005884320353 0.35833333333333334\n",
      "77 번째 loss, accuracy:  1.4104387035321733 0.35833333333333334\n",
      "78 번째 loss, accuracy:  1.4054585474459018 0.35833333333333334\n",
      "79 번째 loss, accuracy:  1.4006429420190443 0.35833333333333334\n",
      "80 번째 loss, accuracy:  1.3959766838872223 0.35833333333333334\n",
      "81 번째 loss, accuracy:  1.3914463115084499 0.35833333333333334\n",
      "82 번째 loss, accuracy:  1.3870398955987824 0.35833333333333334\n",
      "83 번째 loss, accuracy:  1.382746852596117 0.35833333333333334\n",
      "84 번째 loss, accuracy:  1.3785577798620627 0.35833333333333334\n",
      "85 번째 loss, accuracy:  1.374464310858535 0.35833333333333334\n",
      "86 번째 loss, accuracy:  1.3704589883435487 0.35833333333333334\n",
      "87 번째 loss, accuracy:  1.366535153614534 0.35833333333333334\n",
      "88 번째 loss, accuracy:  1.3626868499127747 0.35833333333333334\n",
      "89 번째 loss, accuracy:  1.3589087382424903 0.35833333333333334\n",
      "90 번째 loss, accuracy:  1.355196024023768 0.35833333333333334\n",
      "91 번째 loss, accuracy:  1.3515443931691364 0.35833333333333334\n",
      "92 번째 loss, accuracy:  1.3479499563399389 0.35833333333333334\n",
      "93 번째 loss, accuracy:  1.3444092002937567 0.35833333333333334\n",
      "94 번째 loss, accuracy:  1.3409189453747274 0.35833333333333334\n",
      "95 번째 loss, accuracy:  1.3374763083250045 0.35833333333333334\n",
      "96 번째 loss, accuracy:  1.3340786697063711 0.35833333333333334\n",
      "97 번째 loss, accuracy:  1.330723645318664 0.35833333333333334\n",
      "98 번째 loss, accuracy:  1.327409061085664 0.35833333333333334\n",
      "99 번째 loss, accuracy:  1.3241329309526695 0.35833333333333334\n",
      "100 번째 loss, accuracy:  1.3208934374026886 0.35833333333333334\n",
      "101 번째 loss, accuracy:  1.3176889142525274 0.35833333333333334\n",
      "102 번째 loss, accuracy:  1.3145178314366126 0.35833333333333334\n",
      "103 번째 loss, accuracy:  1.3113787815264473 0.35833333333333334\n",
      "104 번째 loss, accuracy:  1.3082704677675312 0.35833333333333334\n",
      "105 번째 loss, accuracy:  1.3051916934455536 0.35833333333333334\n",
      "106 번째 loss, accuracy:  1.302141352418359 0.35833333333333334\n",
      "107 번째 loss, accuracy:  1.2991184206723223 0.35833333333333334\n",
      "108 번째 loss, accuracy:  1.2961219487799942 0.35833333333333334\n",
      "109 번째 loss, accuracy:  1.2931510551523233 0.35833333333333334\n",
      "110 번째 loss, accuracy:  1.2902049199920929 0.35833333333333334\n",
      "111 번째 loss, accuracy:  1.2872827798675763 0.35833333333333334\n",
      "112 번째 loss, accuracy:  1.2843839228352356 0.35833333333333334\n",
      "113 번째 loss, accuracy:  1.2815076840497042 0.35833333333333334\n",
      "114 번째 loss, accuracy:  1.2786534418062123 0.35833333333333334\n",
      "115 번째 loss, accuracy:  1.2758206139680652 0.35833333333333334\n",
      "116 번째 loss, accuracy:  1.2730086547370965 0.35833333333333334\n",
      "117 번째 loss, accuracy:  1.2702170517301032 0.35833333333333334\n",
      "118 번째 loss, accuracy:  1.2674453233287528 0.35833333333333334\n",
      "119 번째 loss, accuracy:  1.264693016274161 0.35833333333333334\n",
      "120 번째 loss, accuracy:  1.2619597034807553 0.35833333333333334\n",
      "121 번째 loss, accuracy:  1.259244982046805 0.35833333333333334\n",
      "122 번째 loss, accuracy:  1.2565484714418178 0.35833333333333334\n",
      "123 번째 loss, accuracy:  1.2538698118527987 0.35833333333333334\n",
      "124 번째 loss, accuracy:  1.251208662673928 0.35833333333333334\n",
      "125 번째 loss, accuracy:  1.2485647011252468 0.35833333333333334\n",
      "126 번째 loss, accuracy:  1.245937620988142 0.35833333333333334\n",
      "127 번째 loss, accuracy:  1.243327131446244 0.35833333333333334\n",
      "128 번째 loss, accuracy:  1.240732956021763 0.35833333333333334\n",
      "129 번째 loss, accuracy:  1.2381548315983661 0.35833333333333334\n",
      "130 번째 loss, accuracy:  1.2355925075225462 0.35833333333333334\n",
      "131 번째 loss, accuracy:  1.2330457447762277 0.35833333333333334\n",
      "132 번째 loss, accuracy:  1.2305143152142322 0.35833333333333334\n",
      "133 번째 loss, accuracy:  1.2279980008606288 0.35833333333333334\n",
      "134 번째 loss, accuracy:  1.2254965932589739 0.35833333333333334\n",
      "135 번째 loss, accuracy:  1.223009892871388 0.35833333333333334\n",
      "136 번째 loss, accuracy:  1.2205377085225033 0.35833333333333334\n",
      "137 번째 loss, accuracy:  1.2180798568842446 0.35833333333333334\n",
      "138 번째 loss, accuracy:  1.215636161998017 0.35833333333333334\n",
      "139 번째 loss, accuracy:  1.2132064548311479 0.35833333333333334\n",
      "140 번째 loss, accuracy:  1.2107905728646988 0.35833333333333334\n",
      "141 번째 loss, accuracy:  1.2083883597101661 0.35833333333333334\n",
      "142 번째 loss, accuracy:  1.2059996647523792 0.35833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 번째 loss, accuracy:  1.2036243428169773 0.35833333333333334\n",
      "144 번째 loss, accuracy:  1.201262253860069 0.35833333333333334\n",
      "145 번째 loss, accuracy:  1.1989132626784709 0.35833333333333334\n",
      "146 번째 loss, accuracy:  1.196577238639004 0.35833333333333334\n",
      "147 번째 loss, accuracy:  1.194254055425193 0.35833333333333334\n",
      "148 번째 loss, accuracy:  1.1919435908001106 0.35833333333333334\n",
      "149 번째 loss, accuracy:  1.1896457263842326 0.35833333333333334\n",
      "150 번째 loss, accuracy:  1.1873603474469325 0.35833333333333334\n",
      "151 번째 loss, accuracy:  1.18508734271095 0.35833333333333334\n",
      "152 번째 loss, accuracy:  1.1828266041685007 0.35833333333333334\n",
      "153 번째 loss, accuracy:  1.1805780269084782 0.35833333333333334\n",
      "154 번째 loss, accuracy:  1.1783415089537963 0.36666666666666664\n",
      "155 번째 loss, accuracy:  1.1761169511081866 0.36666666666666664\n",
      "156 번째 loss, accuracy:  1.1739042568118192 0.375\n",
      "157 번째 loss, accuracy:  1.1717033320050523 0.375\n",
      "158 번째 loss, accuracy:  1.169514084999894 0.375\n",
      "159 번째 loss, accuracy:  1.1673364263585133 0.38333333333333336\n",
      "160 번째 loss, accuracy:  1.1651702687783827 0.38333333333333336\n",
      "161 번째 loss, accuracy:  1.1630155269835971 0.39166666666666666\n",
      "162 번째 loss, accuracy:  1.1608721176221006 0.39166666666666666\n",
      "163 번째 loss, accuracy:  1.1587399591681777 0.39166666666666666\n",
      "164 번째 loss, accuracy:  1.1566189718301403 0.4\n",
      "165 번째 loss, accuracy:  1.1545090774627345 0.4083333333333333\n",
      "166 번째 loss, accuracy:  1.152410199484005 0.4083333333333333\n",
      "167 번째 loss, accuracy:  1.1503222627963612 0.4083333333333333\n",
      "168 번째 loss, accuracy:  1.1482451937116271 0.4083333333333333\n",
      "169 번째 loss, accuracy:  1.1461789198797374 0.43333333333333335\n",
      "170 번째 loss, accuracy:  1.1441233702210154 0.43333333333333335\n",
      "171 번째 loss, accuracy:  1.1420784748617152 0.44166666666666665\n",
      "172 번째 loss, accuracy:  1.1400441650726383 0.44166666666666665\n",
      "173 번째 loss, accuracy:  1.1380203732107792 0.44166666666666665\n",
      "174 번째 loss, accuracy:  1.1360070326636806 0.44166666666666665\n",
      "175 번째 loss, accuracy:  1.1340040777964908 0.44166666666666665\n",
      "176 번째 loss, accuracy:  1.1320114439014555 0.44166666666666665\n",
      "177 번째 loss, accuracy:  1.1300290671498234 0.4666666666666667\n",
      "178 번째 loss, accuracy:  1.1280568845460228 0.48333333333333334\n",
      "179 번째 loss, accuracy:  1.1260948338839067 0.48333333333333334\n",
      "180 번째 loss, accuracy:  1.1241428537051175 0.48333333333333334\n",
      "181 번째 loss, accuracy:  1.1222008832593136 0.48333333333333334\n",
      "182 번째 loss, accuracy:  1.1202688624663244 0.49166666666666664\n",
      "183 번째 loss, accuracy:  1.1183467318799691 0.49166666666666664\n",
      "184 번째 loss, accuracy:  1.1164344326536655 0.5083333333333333\n",
      "185 번째 loss, accuracy:  1.114531906507582 0.5083333333333333\n",
      "186 번째 loss, accuracy:  1.1126390956973486 0.5166666666666667\n",
      "187 번째 loss, accuracy:  1.1107559429842147 0.5166666666666667\n",
      "188 번째 loss, accuracy:  1.1088823916066555 0.5416666666666666\n",
      "189 번째 loss, accuracy:  1.1070183852533453 0.55\n",
      "190 번째 loss, accuracy:  1.105163868037362 0.55\n",
      "191 번째 loss, accuracy:  1.103318784471712 0.5666666666666667\n",
      "192 번째 loss, accuracy:  1.1014830794460175 0.5833333333333334\n",
      "193 번째 loss, accuracy:  1.0996566982043616 0.5833333333333334\n",
      "194 번째 loss, accuracy:  1.0978395863242554 0.6\n",
      "195 번째 loss, accuracy:  1.09603168969664 0.6\n",
      "196 번째 loss, accuracy:  1.0942329545069929 0.6\n",
      "197 번째 loss, accuracy:  1.0924433272173506 0.6\n",
      "198 번째 loss, accuracy:  1.0906627545493188 0.6083333333333333\n",
      "199 번째 loss, accuracy:  1.0888911834680377 0.6083333333333333\n",
      "200 번째 loss, accuracy:  1.08712856116698 0.6083333333333333\n",
      "201 번째 loss, accuracy:  1.085374835053708 0.6083333333333333\n",
      "202 번째 loss, accuracy:  1.0836299527363473 0.6083333333333333\n",
      "203 번째 loss, accuracy:  1.0818938620109657 0.6083333333333333\n",
      "204 번째 loss, accuracy:  1.0801665108496623 0.6166666666666667\n",
      "205 번째 loss, accuracy:  1.0784478473894912 0.625\n",
      "206 번째 loss, accuracy:  1.0767378199220525 0.625\n",
      "207 번째 loss, accuracy:  1.0750363768838451 0.625\n",
      "208 번째 loss, accuracy:  1.073343466847225 0.625\n",
      "209 번째 loss, accuracy:  1.0716590385121292 0.625\n",
      "210 번째 loss, accuracy:  1.0699830406983637 0.625\n",
      "211 번째 loss, accuracy:  1.0683154223385931 0.625\n",
      "212 번째 loss, accuracy:  1.0666561324718615 0.625\n",
      "213 번째 loss, accuracy:  1.0650051202377562 0.625\n",
      "214 번째 loss, accuracy:  1.063362334871108 0.625\n",
      "215 번째 loss, accuracy:  1.0617277256972657 0.6416666666666667\n",
      "216 번째 loss, accuracy:  1.0601012421278644 0.6416666666666667\n",
      "217 번째 loss, accuracy:  1.0584828336571808 0.6416666666666667\n",
      "218 번째 loss, accuracy:  1.0568724498589397 0.6416666666666667\n",
      "219 번째 loss, accuracy:  1.0552700403836137 0.6416666666666667\n",
      "220 번째 loss, accuracy:  1.0536755549561976 0.65\n",
      "221 번째 loss, accuracy:  1.0520889433744467 0.6583333333333333\n",
      "222 번째 loss, accuracy:  1.0505101555075698 0.6583333333333333\n",
      "223 번째 loss, accuracy:  1.0489391412953093 0.6583333333333333\n",
      "224 번째 loss, accuracy:  1.047375850747429 0.6666666666666666\n",
      "225 번째 loss, accuracy:  1.0458202339436873 0.6666666666666666\n",
      "226 번째 loss, accuracy:  1.04427224103409 0.675\n",
      "227 번째 loss, accuracy:  1.0427318222395774 0.675\n",
      "228 번째 loss, accuracy:  1.041198927853044 0.6833333333333333\n",
      "229 번째 loss, accuracy:  1.039673508240703 0.6833333333333333\n",
      "230 번째 loss, accuracy:  1.0381555138438168 0.6833333333333333\n",
      "231 번째 loss, accuracy:  1.0366448951807152 0.6833333333333333\n",
      "232 번째 loss, accuracy:  1.0351416028490878 0.6833333333333333\n",
      "233 번째 loss, accuracy:  1.033645587528649 0.6833333333333333\n",
      "234 번째 loss, accuracy:  1.0321567999840333 0.6833333333333333\n",
      "235 번째 loss, accuracy:  1.0306751910679706 0.6833333333333333\n",
      "236 번째 loss, accuracy:  1.0292007117247206 0.6833333333333333\n",
      "237 번째 loss, accuracy:  1.02773331299374 0.6833333333333333\n",
      "238 번째 loss, accuracy:  1.0262729460136173 0.6916666666666667\n",
      "239 번째 loss, accuracy:  1.0248195620261908 0.6916666666666667\n",
      "240 번째 loss, accuracy:  1.0233731123808798 0.6916666666666667\n",
      "241 번째 loss, accuracy:  1.0219335485392453 0.6916666666666667\n",
      "242 번째 loss, accuracy:  1.020500822079709 0.6916666666666667\n",
      "243 번째 loss, accuracy:  1.0190748847024358 0.6916666666666667\n",
      "244 번째 loss, accuracy:  1.0176556882344319 0.6916666666666667\n",
      "245 번째 loss, accuracy:  1.0162431846347166 0.6916666666666667\n",
      "246 번째 loss, accuracy:  1.0148373259997 0.6916666666666667\n",
      "247 번째 loss, accuracy:  1.0134380645686583 0.6916666666666667\n",
      "248 번째 loss, accuracy:  1.0120453527293303 0.6916666666666667\n",
      "249 번째 loss, accuracy:  1.0106591430236207 0.6916666666666667\n",
      "250 번째 loss, accuracy:  1.0092793881533941 0.6916666666666667\n",
      "251 번째 loss, accuracy:  1.0079060409863665 0.6916666666666667\n",
      "252 번째 loss, accuracy:  1.0065390545620285 0.6916666666666667\n",
      "253 번째 loss, accuracy:  1.0051783820976943 0.6916666666666667\n",
      "254 번째 loss, accuracy:  1.0038239769945214 0.6916666666666667\n",
      "255 번째 loss, accuracy:  1.002475792843659 0.6916666666666667\n",
      "256 번째 loss, accuracy:  1.001133783432331 0.6916666666666667\n",
      "257 번째 loss, accuracy:  0.9997979027500236 0.6916666666666667\n",
      "258 번째 loss, accuracy:  0.9984681049946039 0.6916666666666667\n",
      "259 번째 loss, accuracy:  0.9971443445784746 0.6916666666666667\n",
      "260 번째 loss, accuracy:  0.9958265761347238 0.6916666666666667\n",
      "261 번째 loss, accuracy:  0.9945147545232221 0.6916666666666667\n",
      "262 번째 loss, accuracy:  0.9932088348366996 0.6916666666666667\n",
      "263 번째 loss, accuracy:  0.9919087724067813 0.6916666666666667\n",
      "264 번째 loss, accuracy:  0.9906145228099302 0.6916666666666667\n",
      "265 번째 loss, accuracy:  0.9893260418734086 0.6916666666666667\n",
      "266 번째 loss, accuracy:  0.9880432856810715 0.6916666666666667\n",
      "267 번째 loss, accuracy:  0.9867662105791252 0.6916666666666667\n",
      "268 번째 loss, accuracy:  0.9854947731817989 0.6916666666666667\n",
      "269 번째 loss, accuracy:  0.9842289303768873 0.6916666666666667\n",
      "270 번째 loss, accuracy:  0.9829686393312053 0.6916666666666667\n",
      "271 번째 loss, accuracy:  0.9817138574958887 0.6916666666666667\n",
      "272 번째 loss, accuracy:  0.9804645426116235 0.6916666666666667\n",
      "273 번째 loss, accuracy:  0.9792206527136836 0.6916666666666667\n",
      "274 번째 loss, accuracy:  0.9779821461368567 0.6916666666666667\n",
      "275 번째 loss, accuracy:  0.9767489815201988 0.6916666666666667\n",
      "276 번째 loss, accuracy:  0.9755211178116308 0.6916666666666667\n",
      "277 번째 loss, accuracy:  0.9742985142724016 0.6916666666666667\n",
      "278 번째 loss, accuracy:  0.9730811304813245 0.6916666666666667\n",
      "279 번째 loss, accuracy:  0.9718689263388814 0.6916666666666667\n",
      "280 번째 loss, accuracy:  0.9706618620711025 0.6916666666666667\n",
      "281 번째 loss, accuracy:  0.969459898233277 0.6916666666666667\n",
      "282 번째 loss, accuracy:  0.9682629957134667 0.6916666666666667\n",
      "283 번째 loss, accuracy:  0.9670711157358056 0.6916666666666667\n",
      "284 번째 loss, accuracy:  0.9658842198635841 0.6916666666666667\n",
      "285 번째 loss, accuracy:  0.964702270002162 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286 번째 loss, accuracy:  0.9635252284016161 0.6916666666666667\n",
      "287 번째 loss, accuracy:  0.9623530576591787 0.6916666666666667\n",
      "288 번째 loss, accuracy:  0.9611857207214832 0.6916666666666667\n",
      "289 번째 loss, accuracy:  0.9600231808865728 0.6916666666666667\n",
      "290 번째 loss, accuracy:  0.9588654018056321 0.6916666666666667\n",
      "291 번째 loss, accuracy:  0.9577123474845647 0.6916666666666667\n",
      "292 번째 loss, accuracy:  0.9565639822852753 0.6916666666666667\n",
      "293 번째 loss, accuracy:  0.9554202709267685 0.6916666666666667\n",
      "294 번째 loss, accuracy:  0.9542811784859742 0.6916666666666667\n",
      "295 번째 loss, accuracy:  0.9531466703983528 0.6916666666666667\n",
      "296 번째 loss, accuracy:  0.9520167124582783 0.6916666666666667\n",
      "297 번째 loss, accuracy:  0.9508912708191709 0.6916666666666667\n",
      "298 번째 loss, accuracy:  0.9497703119933975 0.6916666666666667\n",
      "299 번째 loss, accuracy:  0.9486538028519561 0.6916666666666667\n",
      "300 번째 loss, accuracy:  0.9475417106239107 0.6916666666666667\n",
      "301 번째 loss, accuracy:  0.9464340028956238 0.6916666666666667\n",
      "302 번째 loss, accuracy:  0.9453306476097401 0.6916666666666667\n",
      "303 번째 loss, accuracy:  0.944231613063959 0.6916666666666667\n",
      "304 번째 loss, accuracy:  0.9431368679095902 0.6916666666666667\n",
      "305 번째 loss, accuracy:  0.9420463811498947 0.6916666666666667\n",
      "306 번째 loss, accuracy:  0.9409601221382182 0.6916666666666667\n",
      "307 번째 loss, accuracy:  0.9398780605759064 0.6916666666666667\n",
      "308 번째 loss, accuracy:  0.9388001665100488 0.6916666666666667\n",
      "309 번째 loss, accuracy:  0.9377264103309898 0.6916666666666667\n",
      "310 번째 loss, accuracy:  0.9366567627696939 0.6916666666666667\n",
      "311 번째 loss, accuracy:  0.9355911948948861 0.6916666666666667\n",
      "312 번째 loss, accuracy:  0.9345296781100374 0.6916666666666667\n",
      "313 번째 loss, accuracy:  0.9334721841501924 0.6916666666666667\n",
      "314 번째 loss, accuracy:  0.9324186850786045 0.6916666666666667\n",
      "315 번째 loss, accuracy:  0.9313691532832441 0.6916666666666667\n",
      "316 번째 loss, accuracy:  0.9303235614731206 0.6916666666666667\n",
      "317 번째 loss, accuracy:  0.9292818826745254 0.6916666666666667\n",
      "318 번째 loss, accuracy:  0.9282440902270609 0.6916666666666667\n",
      "319 번째 loss, accuracy:  0.9272101577796384 0.6916666666666667\n",
      "320 번째 loss, accuracy:  0.9261800592862607 0.6916666666666667\n",
      "321 번째 loss, accuracy:  0.9251537690017735 0.6916666666666667\n",
      "322 번째 loss, accuracy:  0.924131261477481 0.6916666666666667\n",
      "323 번째 loss, accuracy:  0.9231125115566624 0.6916666666666667\n",
      "324 번째 loss, accuracy:  0.9220974943700428 0.6916666666666667\n",
      "325 번째 loss, accuracy:  0.9210861853311478 0.6916666666666667\n",
      "326 번째 loss, accuracy:  0.9200785601315983 0.6916666666666667\n",
      "327 번째 loss, accuracy:  0.9190745947363915 0.6916666666666667\n",
      "328 번째 loss, accuracy:  0.9180742653790674 0.6916666666666667\n",
      "329 번째 loss, accuracy:  0.9170775485568842 0.6916666666666667\n",
      "330 번째 loss, accuracy:  0.9160844210259291 0.6916666666666667\n",
      "331 번째 loss, accuracy:  0.915094859796233 0.6916666666666667\n",
      "332 번째 loss, accuracy:  0.9141088421268471 0.6916666666666667\n",
      "333 번째 loss, accuracy:  0.9131263455209154 0.6916666666666667\n",
      "334 번째 loss, accuracy:  0.91214734772076 0.6916666666666667\n",
      "335 번째 loss, accuracy:  0.9111718267029658 0.6916666666666667\n",
      "336 번째 loss, accuracy:  0.9101997606734604 0.6916666666666667\n",
      "337 번째 loss, accuracy:  0.9092311280626594 0.6916666666666667\n",
      "338 번째 loss, accuracy:  0.9082659075206122 0.6916666666666667\n",
      "339 번째 loss, accuracy:  0.9073040779121779 0.6916666666666667\n",
      "340 번째 loss, accuracy:  0.9063456183122675 0.6916666666666667\n",
      "341 번째 loss, accuracy:  0.9053905080011134 0.6916666666666667\n",
      "342 번째 loss, accuracy:  0.90443872645961 0.6916666666666667\n",
      "343 번째 loss, accuracy:  0.9034902533647114 0.6916666666666667\n",
      "344 번째 loss, accuracy:  0.9025450685848979 0.6916666666666667\n",
      "345 번째 loss, accuracy:  0.9016031521756962 0.6916666666666667\n",
      "346 번째 loss, accuracy:  0.9006644843753158 0.6916666666666667\n",
      "347 번째 loss, accuracy:  0.899729045600314 0.6916666666666667\n",
      "348 번째 loss, accuracy:  0.8987968164413835 0.6916666666666667\n",
      "349 번째 loss, accuracy:  0.8978677776592411 0.6916666666666667\n",
      "350 번째 loss, accuracy:  0.8969419101805641 0.6916666666666667\n",
      "351 번째 loss, accuracy:  0.8960191950940566 0.6916666666666667\n",
      "352 번째 loss, accuracy:  0.8950996136466168 0.6916666666666667\n",
      "353 번째 loss, accuracy:  0.8941831472395806 0.6916666666666667\n",
      "354 번째 loss, accuracy:  0.8932697774251027 0.6916666666666667\n",
      "355 번째 loss, accuracy:  0.8923594859026055 0.6916666666666667\n",
      "356 번째 loss, accuracy:  0.891452254515379 0.6916666666666667\n",
      "357 번째 loss, accuracy:  0.8905480652472626 0.6916666666666667\n",
      "358 번째 loss, accuracy:  0.889646900219449 0.6916666666666667\n",
      "359 번째 loss, accuracy:  0.8887487416873762 0.6916666666666667\n",
      "360 번째 loss, accuracy:  0.887853572037775 0.6916666666666667\n",
      "361 번째 loss, accuracy:  0.8869613737857874 0.6916666666666667\n",
      "362 번째 loss, accuracy:  0.886072129572223 0.6916666666666667\n",
      "363 번째 loss, accuracy:  0.8851858221609175 0.6916666666666667\n",
      "364 번째 loss, accuracy:  0.8843024344361969 0.6916666666666667\n",
      "365 번째 loss, accuracy:  0.8834219494004601 0.6916666666666667\n",
      "366 번째 loss, accuracy:  0.8825443501718835 0.6916666666666667\n",
      "367 번째 loss, accuracy:  0.8816696199822013 0.6916666666666667\n",
      "368 번째 loss, accuracy:  0.880797742174621 0.6916666666666667\n",
      "369 번째 loss, accuracy:  0.8799287002018411 0.6916666666666667\n",
      "370 번째 loss, accuracy:  0.8790624776241519 0.6916666666666667\n",
      "371 번째 loss, accuracy:  0.8781990581076681 0.6916666666666667\n",
      "372 번째 loss, accuracy:  0.8773384254226297 0.6916666666666667\n",
      "373 번째 loss, accuracy:  0.8764805634418357 0.6916666666666667\n",
      "374 번째 loss, accuracy:  0.8756254561391266 0.6916666666666667\n",
      "375 번째 loss, accuracy:  0.8747730875880028 0.6916666666666667\n",
      "376 번째 loss, accuracy:  0.8739234419603176 0.6916666666666667\n",
      "377 번째 loss, accuracy:  0.8730765035250397 0.6916666666666667\n",
      "378 번째 loss, accuracy:  0.8722322566471215 0.6916666666666667\n",
      "379 번째 loss, accuracy:  0.8713906857864516 0.6916666666666667\n",
      "380 번째 loss, accuracy:  0.8705517754968544 0.6916666666666667\n",
      "381 번째 loss, accuracy:  0.8697155104252119 0.6916666666666667\n",
      "382 번째 loss, accuracy:  0.8688818753106012 0.6916666666666667\n",
      "383 번째 loss, accuracy:  0.8680508549835679 0.6916666666666667\n",
      "384 번째 loss, accuracy:  0.8672224343653983 0.6916666666666667\n",
      "385 번째 loss, accuracy:  0.866396598467507 0.6916666666666667\n",
      "386 번째 loss, accuracy:  0.8655733323908615 0.6916666666666667\n",
      "387 번째 loss, accuracy:  0.8647526213254514 0.6916666666666667\n",
      "388 번째 loss, accuracy:  0.8639344505498436 0.6916666666666667\n",
      "389 번째 loss, accuracy:  0.8631188054307669 0.6916666666666667\n",
      "390 번째 loss, accuracy:  0.8623056714227381 0.6916666666666667\n",
      "391 번째 loss, accuracy:  0.8614950340677615 0.6916666666666667\n",
      "392 번째 loss, accuracy:  0.8606868789950418 0.6916666666666667\n",
      "393 번째 loss, accuracy:  0.8598811919207445 0.6916666666666667\n",
      "394 번째 loss, accuracy:  0.8590779586478037 0.6916666666666667\n",
      "395 번째 loss, accuracy:  0.8582771650657507 0.6916666666666667\n",
      "396 번째 loss, accuracy:  0.857478797150574 0.6916666666666667\n",
      "397 번째 loss, accuracy:  0.8566828409646212 0.6916666666666667\n",
      "398 번째 loss, accuracy:  0.8558892826565079 0.6916666666666667\n",
      "399 번째 loss, accuracy:  0.8550981084610568 0.6916666666666667\n",
      "400 번째 loss, accuracy:  0.8543093046992785 0.6916666666666667\n",
      "401 번째 loss, accuracy:  0.8535228577783248 0.6916666666666667\n",
      "402 번째 loss, accuracy:  0.8527387541915107 0.6916666666666667\n",
      "403 번째 loss, accuracy:  0.851956980518306 0.6916666666666667\n",
      "404 번째 loss, accuracy:  0.8511775234243744 0.6916666666666667\n",
      "405 번째 loss, accuracy:  0.8504003696615963 0.6916666666666667\n",
      "406 번째 loss, accuracy:  0.8496255060681156 0.6916666666666667\n",
      "407 번째 loss, accuracy:  0.8488529195683883 0.6916666666666667\n",
      "408 번째 loss, accuracy:  0.848082597173233 0.6916666666666667\n",
      "409 번째 loss, accuracy:  0.8473145259798951 0.6916666666666667\n",
      "410 번째 loss, accuracy:  0.8465486931720948 0.6916666666666667\n",
      "411 번째 loss, accuracy:  0.845785086020089 0.6916666666666667\n",
      "412 번째 loss, accuracy:  0.8450236918807398 0.6916666666666667\n",
      "413 번째 loss, accuracy:  0.8442644981975512 0.6916666666666667\n",
      "414 번째 loss, accuracy:  0.8435074925007277 0.6916666666666667\n",
      "415 번째 loss, accuracy:  0.842752662407226 0.6916666666666667\n",
      "416 번째 loss, accuracy:  0.8419999956207926 0.6916666666666667\n",
      "417 번째 loss, accuracy:  0.8412494799319944 0.6916666666666667\n",
      "418 번째 loss, accuracy:  0.8405011032182412 0.6916666666666667\n",
      "419 번째 loss, accuracy:  0.8397548534438196 0.6916666666666667\n",
      "420 번째 loss, accuracy:  0.8390107186598823 0.6916666666666667\n",
      "421 번째 loss, accuracy:  0.8382686870044564 0.6916666666666667\n",
      "422 번째 loss, accuracy:  0.8375287467024344 0.6916666666666667\n",
      "423 번째 loss, accuracy:  0.8367908860655439 0.6916666666666667\n",
      "424 번째 loss, accuracy:  0.8360550934923008 0.6916666666666667\n",
      "425 번째 loss, accuracy:  0.8353213574679984 0.6916666666666667\n",
      "426 번째 loss, accuracy:  0.834589666564609 0.6916666666666667\n",
      "427 번째 loss, accuracy:  0.8338600094407286 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428 번째 loss, accuracy:  0.8331323748414962 0.6916666666666667\n",
      "429 번째 loss, accuracy:  0.8324067515984847 0.6916666666666667\n",
      "430 번째 loss, accuracy:  0.8316831286295925 0.6916666666666667\n",
      "431 번째 loss, accuracy:  0.8309614949389129 0.6916666666666667\n",
      "432 번째 loss, accuracy:  0.8302418396166032 0.6916666666666667\n",
      "433 번째 loss, accuracy:  0.8295241518387212 0.6916666666666667\n",
      "434 번째 loss, accuracy:  0.8288084208670655 0.6916666666666667\n",
      "435 번째 loss, accuracy:  0.8280946360489883 0.6916666666666667\n",
      "436 번째 loss, accuracy:  0.8273827868171981 0.6916666666666667\n",
      "437 번째 loss, accuracy:  0.8266728626895593 0.6916666666666667\n",
      "438 번째 loss, accuracy:  0.8259648532688552 0.6916666666666667\n",
      "439 번째 loss, accuracy:  0.825258748242575 0.6916666666666667\n",
      "440 번째 loss, accuracy:  0.824554537382632 0.6916666666666667\n",
      "441 번째 loss, accuracy:  0.8238522105451246 0.6916666666666667\n",
      "442 번째 loss, accuracy:  0.8231517576700559 0.6916666666666667\n",
      "443 번째 loss, accuracy:  0.8224531687810341 0.6916666666666667\n",
      "444 번째 loss, accuracy:  0.8217564339849813 0.6916666666666667\n",
      "445 번째 loss, accuracy:  0.8210615434718185 0.6916666666666667\n",
      "446 번째 loss, accuracy:  0.820368487514127 0.6916666666666667\n",
      "447 번째 loss, accuracy:  0.819677256466831 0.6916666666666667\n",
      "448 번째 loss, accuracy:  0.8189878407668296 0.6916666666666667\n",
      "449 번째 loss, accuracy:  0.8183002309326461 0.6916666666666667\n",
      "450 번째 loss, accuracy:  0.8176144175640483 0.6916666666666667\n",
      "451 번째 loss, accuracy:  0.8169303913416786 0.6916666666666667\n",
      "452 번째 loss, accuracy:  0.816248143026644 0.6916666666666667\n",
      "453 번째 loss, accuracy:  0.8155676634601247 0.6916666666666667\n",
      "454 번째 loss, accuracy:  0.8148889435629493 0.6916666666666667\n",
      "455 번째 loss, accuracy:  0.8142119743351972 0.6916666666666667\n",
      "456 번째 loss, accuracy:  0.8135367468557356 0.6916666666666667\n",
      "457 번째 loss, accuracy:  0.8128632522818007 0.6916666666666667\n",
      "458 번째 loss, accuracy:  0.8121914818485372 0.6916666666666667\n",
      "459 번째 loss, accuracy:  0.8115214268685537 0.6916666666666667\n",
      "460 번째 loss, accuracy:  0.810853078731445 0.6916666666666667\n",
      "461 번째 loss, accuracy:  0.8101864289033295 0.6916666666666667\n",
      "462 번째 loss, accuracy:  0.8095214689263599 0.6916666666666667\n",
      "463 번째 loss, accuracy:  0.808858190418248 0.6916666666666667\n",
      "464 번째 loss, accuracy:  0.8081965850717618 0.6916666666666667\n",
      "465 번째 loss, accuracy:  0.8075366446542417 0.6916666666666667\n",
      "466 번째 loss, accuracy:  0.8068783610070712 0.6916666666666667\n",
      "467 번째 loss, accuracy:  0.8062217260451952 0.6916666666666667\n",
      "468 번째 loss, accuracy:  0.8055667317565792 0.6916666666666667\n",
      "469 번째 loss, accuracy:  0.8049133702017037 0.6916666666666667\n",
      "470 번째 loss, accuracy:  0.8042616335130371 0.6916666666666667\n",
      "471 번째 loss, accuracy:  0.8036115138945052 0.6916666666666667\n",
      "472 번째 loss, accuracy:  0.8029630036209511 0.6916666666666667\n",
      "473 번째 loss, accuracy:  0.8023160950376028 0.6916666666666667\n",
      "474 번째 loss, accuracy:  0.8016707805595388 0.6916666666666667\n",
      "475 번째 loss, accuracy:  0.8010270526711264 0.6916666666666667\n",
      "476 번째 loss, accuracy:  0.8003849039254985 0.6916666666666667\n",
      "477 번째 loss, accuracy:  0.7997443269439769 0.6916666666666667\n",
      "478 번째 loss, accuracy:  0.7991053144155399 0.6916666666666667\n",
      "479 번째 loss, accuracy:  0.7984678590962558 0.6916666666666667\n",
      "480 번째 loss, accuracy:  0.7978319538087333 0.6916666666666667\n",
      "481 번째 loss, accuracy:  0.7971975914415592 0.6916666666666667\n",
      "482 번째 loss, accuracy:  0.796564764948732 0.6916666666666667\n",
      "483 번째 loss, accuracy:  0.795933467349119 0.6916666666666667\n",
      "484 번째 loss, accuracy:  0.7953036917258698 0.6916666666666667\n",
      "485 번째 loss, accuracy:  0.7946754312258686 0.6916666666666667\n",
      "486 번째 loss, accuracy:  0.7940486790591597 0.6916666666666667\n",
      "487 번째 loss, accuracy:  0.7934234284983946 0.6916666666666667\n",
      "488 번째 loss, accuracy:  0.7927996728782546 0.6916666666666667\n",
      "489 번째 loss, accuracy:  0.7921774055948915 0.6916666666666667\n",
      "490 번째 loss, accuracy:  0.7915566201053629 0.6916666666666667\n",
      "491 번째 loss, accuracy:  0.7909373099270601 0.6916666666666667\n",
      "492 번째 loss, accuracy:  0.7903194686371524 0.6916666666666667\n",
      "493 번째 loss, accuracy:  0.7897030898720153 0.6916666666666667\n",
      "494 번째 loss, accuracy:  0.7890881673266716 0.6916666666666667\n",
      "495 번째 loss, accuracy:  0.7884746947542233 0.6916666666666667\n",
      "496 번째 loss, accuracy:  0.7878626659652933 0.6916666666666667\n",
      "497 번째 loss, accuracy:  0.7872520748274617 0.6916666666666667\n",
      "498 번째 loss, accuracy:  0.786642915264721 0.6916666666666667\n",
      "499 번째 loss, accuracy:  0.7860351812568869 0.6916666666666667\n",
      "500 번째 loss, accuracy:  0.7854288668390691 0.6916666666666667\n",
      "501 번째 loss, accuracy:  0.7848239661011077 0.6916666666666667\n",
      "502 번째 loss, accuracy:  0.7842204731870209 0.6916666666666667\n",
      "503 번째 loss, accuracy:  0.7836183822944389 0.6916666666666667\n",
      "504 번째 loss, accuracy:  0.7830176876740815 0.6916666666666667\n",
      "505 번째 loss, accuracy:  0.7824183836291922 0.6916666666666667\n",
      "506 번째 loss, accuracy:  0.7818204645149908 0.6916666666666667\n",
      "507 번째 loss, accuracy:  0.7812239247381441 0.6916666666666667\n",
      "508 번째 loss, accuracy:  0.7806287587562101 0.6916666666666667\n",
      "509 번째 loss, accuracy:  0.7800349610771204 0.6916666666666667\n",
      "510 번째 loss, accuracy:  0.7794425262586112 0.6916666666666667\n",
      "511 번째 loss, accuracy:  0.7788514489077286 0.6916666666666667\n",
      "512 번째 loss, accuracy:  0.778261723680271 0.6916666666666667\n",
      "513 번째 loss, accuracy:  0.7776733452802725 0.6916666666666667\n",
      "514 번째 loss, accuracy:  0.7770863084594704 0.6916666666666667\n",
      "515 번째 loss, accuracy:  0.7765006080167917 0.6916666666666667\n",
      "516 번째 loss, accuracy:  0.7759162387978347 0.6916666666666667\n",
      "517 번째 loss, accuracy:  0.775333195694348 0.6916666666666667\n",
      "518 번째 loss, accuracy:  0.7747514736437175 0.6916666666666667\n",
      "519 번째 loss, accuracy:  0.7741710676284512 0.6916666666666667\n",
      "520 번째 loss, accuracy:  0.7735919726756918 0.6916666666666667\n",
      "521 번째 loss, accuracy:  0.7730141838566951 0.6916666666666667\n",
      "522 번째 loss, accuracy:  0.7724376962863242 0.6916666666666667\n",
      "523 번째 loss, accuracy:  0.7718625051225861 0.6916666666666667\n",
      "524 번째 loss, accuracy:  0.7712886055660986 0.6916666666666667\n",
      "525 번째 loss, accuracy:  0.7707159928596267 0.6916666666666667\n",
      "526 번째 loss, accuracy:  0.770144662287583 0.6916666666666667\n",
      "527 번째 loss, accuracy:  0.7695746091755519 0.6916666666666667\n",
      "528 번째 loss, accuracy:  0.7690058288898063 0.6916666666666667\n",
      "529 번째 loss, accuracy:  0.7684383168368285 0.6916666666666667\n",
      "530 번째 loss, accuracy:  0.7678720684628467 0.6916666666666667\n",
      "531 번째 loss, accuracy:  0.7673070792533587 0.6916666666666667\n",
      "532 번째 loss, accuracy:  0.7667433447326697 0.6916666666666667\n",
      "533 번째 loss, accuracy:  0.7661808604634266 0.6916666666666667\n",
      "534 번째 loss, accuracy:  0.7656196220461645 0.6916666666666667\n",
      "535 번째 loss, accuracy:  0.7650596251188558 0.6916666666666667\n",
      "536 번째 loss, accuracy:  0.764500865356451 0.6916666666666667\n",
      "537 번째 loss, accuracy:  0.7639433384704348 0.6916666666666667\n",
      "538 번째 loss, accuracy:  0.7633870402083853 0.6916666666666667\n",
      "539 번째 loss, accuracy:  0.7628319663535291 0.6916666666666667\n",
      "540 번째 loss, accuracy:  0.7622781127243187 0.6916666666666667\n",
      "541 번째 loss, accuracy:  0.7617254751739871 0.6916666666666667\n",
      "542 번째 loss, accuracy:  0.7611740495901244 0.6916666666666667\n",
      "543 번째 loss, accuracy:  0.7606238318942574 0.6916666666666667\n",
      "544 번째 loss, accuracy:  0.7600748180414126 0.6916666666666667\n",
      "545 번째 loss, accuracy:  0.7595270040197257 0.6916666666666667\n",
      "546 번째 loss, accuracy:  0.7589803858500086 0.6916666666666667\n",
      "547 번째 loss, accuracy:  0.7584349595853457 0.6916666666666667\n",
      "548 번째 loss, accuracy:  0.7578907213106911 0.6916666666666667\n",
      "549 번째 loss, accuracy:  0.7573476671424682 0.6916666666666667\n",
      "550 번째 loss, accuracy:  0.7568057932281635 0.6916666666666667\n",
      "551 번째 loss, accuracy:  0.7562650957459441 0.6916666666666667\n",
      "552 번째 loss, accuracy:  0.7557255709042543 0.6916666666666667\n",
      "553 번째 loss, accuracy:  0.7551872149414429 0.6916666666666667\n",
      "554 번째 loss, accuracy:  0.7546500241253717 0.6916666666666667\n",
      "555 번째 loss, accuracy:  0.754113994753041 0.6916666666666667\n",
      "556 번째 loss, accuracy:  0.7535791231502086 0.6916666666666667\n",
      "557 번째 loss, accuracy:  0.7530454056710251 0.6916666666666667\n",
      "558 번째 loss, accuracy:  0.7525128386976685 0.6916666666666667\n",
      "559 번째 loss, accuracy:  0.7519814186399694 0.6916666666666667\n",
      "560 번째 loss, accuracy:  0.7514511419350565 0.6916666666666667\n",
      "561 번째 loss, accuracy:  0.7509220050470056 0.6916666666666667\n",
      "562 번째 loss, accuracy:  0.750394004466481 0.6916666666666667\n",
      "563 번째 loss, accuracy:  0.7498671367103877 0.6916666666666667\n",
      "564 번째 loss, accuracy:  0.7493413983215264 0.6916666666666667\n",
      "565 번째 loss, accuracy:  0.7488167858682538 0.6916666666666667\n",
      "566 번째 loss, accuracy:  0.7482932959441418 0.6916666666666667\n",
      "567 번째 loss, accuracy:  0.7477709251676433 0.6916666666666667\n",
      "568 번째 loss, accuracy:  0.7472496701817659 0.6916666666666667\n",
      "569 번째 loss, accuracy:  0.7467295276537372 0.6916666666666667\n",
      "570 번째 loss, accuracy:  0.7462104942746884 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571 번째 loss, accuracy:  0.7456925667593358 0.6916666666666667\n",
      "572 번째 loss, accuracy:  0.745175741845653 0.6916666666666667\n",
      "573 번째 loss, accuracy:  0.744660016294573 0.6916666666666667\n",
      "574 번째 loss, accuracy:  0.7441453868896688 0.6916666666666667\n",
      "575 번째 loss, accuracy:  0.7436318504368552 0.6916666666666667\n",
      "576 번째 loss, accuracy:  0.7431194037640807 0.6916666666666667\n",
      "577 번째 loss, accuracy:  0.7426080437210257 0.6916666666666667\n",
      "578 번째 loss, accuracy:  0.7420977671788179 0.6916666666666667\n",
      "579 번째 loss, accuracy:  0.741588571029737 0.6916666666666667\n",
      "580 번째 loss, accuracy:  0.7410804521869192 0.6916666666666667\n",
      "581 번째 loss, accuracy:  0.7405734075840816 0.6916666666666667\n",
      "582 번째 loss, accuracy:  0.7400674341752328 0.6916666666666667\n",
      "583 번째 loss, accuracy:  0.7395625289344052 0.6916666666666667\n",
      "584 번째 loss, accuracy:  0.739058688855369 0.6916666666666667\n",
      "585 번째 loss, accuracy:  0.7385559109513581 0.6916666666666667\n",
      "586 번째 loss, accuracy:  0.7380541922548252 0.6916666666666667\n",
      "587 번째 loss, accuracy:  0.7375535298171492 0.6916666666666667\n",
      "588 번째 loss, accuracy:  0.7370539207083906 0.6916666666666667\n",
      "589 번째 loss, accuracy:  0.7365553620170211 0.6916666666666667\n",
      "590 번째 loss, accuracy:  0.7360578508496772 0.6916666666666667\n",
      "591 번째 loss, accuracy:  0.7355613843309089 0.6916666666666667\n",
      "592 번째 loss, accuracy:  0.7350659596029253 0.6916666666666667\n",
      "593 번째 loss, accuracy:  0.734571573825347 0.6916666666666667\n",
      "594 번째 loss, accuracy:  0.7340782241749723 0.6916666666666667\n",
      "595 번째 loss, accuracy:  0.73358590784553 0.6916666666666667\n",
      "596 번째 loss, accuracy:  0.7330946220474495 0.6916666666666667\n",
      "597 번째 loss, accuracy:  0.7326043640076136 0.6916666666666667\n",
      "598 번째 loss, accuracy:  0.7321151309691502 0.6916666666666667\n",
      "599 번째 loss, accuracy:  0.7316269201911806 0.6916666666666667\n",
      "600 번째 loss, accuracy:  0.7311397289486157 0.6916666666666667\n",
      "601 번째 loss, accuracy:  0.7306535545319224 0.6916666666666667\n",
      "602 번째 loss, accuracy:  0.730168394246903 0.6916666666666667\n",
      "603 번째 loss, accuracy:  0.7296842454144892 0.6916666666666667\n",
      "604 번째 loss, accuracy:  0.7292011053705235 0.6916666666666667\n",
      "605 번째 loss, accuracy:  0.7287189714655483 0.6916666666666667\n",
      "606 번째 loss, accuracy:  0.7282378410646047 0.6916666666666667\n",
      "607 번째 loss, accuracy:  0.7277577115470183 0.6916666666666667\n",
      "608 번째 loss, accuracy:  0.7272785803062012 0.6916666666666667\n",
      "609 번째 loss, accuracy:  0.7268004447494678 0.6916666666666667\n",
      "610 번째 loss, accuracy:  0.7263233022978113 0.6916666666666667\n",
      "611 번째 loss, accuracy:  0.7258471503857297 0.6916666666666667\n",
      "612 번째 loss, accuracy:  0.7253719864610282 0.6916666666666667\n",
      "613 번째 loss, accuracy:  0.7248978079846363 0.6916666666666667\n",
      "614 번째 loss, accuracy:  0.7244246124304141 0.6916666666666667\n",
      "615 번째 loss, accuracy:  0.7239523972849782 0.6916666666666667\n",
      "616 번째 loss, accuracy:  0.7234811600475157 0.6916666666666667\n",
      "617 번째 loss, accuracy:  0.7230108982296047 0.6916666666666667\n",
      "618 번째 loss, accuracy:  0.7225416093550439 0.6916666666666667\n",
      "619 번째 loss, accuracy:  0.7220732909596793 0.6916666666666667\n",
      "620 번째 loss, accuracy:  0.7216059405912264 0.6916666666666667\n",
      "621 번째 loss, accuracy:  0.7211395558091198 0.6916666666666667\n",
      "622 번째 loss, accuracy:  0.7206741341843229 0.6916666666666667\n",
      "623 번째 loss, accuracy:  0.7202096732991872 0.6916666666666667\n",
      "624 번째 loss, accuracy:  0.7197461707472778 0.6916666666666667\n",
      "625 번째 loss, accuracy:  0.7192836241332249 0.6916666666666667\n",
      "626 번째 loss, accuracy:  0.718822031072555 0.6916666666666667\n",
      "627 번째 loss, accuracy:  0.7183613891915537 0.6916666666666667\n",
      "628 번째 loss, accuracy:  0.7179016961270993 0.6916666666666667\n",
      "629 번째 loss, accuracy:  0.7174429495265199 0.6916666666666667\n",
      "630 번째 loss, accuracy:  0.7169851470474489 0.6916666666666667\n",
      "631 번째 loss, accuracy:  0.7165282863576832 0.6916666666666667\n",
      "632 번째 loss, accuracy:  0.716072365135025 0.6916666666666667\n",
      "633 번째 loss, accuracy:  0.7156173810671578 0.6916666666666667\n",
      "634 번째 loss, accuracy:  0.7151633318515019 0.6916666666666667\n",
      "635 번째 loss, accuracy:  0.7147102151950745 0.6916666666666667\n",
      "636 번째 loss, accuracy:  0.714258028814366 0.6916666666666667\n",
      "637 번째 loss, accuracy:  0.7138067704351928 0.6916666666666667\n",
      "638 번째 loss, accuracy:  0.713356437792586 0.6916666666666667\n",
      "639 번째 loss, accuracy:  0.7129070286306456 0.6916666666666667\n",
      "640 번째 loss, accuracy:  0.7124585407024286 0.6916666666666667\n",
      "641 번째 loss, accuracy:  0.712010971769819 0.6916666666666667\n",
      "642 번째 loss, accuracy:  0.7115643196033962 0.6916666666666667\n",
      "643 번째 loss, accuracy:  0.7111185819823385 0.6916666666666667\n",
      "644 번째 loss, accuracy:  0.7106737566942848 0.6916666666666667\n",
      "645 번째 loss, accuracy:  0.7102298415352242 0.6916666666666667\n",
      "646 번째 loss, accuracy:  0.709786834309382 0.6916666666666667\n",
      "647 번째 loss, accuracy:  0.7093447328291058 0.6916666666666667\n",
      "648 번째 loss, accuracy:  0.7089035349147588 0.6916666666666667\n",
      "649 번째 loss, accuracy:  0.7084632383946077 0.6916666666666667\n",
      "650 번째 loss, accuracy:  0.7080238411047177 0.6916666666666667\n",
      "651 번째 loss, accuracy:  0.7075853408888383 0.6916666666666667\n",
      "652 번째 loss, accuracy:  0.7071477355983159 0.6916666666666667\n",
      "653 번째 loss, accuracy:  0.7067110230919739 0.6916666666666667\n",
      "654 번째 loss, accuracy:  0.7062752012360295 0.6916666666666667\n",
      "655 번째 loss, accuracy:  0.7058402679039782 0.6916666666666667\n",
      "656 번째 loss, accuracy:  0.705406220976511 0.6916666666666667\n",
      "657 번째 loss, accuracy:  0.7049730583414113 0.6916666666666667\n",
      "658 번째 loss, accuracy:  0.7045407778934601 0.6916666666666667\n",
      "659 번째 loss, accuracy:  0.7041093775343501 0.6916666666666667\n",
      "660 번째 loss, accuracy:  0.7036788551725895 0.6916666666666667\n",
      "661 번째 loss, accuracy:  0.703249208723417 0.6916666666666667\n",
      "662 번째 loss, accuracy:  0.702820436108716 0.6916666666666667\n",
      "663 번째 loss, accuracy:  0.7023925352569121 0.6916666666666667\n",
      "664 번째 loss, accuracy:  0.7019655041029129 0.6916666666666667\n",
      "665 번째 loss, accuracy:  0.7015393405880087 0.6916666666666667\n",
      "666 번째 loss, accuracy:  0.7011140426597964 0.6916666666666667\n",
      "667 번째 loss, accuracy:  0.7006896082720989 0.6916666666666667\n",
      "668 번째 loss, accuracy:  0.7002660353848819 0.6916666666666667\n",
      "669 번째 loss, accuracy:  0.6998433219641856 0.6916666666666667\n",
      "670 번째 loss, accuracy:  0.6994214659820398 0.6916666666666667\n",
      "671 번째 loss, accuracy:  0.6990004654163899 0.6916666666666667\n",
      "672 번째 loss, accuracy:  0.6985803182510312 0.6916666666666667\n",
      "673 번째 loss, accuracy:  0.6981610224755272 0.6916666666666667\n",
      "674 번째 loss, accuracy:  0.6977425760851439 0.6916666666666667\n",
      "675 번째 loss, accuracy:  0.6973249770807847 0.6916666666666667\n",
      "676 번째 loss, accuracy:  0.6969082234689107 0.6916666666666667\n",
      "677 번째 loss, accuracy:  0.6964923132614856 0.6916666666666667\n",
      "678 번째 loss, accuracy:  0.6960772444758995 0.6916666666666667\n",
      "679 번째 loss, accuracy:  0.6956630151349124 0.6916666666666667\n",
      "680 번째 loss, accuracy:  0.6952496232665889 0.6916666666666667\n",
      "681 번째 loss, accuracy:  0.6948370669042278 0.6916666666666667\n",
      "682 번째 loss, accuracy:  0.6944253440863136 0.6916666666666667\n",
      "683 번째 loss, accuracy:  0.6940144528564532 0.6916666666666667\n",
      "684 번째 loss, accuracy:  0.6936043912633092 0.6916666666666667\n",
      "685 번째 loss, accuracy:  0.693195157360549 0.6916666666666667\n",
      "686 번째 loss, accuracy:  0.6927867492067894 0.6916666666666667\n",
      "687 번째 loss, accuracy:  0.6923791648655391 0.6916666666666667\n",
      "688 번째 loss, accuracy:  0.6919724024051426 0.6916666666666667\n",
      "689 번째 loss, accuracy:  0.6915664598987258 0.6916666666666667\n",
      "690 번째 loss, accuracy:  0.6911613354241501 0.6916666666666667\n",
      "691 번째 loss, accuracy:  0.6907570270639525 0.6916666666666667\n",
      "692 번째 loss, accuracy:  0.6903535329053069 0.6916666666666667\n",
      "693 번째 loss, accuracy:  0.6899508510399556 0.6916666666666667\n",
      "694 번째 loss, accuracy:  0.6895489795641859 0.6916666666666667\n",
      "695 번째 loss, accuracy:  0.6891479165787583 0.6916666666666667\n",
      "696 번째 loss, accuracy:  0.6887476601888768 0.6916666666666667\n",
      "697 번째 loss, accuracy:  0.6883482085041354 0.6916666666666667\n",
      "698 번째 loss, accuracy:  0.687949559638479 0.6916666666666667\n",
      "699 번째 loss, accuracy:  0.6875517117101547 0.6916666666666667\n",
      "700 번째 loss, accuracy:  0.6871546628416653 0.6916666666666667\n",
      "701 번째 loss, accuracy:  0.6867584111597392 0.6916666666666667\n",
      "702 번째 loss, accuracy:  0.686362954795277 0.6916666666666667\n",
      "703 번째 loss, accuracy:  0.6859682918833173 0.6916666666666667\n",
      "704 번째 loss, accuracy:  0.6855744205630006 0.6916666666666667\n",
      "705 번째 loss, accuracy:  0.6851813389775242 0.6916666666666667\n",
      "706 번째 loss, accuracy:  0.6847890452741043 0.6916666666666667\n",
      "707 번째 loss, accuracy:  0.6843975376039407 0.6916666666666667\n",
      "708 번째 loss, accuracy:  0.6840068141221849 0.6916666666666667\n",
      "709 번째 loss, accuracy:  0.6836168729878944 0.6916666666666667\n",
      "710 번째 loss, accuracy:  0.6832277123640116 0.6916666666666667\n",
      "711 번째 loss, accuracy:  0.6828393304173163 0.6916666666666667\n",
      "712 번째 loss, accuracy:  0.6824517253183965 0.6916666666666667\n",
      "713 번째 loss, accuracy:  0.6820648952416223 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714 번째 loss, accuracy:  0.6816788383651066 0.6916666666666667\n",
      "715 번째 loss, accuracy:  0.6812935528706769 0.6916666666666667\n",
      "716 번째 loss, accuracy:  0.6809090369438404 0.6916666666666667\n",
      "717 번째 loss, accuracy:  0.680525288773764 0.6916666666666667\n",
      "718 번째 loss, accuracy:  0.6801423065532314 0.6916666666666667\n",
      "719 번째 loss, accuracy:  0.679760088478626 0.6916666666666667\n",
      "720 번째 loss, accuracy:  0.6793786327498995 0.6916666666666667\n",
      "721 번째 loss, accuracy:  0.6789979375705443 0.6916666666666667\n",
      "722 번째 loss, accuracy:  0.6786180011475629 0.6916666666666667\n",
      "723 번째 loss, accuracy:  0.678238821691448 0.6916666666666667\n",
      "724 번째 loss, accuracy:  0.6778603974161516 0.6916666666666667\n",
      "725 번째 loss, accuracy:  0.6774827265390665 0.6916666666666667\n",
      "726 번째 loss, accuracy:  0.6771058072809935 0.6916666666666667\n",
      "727 번째 loss, accuracy:  0.6767296378661276 0.6916666666666667\n",
      "728 번째 loss, accuracy:  0.6763542165220208 0.6916666666666667\n",
      "729 번째 loss, accuracy:  0.6759795414795817 0.6916666666666667\n",
      "730 번째 loss, accuracy:  0.6756056109730229 0.6916666666666667\n",
      "731 번째 loss, accuracy:  0.6752324232398661 0.7\n",
      "732 번째 loss, accuracy:  0.6748599765209101 0.7\n",
      "733 번째 loss, accuracy:  0.6744882690602056 0.7\n",
      "734 번째 loss, accuracy:  0.6741172991050469 0.7\n",
      "735 번째 loss, accuracy:  0.6737470649059433 0.7\n",
      "736 번째 loss, accuracy:  0.6733775647166013 0.7\n",
      "737 번째 loss, accuracy:  0.6730087967939082 0.7\n",
      "738 번째 loss, accuracy:  0.6726407593979128 0.7\n",
      "739 번째 loss, accuracy:  0.6722734507918011 0.7\n",
      "740 번째 loss, accuracy:  0.6719068692418936 0.7\n",
      "741 번째 loss, accuracy:  0.6715410130176166 0.7\n",
      "742 번째 loss, accuracy:  0.6711758803914877 0.7\n",
      "743 번째 loss, accuracy:  0.6708114696390998 0.7\n",
      "744 번째 loss, accuracy:  0.6704477790391096 0.7\n",
      "745 번째 loss, accuracy:  0.6700848068732216 0.7\n",
      "746 번째 loss, accuracy:  0.6697225514261628 0.7\n",
      "747 번째 loss, accuracy:  0.669361010985688 0.7\n",
      "748 번째 loss, accuracy:  0.6690001838425421 0.7\n",
      "749 번째 loss, accuracy:  0.6686400682904661 0.7\n",
      "750 번째 loss, accuracy:  0.6682806626261749 0.7\n",
      "751 번째 loss, accuracy:  0.667921965149344 0.7\n",
      "752 번째 loss, accuracy:  0.6675639741625957 0.7\n",
      "753 번째 loss, accuracy:  0.6672066879714953 0.7\n",
      "754 번째 loss, accuracy:  0.6668501048845308 0.7\n",
      "755 번째 loss, accuracy:  0.6664942232130999 0.7\n",
      "756 번째 loss, accuracy:  0.6661390412715067 0.7\n",
      "757 번째 loss, accuracy:  0.6657845573769395 0.7083333333333334\n",
      "758 번째 loss, accuracy:  0.6654307698494786 0.7083333333333334\n",
      "759 번째 loss, accuracy:  0.6650776770120627 0.7083333333333334\n",
      "760 번째 loss, accuracy:  0.6647252771904983 0.7083333333333334\n",
      "761 번째 loss, accuracy:  0.6643735687134399 0.7083333333333334\n",
      "762 번째 loss, accuracy:  0.6640225499123787 0.7083333333333334\n",
      "763 번째 loss, accuracy:  0.6636722191216398 0.7083333333333334\n",
      "764 번째 loss, accuracy:  0.6633225746783716 0.7083333333333334\n",
      "765 번째 loss, accuracy:  0.6629736149225344 0.7083333333333334\n",
      "766 번째 loss, accuracy:  0.6626253381968947 0.7083333333333334\n",
      "767 번째 loss, accuracy:  0.6622777428470121 0.7083333333333334\n",
      "768 번째 loss, accuracy:  0.6619308272212393 0.7083333333333334\n",
      "769 번째 loss, accuracy:  0.6615845896707117 0.7083333333333334\n",
      "770 번째 loss, accuracy:  0.6612390285493324 0.7083333333333334\n",
      "771 번째 loss, accuracy:  0.6608941422137758 0.7083333333333334\n",
      "772 번째 loss, accuracy:  0.660549929023476 0.7083333333333334\n",
      "773 번째 loss, accuracy:  0.6602063873406155 0.7083333333333334\n",
      "774 번째 loss, accuracy:  0.6598635155301267 0.7083333333333334\n",
      "775 번째 loss, accuracy:  0.6595213119596822 0.7083333333333334\n",
      "776 번째 loss, accuracy:  0.6591797749996845 0.7083333333333334\n",
      "777 번째 loss, accuracy:  0.6588389030232685 0.7083333333333334\n",
      "778 번째 loss, accuracy:  0.6584986944062842 0.7083333333333334\n",
      "779 번째 loss, accuracy:  0.6581591475273046 0.7083333333333334\n",
      "780 번째 loss, accuracy:  0.6578202607676105 0.7083333333333334\n",
      "781 번째 loss, accuracy:  0.6574820325111947 0.7083333333333334\n",
      "782 번째 loss, accuracy:  0.6571444611447409 0.7083333333333334\n",
      "783 번째 loss, accuracy:  0.6568075450576352 0.7083333333333334\n",
      "784 번째 loss, accuracy:  0.6564712826419569 0.7083333333333334\n",
      "785 번째 loss, accuracy:  0.6561356722924647 0.7083333333333334\n",
      "786 번째 loss, accuracy:  0.6558007124066092 0.7083333333333334\n",
      "787 번째 loss, accuracy:  0.6554664013845121 0.7083333333333334\n",
      "788 번째 loss, accuracy:  0.6551327376289715 0.7083333333333334\n",
      "789 번째 loss, accuracy:  0.6547997195454603 0.7083333333333334\n",
      "790 번째 loss, accuracy:  0.6544673455421128 0.7083333333333334\n",
      "791 번째 loss, accuracy:  0.6541356140297285 0.7083333333333334\n",
      "792 번째 loss, accuracy:  0.6538045234217705 0.7083333333333334\n",
      "793 번째 loss, accuracy:  0.6534740721343529 0.7083333333333334\n",
      "794 번째 loss, accuracy:  0.6531442585862465 0.7083333333333334\n",
      "795 번째 loss, accuracy:  0.6528150811988712 0.7083333333333334\n",
      "796 번째 loss, accuracy:  0.6524865383962966 0.7083333333333334\n",
      "797 번째 loss, accuracy:  0.6521586286052335 0.7083333333333334\n",
      "798 번째 loss, accuracy:  0.6518313502550418 0.7083333333333334\n",
      "799 번째 loss, accuracy:  0.6515047017777129 0.7083333333333334\n",
      "800 번째 loss, accuracy:  0.6511786816078778 0.7083333333333334\n",
      "801 번째 loss, accuracy:  0.6508532881828063 0.7083333333333334\n",
      "802 번째 loss, accuracy:  0.6505285199423945 0.7083333333333334\n",
      "803 번째 loss, accuracy:  0.650204375329174 0.7083333333333334\n",
      "804 번째 loss, accuracy:  0.6498808527883034 0.7083333333333334\n",
      "805 번째 loss, accuracy:  0.6495579507675688 0.7083333333333334\n",
      "806 번째 loss, accuracy:  0.6492356677173772 0.7083333333333334\n",
      "807 번째 loss, accuracy:  0.6489140020907627 0.7083333333333334\n",
      "808 번째 loss, accuracy:  0.6485929523433781 0.7083333333333334\n",
      "809 번째 loss, accuracy:  0.6482725169335015 0.7083333333333334\n",
      "810 번째 loss, accuracy:  0.647952694322021 0.7083333333333334\n",
      "811 번째 loss, accuracy:  0.6476334829724467 0.7166666666666667\n",
      "812 번째 loss, accuracy:  0.6473148813509043 0.7166666666666667\n",
      "813 번째 loss, accuracy:  0.6469968879261304 0.7166666666666667\n",
      "814 번째 loss, accuracy:  0.6466795011694783 0.7166666666666667\n",
      "815 번째 loss, accuracy:  0.6463627195549094 0.7166666666666667\n",
      "816 번째 loss, accuracy:  0.6460465415590018 0.7166666666666667\n",
      "817 번째 loss, accuracy:  0.6457309656609385 0.7166666666666667\n",
      "818 번째 loss, accuracy:  0.6454159903425111 0.7166666666666667\n",
      "819 번째 loss, accuracy:  0.6451016140881245 0.7166666666666667\n",
      "820 번째 loss, accuracy:  0.6447878353847882 0.7166666666666667\n",
      "821 번째 loss, accuracy:  0.6444746527221165 0.7166666666666667\n",
      "822 번째 loss, accuracy:  0.6441620645923292 0.7166666666666667\n",
      "823 번째 loss, accuracy:  0.6438500694902562 0.7166666666666667\n",
      "824 번째 loss, accuracy:  0.6435386659133276 0.7166666666666667\n",
      "825 번째 loss, accuracy:  0.6432278523615782 0.7166666666666667\n",
      "826 번째 loss, accuracy:  0.6429176273376462 0.7166666666666667\n",
      "827 번째 loss, accuracy:  0.6426079893467755 0.7166666666666667\n",
      "828 번째 loss, accuracy:  0.642298936896809 0.7166666666666667\n",
      "829 번째 loss, accuracy:  0.641990468498197 0.7166666666666667\n",
      "830 번째 loss, accuracy:  0.6416825826639865 0.7166666666666667\n",
      "831 번째 loss, accuracy:  0.6413752779098303 0.7166666666666667\n",
      "832 번째 loss, accuracy:  0.6410685527539767 0.725\n",
      "833 번째 loss, accuracy:  0.6407624057172829 0.725\n",
      "834 번째 loss, accuracy:  0.6404568353232011 0.725\n",
      "835 번째 loss, accuracy:  0.6401518400977847 0.725\n",
      "836 번째 loss, accuracy:  0.6398474185696912 0.725\n",
      "837 번째 loss, accuracy:  0.6395435692701734 0.725\n",
      "838 번째 loss, accuracy:  0.6392402907330897 0.725\n",
      "839 번째 loss, accuracy:  0.6389375814948935 0.725\n",
      "840 번째 loss, accuracy:  0.6386354400946466 0.725\n",
      "841 번째 loss, accuracy:  0.6383338650740004 0.725\n",
      "842 번째 loss, accuracy:  0.6380328549772134 0.725\n",
      "843 번째 loss, accuracy:  0.6377324083511432 0.725\n",
      "844 번째 loss, accuracy:  0.6374325237452432 0.725\n",
      "845 번째 loss, accuracy:  0.6371331997115725 0.725\n",
      "846 번째 loss, accuracy:  0.6368344348047853 0.725\n",
      "847 번째 loss, accuracy:  0.6365362275821395 0.725\n",
      "848 번째 loss, accuracy:  0.636238576603496 0.725\n",
      "849 번째 loss, accuracy:  0.6359414804313066 0.725\n",
      "850 번째 loss, accuracy:  0.6356449376306298 0.725\n",
      "851 번째 loss, accuracy:  0.6353489467691247 0.725\n",
      "852 번째 loss, accuracy:  0.6350535064170497 0.725\n",
      "853 번째 loss, accuracy:  0.6347586151472607 0.725\n",
      "854 번째 loss, accuracy:  0.6344642715352243 0.725\n",
      "855 번째 loss, accuracy:  0.6341704741589953 0.725\n",
      "856 번째 loss, accuracy:  0.6338772215992371 0.725\n",
      "857 번째 loss, accuracy:  0.6335845124392087 0.725\n",
      "858 번째 loss, accuracy:  0.6332923452647766 0.725\n",
      "859 번째 loss, accuracy:  0.6330007186644017 0.725\n",
      "860 번째 loss, accuracy:  0.6327096312291516 0.725\n",
      "861 번째 loss, accuracy:  0.6324190815526904 0.725\n",
      "862 번째 loss, accuracy:  0.6321290682312913 0.725\n",
      "863 번째 loss, accuracy:  0.6318395898638182 0.725\n",
      "864 번째 loss, accuracy:  0.6315506450517436 0.725\n",
      "865 번째 loss, accuracy:  0.63126223239914 0.725\n",
      "866 번째 loss, accuracy:  0.6309743505126806 0.725\n",
      "867 번째 loss, accuracy:  0.630686998001644 0.725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868 번째 loss, accuracy:  0.6304001734779054 0.725\n",
      "869 번째 loss, accuracy:  0.630113875555945 0.725\n",
      "870 번째 loss, accuracy:  0.6298281028528433 0.725\n",
      "871 번째 loss, accuracy:  0.629542853988284 0.725\n",
      "872 번째 loss, accuracy:  0.6292581275845492 0.725\n",
      "873 번째 loss, accuracy:  0.6289739222665265 0.725\n",
      "874 번째 loss, accuracy:  0.6286902366617032 0.725\n",
      "875 번째 loss, accuracy:  0.6284070694001695 0.725\n",
      "876 번째 loss, accuracy:  0.6281244191146195 0.725\n",
      "877 번째 loss, accuracy:  0.6278422844403436 0.725\n",
      "878 번째 loss, accuracy:  0.627560664015237 0.725\n",
      "879 번째 loss, accuracy:  0.6272795564797937 0.725\n",
      "880 번째 loss, accuracy:  0.6269989604771137 0.725\n",
      "881 번째 loss, accuracy:  0.6267188746528929 0.725\n",
      "882 번째 loss, accuracy:  0.6264392976554312 0.725\n",
      "883 번째 loss, accuracy:  0.6261602281356294 0.725\n",
      "884 번째 loss, accuracy:  0.6258816647469889 0.725\n",
      "885 번째 loss, accuracy:  0.6256036061456107 0.725\n",
      "886 번째 loss, accuracy:  0.6253260509901936 0.725\n",
      "887 번째 loss, accuracy:  0.6250489979420427 0.725\n",
      "888 번째 loss, accuracy:  0.6247724456650581 0.7333333333333333\n",
      "889 번째 loss, accuracy:  0.6244963928257399 0.7333333333333333\n",
      "890 번째 loss, accuracy:  0.6242208380931887 0.7333333333333333\n",
      "891 번째 loss, accuracy:  0.6239457801391033 0.7333333333333333\n",
      "892 번째 loss, accuracy:  0.6236712176377797 0.7333333333333333\n",
      "893 번째 loss, accuracy:  0.6233971492661138 0.7333333333333333\n",
      "894 번째 loss, accuracy:  0.6231235737036018 0.7333333333333333\n",
      "895 번째 loss, accuracy:  0.622850489632329 0.7333333333333333\n",
      "896 번째 loss, accuracy:  0.6225778957369862 0.7333333333333333\n",
      "897 번째 loss, accuracy:  0.622305790704857 0.7333333333333333\n",
      "898 번째 loss, accuracy:  0.6220341732258187 0.7333333333333333\n",
      "899 번째 loss, accuracy:  0.6217630419923492 0.7333333333333333\n",
      "900 번째 loss, accuracy:  0.6214923956995174 0.7333333333333333\n",
      "901 번째 loss, accuracy:  0.6212222330449884 0.7333333333333333\n",
      "902 번째 loss, accuracy:  0.6209525527290215 0.7333333333333333\n",
      "903 번째 loss, accuracy:  0.6206833534544677 0.7333333333333333\n",
      "904 번째 loss, accuracy:  0.6204146339267715 0.7333333333333333\n",
      "905 번째 loss, accuracy:  0.6201463928539691 0.7333333333333333\n",
      "906 번째 loss, accuracy:  0.6198786289466872 0.7333333333333333\n",
      "907 번째 loss, accuracy:  0.6196113409181474 0.7333333333333333\n",
      "908 번째 loss, accuracy:  0.6193445274841558 0.7333333333333333\n",
      "909 번째 loss, accuracy:  0.6190781873631103 0.7333333333333333\n",
      "910 번째 loss, accuracy:  0.6188123192759979 0.7333333333333333\n",
      "911 번째 loss, accuracy:  0.6185469219463916 0.7333333333333333\n",
      "912 번째 loss, accuracy:  0.6182819941004519 0.7333333333333333\n",
      "913 번째 loss, accuracy:  0.6180175344669248 0.7333333333333333\n",
      "914 번째 loss, accuracy:  0.6177535417771425 0.7333333333333333\n",
      "915 번째 loss, accuracy:  0.617490014765022 0.7333333333333333\n",
      "916 번째 loss, accuracy:  0.6172269521670606 0.7333333333333333\n",
      "917 번째 loss, accuracy:  0.6169643527223393 0.7333333333333333\n",
      "918 번째 loss, accuracy:  0.6167022151725241 0.7333333333333333\n",
      "919 번째 loss, accuracy:  0.6164405382618562 0.7333333333333333\n",
      "920 번째 loss, accuracy:  0.6161793207371602 0.7333333333333333\n",
      "921 번째 loss, accuracy:  0.6159185613478351 0.7333333333333333\n",
      "922 번째 loss, accuracy:  0.6156582588458613 0.7333333333333333\n",
      "923 번째 loss, accuracy:  0.6153984119857895 0.7333333333333333\n",
      "924 번째 loss, accuracy:  0.6151390195247541 0.7416666666666667\n",
      "925 번째 loss, accuracy:  0.6148800802224527 0.7416666666666667\n",
      "926 번째 loss, accuracy:  0.6146215928411689 0.7416666666666667\n",
      "927 번째 loss, accuracy:  0.6143635561457447 0.7416666666666667\n",
      "928 번째 loss, accuracy:  0.6141059689035996 0.7416666666666667\n",
      "929 번째 loss, accuracy:  0.6138488298847229 0.7416666666666667\n",
      "930 번째 loss, accuracy:  0.6135921378616668 0.7416666666666667\n",
      "931 번째 loss, accuracy:  0.6133358916095537 0.7416666666666667\n",
      "932 번째 loss, accuracy:  0.6130800899060719 0.7416666666666667\n",
      "933 번째 loss, accuracy:  0.6128247315314695 0.7416666666666667\n",
      "934 번째 loss, accuracy:  0.6125698152685589 0.7416666666666667\n",
      "935 번째 loss, accuracy:  0.6123153399027115 0.7416666666666667\n",
      "936 번째 loss, accuracy:  0.6120613042218623 0.7416666666666667\n",
      "937 번째 loss, accuracy:  0.6118077070165017 0.7416666666666667\n",
      "938 번째 loss, accuracy:  0.6115545470796767 0.7416666666666667\n",
      "939 번째 loss, accuracy:  0.6113018232069904 0.7416666666666667\n",
      "940 번째 loss, accuracy:  0.6110495341965971 0.7416666666666667\n",
      "941 번째 loss, accuracy:  0.6107976788492095 0.7416666666666667\n",
      "942 번째 loss, accuracy:  0.6105462559680821 0.7416666666666667\n",
      "943 번째 loss, accuracy:  0.6102952643590208 0.7416666666666667\n",
      "944 번째 loss, accuracy:  0.6100447028303797 0.7416666666666667\n",
      "945 번째 loss, accuracy:  0.6097945701930605 0.7416666666666667\n",
      "946 번째 loss, accuracy:  0.6095448652605064 0.7416666666666667\n",
      "947 번째 loss, accuracy:  0.6092955868486982 0.7416666666666667\n",
      "948 번째 loss, accuracy:  0.6090467337761649 0.7416666666666667\n",
      "949 번째 loss, accuracy:  0.6087983048639684 0.7416666666666667\n",
      "950 번째 loss, accuracy:  0.6085502989357106 0.7416666666666667\n",
      "951 번째 loss, accuracy:  0.6083027148175247 0.7416666666666667\n",
      "952 번째 loss, accuracy:  0.6080555513380805 0.7416666666666667\n",
      "953 번째 loss, accuracy:  0.6078088073285772 0.7416666666666667\n",
      "954 번째 loss, accuracy:  0.6075624816227427 0.7416666666666667\n",
      "955 번째 loss, accuracy:  0.6073165730568323 0.7416666666666667\n",
      "956 번째 loss, accuracy:  0.6070710804696273 0.7416666666666667\n",
      "957 번째 loss, accuracy:  0.6068260027024325 0.7416666666666667\n",
      "958 번째 loss, accuracy:  0.6065813385990734 0.7416666666666667\n",
      "959 번째 loss, accuracy:  0.6063370870058942 0.7416666666666667\n",
      "960 번째 loss, accuracy:  0.6060932467717598 0.7416666666666667\n",
      "961 번째 loss, accuracy:  0.605849816748048 0.7416666666666667\n",
      "962 번째 loss, accuracy:  0.6056067957886486 0.7416666666666667\n",
      "963 번째 loss, accuracy:  0.6053641827499644 0.75\n",
      "964 번째 loss, accuracy:  0.6051219764909058 0.75\n",
      "965 번째 loss, accuracy:  0.6048801758728904 0.75\n",
      "966 번째 loss, accuracy:  0.6046387797598427 0.75\n",
      "967 번째 loss, accuracy:  0.6043977870181844 0.75\n",
      "968 번째 loss, accuracy:  0.6041571965168437 0.75\n",
      "969 번째 loss, accuracy:  0.6039170071272414 0.75\n",
      "970 번째 loss, accuracy:  0.6036772177232979 0.75\n",
      "971 번째 loss, accuracy:  0.6034378271814239 0.75\n",
      "972 번째 loss, accuracy:  0.6031988343805229 0.75\n",
      "973 번째 loss, accuracy:  0.6029602382019872 0.75\n",
      "974 번째 loss, accuracy:  0.6027220375296936 0.75\n",
      "975 번째 loss, accuracy:  0.602484231250005 0.75\n",
      "976 번째 loss, accuracy:  0.6022468182517645 0.75\n",
      "977 번째 loss, accuracy:  0.6020097974262968 0.75\n",
      "978 번째 loss, accuracy:  0.601773167667401 0.75\n",
      "979 번째 loss, accuracy:  0.6015369278713486 0.75\n",
      "980 번째 loss, accuracy:  0.6013010769368876 0.75\n",
      "981 번째 loss, accuracy:  0.6010656137652305 0.75\n",
      "982 번째 loss, accuracy:  0.6008305372600626 0.75\n",
      "983 번째 loss, accuracy:  0.6005958463275279 0.75\n",
      "984 번째 loss, accuracy:  0.6003615398762295 0.75\n",
      "985 번째 loss, accuracy:  0.6001276168172373 0.75\n",
      "986 번째 loss, accuracy:  0.599894076064075 0.75\n",
      "987 번째 loss, accuracy:  0.599660916532716 0.75\n",
      "988 번째 loss, accuracy:  0.5994281371415903 0.75\n",
      "989 번째 loss, accuracy:  0.5991957368115722 0.75\n",
      "990 번째 loss, accuracy:  0.598963714465985 0.75\n",
      "991 번째 loss, accuracy:  0.5987320690305952 0.75\n",
      "992 번째 loss, accuracy:  0.598500799433604 0.75\n",
      "993 번째 loss, accuracy:  0.598269904605658 0.75\n",
      "994 번째 loss, accuracy:  0.5980393834798321 0.75\n",
      "995 번째 loss, accuracy:  0.5978092349916366 0.75\n",
      "996 번째 loss, accuracy:  0.5975794580790094 0.75\n",
      "997 번째 loss, accuracy:  0.5973500516823195 0.75\n",
      "998 번째 loss, accuracy:  0.5971210147443545 0.75\n",
      "999 번째 loss, accuracy:  0.5968923462103226 0.75\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEXCAYAAAC9A7+nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxU1d348c83+76TsIaAbLLvoiguCKK17tW6VHGttlb7tPX5aVtr1fZpa60+1WotdfdRi3WviltdsS7siICArIEACSEbSSDL+f1xTsIlTJJJMmGYme/79ZrXzNxz7txzt+89c+6954oxBqWUUqEvKtgFUEopFRga0JVSKkxoQFdKqTChAV0ppcKEBnSllAoTGtCVUipMhFRAF5GNInJyK2nHicjXbYz7uIj8po10IyKDAlHONqaRLyJVIhLdndPpikOxHMJVOC47EflARK46BNOZLSLzOzlum/t+JAmpgN4WY8zHxpihwS6HiPQVkRdEpEREykXkSxGZDWCM2WyMSTHGNHTj9IeLyEIR2e1e74rI8O6ano/px4vIoyJSISLbReQn7eT/L5ev3I0X74bnisizIrLNpX0iIkcdmrk49ERkpIi85babg24OEZEsEXlJRPaIyCYRuahF+kVu+B4ReVlEsvwdtxvnqc1KVAd/a4SIvO226TIRWSQip8Gh2fdFpJeIvOq2RyMiBS3S7xaRtSJSKSKrReTSFuljXZmr3fvY7ihn2AT0w8hTwBagP5ANXArsOITT3wacB2QBOcCrwD8C8cN+/rP4NTAYO/8nAv8tIrNa+b1TgJuB6UABMBC43SWnAAuACdh5eQJ4XURSOj8Hh7U64DngylbSHwD2AXnAxcBfRWQE2GAH/A34nkuvBh70Z9wQ8i/gHew85AI3ABWHcPqNwJvAua2k7wG+DaQDlwF/FpFjAEQkDngF+D8gE7stv+KGB5YxJmRewEbgZ8ByoByYCyS4tBOAQk/eccBioNLl+wfwG0/6TUARNgBeARhgkEuLB+4GNmOD8UNAonc6wE+Bne43Lvf8bhUwtpXyF7jpxABHu7xNr1pgo8sXhQ103wC7sDt6VieWVwzwQ6C6A+N4l8PjwF+BN7Ab7Ml+jL8VmOn5fifwj1byPgP8j+f7dGB7G79dAUzwcz7SgUfc+tkK/AaIdmmzgU+A+912tBqY7hm3N/ZAWAqsA672pEUDP3frphJYBPTzLLtrgbXAbmwglQ6us0GAaTEsGRuQh3iGPQX83n3+H+AZT9oRLn9qe+P6UZ4PgN8BX7hl9Yp3WwT+CWx3aR8BI9zwa7AHqX1u+/6XG94PeBEodtv2XzzrZD52v9sNbABOdWk5btlmtFLGE3D7PnABB+5Xe4EP2tuvO7hPGaCgnXyvAj91n2e6bVA86ZuBWR3dp9t7hWIN/XxgFjAAGI3dEA7gjnwvYzfcLOxGd64nfRb2wDADW5ts2S7/B2AIMBa7g/UBfuVJ74kNGH2wNaoHRCTTpX3mvn9XRPJbmwljzKfGNr+kYI/anwHPuuQbgLOA47HBpSk4NJV/eXt/m0WkDHuQuB+7w3fWRcBvscFhvvtrv7yVaWa68i7zDF4GtFYbHOEjb56IZPv47bFAHDbA+uMJoB67/sZhdypvW/BRwHpssLgNeNHTTPEs9qDdG/tv539EZLpL+wlwIXAakIatDFR7fvd0YBIwBrutnuLKn++aClrdJtowBGgwxqzxDPMu1wOWozHmG1wQ92Ncf1yKnc/e2GV6nydtHnYfysVWoJ52ZZjjPt/ltvNvu394rwGbsJWbPhz47/Eo4GvsOrkLeEREBBv41wH/JyJniUheawU1xsz17Fe9seu4ab9qc7926+fYDiwXn0QkEbsNfOUGjQCWGxfJneV0bB34J9BHiO58YWvol3i+3wU85OMoPQ1b8/YeEf+Dq6EDj+KpoWBXssGuZMHWRo/wpB8NbPBMpwaI8aTvBKa4z5nA77ErswFYCkxyaQVuOjEt5uuvwOtAlPu+igNrjL2wtZ0Yf5aTZ7xk4AfAtzowTssa+pMdGLefGz/BM2wG7p+Hj/zf4KmlALH4qP1gA+eXwC1+liMPWzNL9Ay7EHjffZ7tY/v4Attk0c+tt1RP2u+Ax93nr4Ez21h2x3q+Pwfc3MF15quGfhwt/rkAV7O/5vlv4NoW6VvdttrmuH6U54MW+8pw7MEi2kfeDLcM0j3bj/df8dHYmvlB27FbJ+s835Pcb/V03/sCf3HbTCP238Bgs3+fLGzxe1HYg8df3fc29+sOrJ92a+jYysSbTdsXcCst/qViD3a/7si0/XnFEHq2ez5XY4/CLfUGthq35JxNLdIXtZLWA7sxLbKVA8BuDN72413GmPoW5UgBMMbsxjaX3CwiOdi/eC+LSF9fMyMi38dukFOMMY1ucH/gJRFp9GRtwAaqrb5+xxdjzB4ReQgoFpEjjTE7/R3XY0sH8la59zTsv4Omz5Vt5E/zfG/63Jzf1Xb+BXxmjPmdn+Xojz04FHnWYRQHzouv7aO3e5UaYypbpE10n/thg0prWm6fgWjzb7mc4MDl2lZ6Yzvj+sO73DZhl22OiJRg/719B7vfNG2vOdgmmJb6AZta7DtezcvOGFPt1l3TflUIXA8gIv2AOcCT2KDsS9O/yhvcd3/26y4TkT8CI4ETPdtXe+svYEKxycUfRUAf8aw5IL9Fer9W0kqwNfARxpgM90o39i9chxhjSrABvTe26ecAInIcto35TGOMdwfYgm0/zPC8EowxfgdzjyjshtynE+OCrY34l9EezIqwzQ1NxrD/r2dLX/nIu8MYswvsFTPYprOtwPc7UOYt2Bp6jmf5pRljvH9xfW0f29wrS0RSW6Q1Lfst2DbqQ2kNECMigz3DvMv1gOUoIgOx7cVr/BjXHy33lTrsfnIRcCa2yTId+w8UbKCEg7edLUC+iHSpImmM2YJtghzpK11Evov9R3aeMabODQ7Yft0aEbkdOBV7Dsl7wvYrYHSL7W00HVsHfgnXgP4ptq3vBhGJEZFzgMme9OeA2WIv8UvCtqEC4GrJfwfuFZFcABHp467IaJeI/MFdghbjgsJ12L+Su1rk64c9WXupObB9E+zJmt+KSH+Xt4eInOnn9GeIyDgRiRaRNOAebBv8Kpc+W0Q2+vNbnfQk8EsRyRSRYdi/94+3kfdKtx4ygV825RWRWOB57E54qeffCy69wNflYwDGmCLgbeBPIpImIlEicoSIHO/JlovdPmJF5DvAkcAbLlj8B/idiCSIyGjseZKn3XgPA3eKyGCxRvtq8+8o91sJ2PMEuGnHu/nZgz2ReIeIJIvIVGwgfcqN/jTwbbHXYycDdwAvGmMq2xu3reXocYlnX7kDeN7YS29TsQfOXdhKQ8tzNTuwVy41+QJ7wP+9K0uCK097yyZTRG4XkUFuXeZg2/Q/85F3HPa80VnGmOKm4V3dr13+BOyBEiDefW9KuwV7gJvRcl/HNls1YLe3eBG53g1/z99p+yssA7oxZh9wDrZdbjf2zPeLnvR5wP9iF+g6Dl6w/88N/0xEKoB3AX+vc00CXgLKsCdk+gNn+Mg3HXty9XmxNxtViUjTEfvP2LPkb4tIJXbDbb4GW0S+EpGLW5l+BvYkUDm2aWAQtp26qQmkH/YKj04RkYs95fTlNjfdTcCHwB+NMW+6cZturMoHcMPvAt53+Tex/+B6DPYE40ygzLOMjvPMxyZab4K6FBscV2K3geex5yKafI49mdfUbHCeZ0e8EFvb3IZdl7cZY95xafdgKwRvY6+6eQRIbGN54GvefeiPPXg1LdsabHt9kx+46ezErt/rjDFfAbj3a7GBfSc20P7An3FpfzmCDf6PY5tEEtjfjPGkZ9yVHBxgHwGGiz3Z+LI7CHwbu01uxp54vqCN6TbZh10f72KX+QrsgWS2j7xnYs9jzfdsM/NcWpv7dYvty5ca9jcrrnbfm/wP9t/LWs90fw7N8egs7DZZhj0YneWGB1RTo72KECLyNnCjMWZVsMvSFSLyS6DYGPO3Tow7G7jKGNPlKxpCXVeWozr8aEBXEUcDugpXYdnkopRSkUhr6EopFSa0hq6UUmFCA7pSSoUJDegeEob9WYcrd519W5cBKhVxNKAfJqQD/Yi7m4MaPNe7VonICZ70Y0TkC7F9My+XFh0OuRuVnnHXB+8WkadbpJ8sIovF9p+9RUTO96RFi8hvxPYLXSkiS0Qkw4/585a1UURqPN9bu6a+VcaYBmM7Ydrc0XF9lE3E9hPus9OxQBORbBF5xS3fjSLS6rXYYvsA9y67fSKyxJNe2GJZzmvttzzjRLezPvy5Nry1314o9k5Nf/NHi0iRiHzR2Wl2hIjkicjrbtmvF5Gz28k/RUQ+dculSESuPhTl7LRAdw4Tyi88HVMFYdq/Az7G3hRxJPYmDp/da+K6Gm0lLQt7s8x3sP1UXIK9sSbTk+dj7A0y6dh+OcZ50oZjb0A5FdsRUTYHdmj0G+yNWP2xt3iPxNMZl5/zupF2uuKlgx2RdXHZT8f2q7HPuyy6cXr/xHYdnIztUbMCGObnuPOBn3u+FwIndLE8Xf4Nz28tBL7bgfzfwt5sUw8ceQiW/b+Ax7A3AJ7slv3AVvL2wXYmdp7bT9KBod1dxi7NX7ALcDi9OLCnwXTsnXDF2Lvhfsn+3hAHYe+CLHfBc64bLsC9LiCWY7vIHOnntDvSj3hbAf104KsWw9YAV7rPM11APai3PJf+DHBnK2mZ2DvljmhtPvyc14MCujtQzMXeyVjp5vFo7N2HZdhbxu8DYl3+A3q9wz484D5sd66V2O4fBvhZniexPeS9Cvxvi7Rs7F2SRdgD4wuetHOwvWlWYO9AnOnHtNKwfaEM9Ax7Fk+vhG2MewT2FvJ+nmHdEtDd8v01tl/yEuzdomkuLQV7t2ypWyafuf3lf135at128gc/pv0c9uEcb9Oij3Zs9wxPYys3pRzY5/sF2B44K9323e4ywHYaVg/08Qx7CfhlK/nvw/XWGCovbXJp3f3YjXQgthZ1KXC5S7sTuwFmYrv1vN8Nn4ntuncI9hb8C7D9XDQ9IixQ/YgDjBP7uLI1InKr7O/wSNjfORKeYU0dGU3B3lL+hIjsEpEFcmAfJ1Ncmb50fzH/T/b3Ez4Ku0Oc55qF1ojID9soY0edjT2gpGODez1wI3ZHnIrtB7+tTrouwnZVmoW9tfzO9iYo9glI52ADx9PAhXJg51HPYLsQGI7t7fLPbrxjsN0w/xS7rk/E9dopIr8QkZdbmeRQoNYYs94zzN/+yS/DdgHcsgfMf4jITrGPsBvlx+/442ZsL6BHs79zrj+59+9jD6a9sb0Y3gDsM8b8GFgCzDa2Oez/tTUB11R3BvuX/SUi4o1Jz2H/NQ3FdpPxkBvvROwTma7HbiszcF0XiMidItLaE7qOBHabAzu5a2vZTwGq3D6yQ0ReFJFereQ9PAT7iHI4vdjfJ3o0tq+I4Z6077O//+knsd139m0x/knY2sIUXG3ez+l2tB/xgdgHfERhg+xKXF/h2BplGbY/klhsEGgE/ubS57hpXenSv+vy57j0fdga9BBsTewF4GmXdpEbt6n/ktHYfzAzOricN+K7hv5eO+P9DPin++yrhv6QJ+8ZwAo/yjIbWwOMdvNUCXzbs17qcf17txjvEWw/NR3dxk7k4L67rwPebWc8wdaWL2kx/Fhs/yrJ2IPZNl/lbee3fdXQtwBHeb4PBird5xuwzW7DffyW300u2P5nNrt5S8X2jTLDM71aINnHeE/Tyr/IdqZ3Kp4+192w/wJeayX/Nuy/7TFu23gYeKuj0z2UL62h+5aDrZV5+0nfxP4uaP8buxF+IbajrCsAjDHvYTvhfwDYISJzxPZ42B5vP+J4PvvsL9kYs94Ys8EY02iM+RLbA955Lm0XtoOin2B7u5uF7YSo0I1egz1QPGKMqTPG/AO78071pD9mjFljjKnCdjp0micN4A5jTI0xZjn2iTNN6V11QM1TRIa5E1jbxXamdAd23bSmM32RX4ZtMmswxtRg/4Jf5tL6ASXmwK6N8aS11S96azrbN/bx2IP1i96Bxpj5xphaY8weY8yd2Pk+phPlaib2yUJ9sJ3DlYl9+tUCINbVqudge6R8yZ00/02LmrW/LgOeNVYltn3bu+y3G9tbZEuHatnXYLeNZW7buBOYLt3xLNAA0YDuWwm2nbO/Z1hzn9jGmO3GmKuNMb2xNfcHxV3uaIy5zxgzAfs3bgj22aVtMh3vR/ygn8DTzGKM+dAYM8kYk4V9Cs9QbNelYNv127o9uK30piaj7rq9uOXv/g3bs94gY0wa9nFhLZuTOk1s98THY7tS3i4i27G94p3umsG2YB/k4Oug3Nl+0b8GEkVkgGeYP+v6Mmy3tdXt5DtgW+gMY3tFLAKmmYP75C9zB5BfGmOGYv9xXIQ9Cd80/XaJyBDsP9nve5b9qcDZYrud3gL0FNtlb0udXfarsH3dex+K09ayb21fCNg2GGga0H1wG/Rz2D7JU92O/xPs33pE5Duy/wlEu7ErvUFEJonIUWL78t6D/cvY4Odk/e5HXEROFfdcRZf3VuzDe5vSx4nt5zsN+4CNQmPMWy75JSBTRC5zl4ydh62NNXWp+xhwuYgMdDvT/8M+ygtjn1X5MfALsZdZHok9T/Cam+4JIhLIYJ+KPbm8x02rIw+58Mel2OaqodjnTI51n3dgmw22YP/dPCAiGW6ZTnPjPgJcJSIniu2ju6+ItNvFsrEPPngF26d6ktjuWr+F27Z8cevhPFpsD2L7Mj/GlStBRG7G1jg/dekni0hrTwdqz0PAH0Skj/utPBE53X2eISJHulp5BbZZqmk7b9kHemsuwz41bBgHLvsKbFfGa7EnW+8X26d9nOzv2vZh4DoROVasfDnwAR4+GfvAmTexyz5RRE7CXunyTCujPAZ8V0RGuFr5z4F3jDF7/Zi/4Ah2m8/h9OLAq1wysTtZMbZG8Cv2X+VyF7a2XoX963eNGz4de1SvwtbynwZSXNrFtLj6pMW047En2SqwO8VPPGn57jfz3fe7XZ492D7X78Bd/eHSm/pDL8eeXMxtMa3jsFcIVGHbPI9rkX67m+9i7NUN3kse+2B3iio37e970r4H/MeP5bwR323oj7cYdiK2RluFfYbkb9h/HsNXG/qvPeOeTCvnIDx51mL7Bm85/OfYR96BbeJ5CtuWWoprw3dp57H/Sou1TfOEPcD+q43p5mCvqNmDbcq7wJN2AlDWIv/33LKWFsNHedbjLuAdYLwn/XLgQz/WR2tXufwce/VOhZu/W13aFW74HmxN/o/s3zdOxO4TZcDvWpleFLbt/HIfaXd41nGu25aL3bJ/ypPvQmzNutJtI8e74b/1riMfv58HvIFtmloPnONJm8XBz1/9iZvHUmxzV8+uxpnufGnnXCpgRORh7M70VruZVbcTkcexQfDfwS6LOjQ0oCulVJjQNnQV1ly7fpWPV1mwyxbuRGRWK8t+e/tjq87QGrpSSoWJmPazdI+cnBxTUFAQrMkrpVRIWrRoUYkxpoevtKAF9IKCAhYuXBisySulVEgSkU2tpWkbulJKhQkN6EopFSY0oCulVJgIWhu6Uqpz6urqKCwspLa2NthFUd0oISGBvn37Ehsb6/c4GtCVCjGFhYWkpqZSUFCAyGHbT5TqAmMMu3btorCwkAEDBrQ/gtNuk4uI9BOR90Vklesq9kYfeU4QkXIRWepev+pg+ZVSfqqtrSU7O1uDeRgTEbKzszv8L8yfGno98FNjzGLXreUiEXnHGLOyRb6PjTGnd2jqSqlO0WAe/jqzjtutoRtjiowxi93nSmyfwn3aHqv7fL29kt/PW01lbV2wiqCUUoelDl3lIiIFwDjgcx/JR4vIMhGZJyI+n9EnIteIyEIRWVhcXNzhwgJsKa3moQ+/Yc2OqvYzK6W6RUqKPw+DUoea3wFd7MN0XwB+bGwn/V6Lgf7GmDHYByb7fECuMWaOMWaiMWZijx4+71xt19CeqQCs2dHeE7uUUiqy+BXQ3RN4mh4W/GLLdGNMhbHPn8QY8wb22YNtPfux0/pkJJIUF83X2zWgKxVsxhhuuukmRo4cyahRo5g7dy4ARUVFTJs2jbFjxzJy5Eg+/vhjGhoamD17dnPee++9N8ilDz/tnhQV2zL/CLDKGHNPK3l6AjuMMUZEJmMPFLsCWlInKkoYnJeqNXSlgNv/9RUrt7X8w9w1w3uncdu3fbaaHuTFF19k6dKlLFu2jJKSEiZNmsS0adN45plnOOWUU/jFL35BQ0MD1dXVLF26lK1bt7JixQoAysq0B+NA8+cql6nYR2B9KSJL3bCfYx+LhjHmIeyjuK5zzy+swT6Psdv65R2al8J7q3d2188rpfw0f/58LrzwQqKjo8nLy+P4449nwYIFTJo0iSuuuIK6ujrOOussxo4dy8CBA1m/fj0/+tGP+Na3vsXMmTODXfyw025AN8bMp52nXBtj/gL8JVCFas+QvFSeW1hISdVeclLiD9VklTrs+FuT7i6t1dumTZvGRx99xOuvv873vvc9brrpJi699FKWLVvGW2+9xQMPPMBzzz3Ho48+eohLHN5Csi8XPTGq1OFh2rRpzJ07l4aGBoqLi/noo4+YPHkymzZtIjc3l6uvvporr7ySxYsXU1JSQmNjI+eeey533nknixcvDnbxw05I3vo/NM8F9O2VHHNEt5x7VUr54eyzz+bTTz9lzJgxiAh33XUXPXv25IknnuCPf/wjsbGxpKSk8OSTT7J161Yuv/xyGhsbAfjd734X5NKHn6A9gm7ixImmsw+4MMYw7s53OHVkL353zqgAl0ypw9uqVas48sgjg10MdQj4WtcissgYM9FX/pBschERhuiVLkopdYCQDOhgm13WbK9s9aSMUkpFmpAN6EN6plK5t56icu0TWimlIIQDetOJ0a+12UUppYAQDuhD8mznQGu0CwCllAJCOKBnJMWRlxbPag3oSikFhHBABxjeKy3g/VgopdpWVlbGgw8+2KlxTzvttHb7cPnVr37Fu+++26nfj3QhHdBH9E5nXXEVtXUNwS6KUhGjrYDe0ND2vvjGG2+QkZHRZp477riDk08+udPlC4b6+vpgFwEI8YA+vHcaDY2GtfqwC6UOmZtvvplvvvmGsWPHctNNN/HBBx9w4oknctFFFzFqlL3R76yzzmLChAmMGDGCOXPmNI9bUFBASUkJGzdu5Mgjj+Tqq69mxIgRzJw5k5qaGgBmz57N888/35z/tttuY/z48YwaNYrVq1cDUFxczIwZMxg/fjzf//736d+/PyUlJQeV9brrrmPixImMGDGC2267rXn4ggULOOaYYxgzZgyTJ0+msrKShoYGfvaznzFq1ChGjx7N/ffff0CZARYuXMgJJ5wAwK9//WuuueYaZs6cyaWXXsrGjRs57rjjGD9+POPHj+c///lP8/TuuusuRo0axZgxY5qX3/jx45vT165dy4QJE7q8bkLy1v8mI3qnAfDVtnJG9U0PcmmUCoJ5N8P2LwP7mz1Hwam/bzX597//PStWrGDpUtv56gcffMAXX3zBihUrmp9Q/+ijj5KVlUVNTQ2TJk3i3HPPJTs7+4DfWbt2Lc8++yx///vfOf/883nhhRe45JJLDppeTk4Oixcv5sEHH+Tuu+/m4Ycf5vbbb+ekk07illtu4c033zzgoOH129/+lqysLBoaGpg+fTrLly9n2LBhXHDBBcydO5dJkyZRUVFBYmIic+bMYcOGDSxZsoSYmBhKS0vbXVSLFi1i/vz5JCYmUl1dzTvvvENCQgJr167lwgsvZOHChcybN4+XX36Zzz//nKSkJEpLS8nKyiI9PZ2lS5cyduxYHnvsMWbPnt3u9NoT0gG9X2YSKfExrCzSdnSlgmny5MnNwRzgvvvu46WXXgJgy5YtrF279qCAPmDAAMaOHQvAhAkT2Lhxo8/fPuecc5rzvPiifb7O/Pnzm39/1qxZZGZm+hz3ueeeY86cOdTX11NUVMTKlSsREXr16sWkSZMASEuzFcN3332Xa6+9lpgYGxazsrLane8zzjiDxMREAOrq6rj++utZunQp0dHRrFmzpvl3L7/8cpKSkg743auuuorHHnuMe+65h7lz5/LFF1+0O732hHRAj4oShvdK4ys9MaoiVRs16UMpOTm5+fMHH3zAu+++y6effkpSUhInnHACtbUH3wAYH7+/6+vo6OjmJpfW8kVHRze3Vftzh/iGDRu4++67WbBgAZmZmcyePZva2lqMMdjn9hyoteExMTHNHYq1nA/vfN97773k5eWxbNkyGhsbSUhIaPN3zz333OZ/GhMmTDjogNcZId2GDrYdfVVRBY2N2gWAUodCamoqlZWtXy5cXl5OZmYmSUlJrF69ms8++yzgZTj22GN57rnnAHj77bfZvXv3QXkqKipITk4mPT2dHTt2MG/ePACGDRvGtm3bWLBgAQCVlZXU19czc+ZMHnrooeaDRlOTS0FBAYsWLQLghRdeaLVM5eXl9OrVi6ioKJ566qnmE8QzZ87k0Ucfpbq6+oDfTUhI4JRTTuG6667j8ssv7/IygXAI6L3SqN7XwKbS6mAXRamIkJ2dzdSpUxk5ciQ33XTTQemzZs2ivr6e0aNHc+uttzJlypSAl+G2227j7bffZvz48cybN49evXqRmpp6QJ4xY8Ywbtw4RowYwRVXXMHUqVMBiIuLY+7cufzoRz9izJgxzJgxg9raWq666iry8/MZPXo0Y8aM4Zlnnmme1o033shxxx1HdHR0q2X6wQ9+wBNPPMGUKVNYs2ZNc+191qxZnHHGGUycOJGxY8dy9913N49z8cUXIyIBe3pTSHaf67Viazmn3z+fv1w0jtNH9w5AyZQ6vGn3ubB3716io6OJiYnh008/5brrrms+SRtK7r77bsrLy7nzzjt9pne0+9yQbkMHGJyXQkyUsHJbhQZ0pSLE5s2bOf/882lsbCQuLo6///3vwS5Sh5199tl88803vPfeewH7zZAP6PEx0QzOS9UrXZSKIIMHD2bJkiXBLkaXNF2lE0gh34YO6JUuKuLocwDCX2fWcVgE9DZbNokAAB4USURBVBG90yiu3MvOCu0bXYW/hIQEdu3apUE9jBlj2LVrV/Olj/4K+SYXgNHuLtFlheXMGN6xBaBUqOnbty+FhYUUFxcHuyiqGyUkJNC3b98OjRMWAX1E73Sio4TlhWXMGJ4X7OIo1a1iY2MPuCtTqSZh0eSSGBfN4NwUlhWWB7soSikVNGER0AHG9M1geWGZtisqpSJW2AT00f3SKauuY0up7/4glFIq3IVNQB/T13aav6yw7aehKKVUuAqbgD60ZypxMVEs14CulIpQYRPQY6OjGN4rjWVb9MSoUioyhU1ABxjTN50V28pp0K50lVIRKKwC+ui+GVTva2DdTn3GqFIq8oRVQB/Tr+mOUW1HV0pFnrAK6ANzUkhNiGHpFg3oSqnIE1YBPSpKGJefyeJNBz+OSimlwl1YBXSA8fkZfL2jkoraumAXRSmlDqmwC+gT+mdiDCzTZhelVIRpN6CLSD8ReV9EVonIVyJyo488IiL3icg6EVkuIuO7p7jtG9svAxFYpM0uSqkI40/3ufXAT40xi0UkFVgkIu8YY1Z68pwKDHavo4C/uvdDLjUhlqF5qRrQlVIRp90aujGmyBiz2H2uBFYBfVpkOxN40lifARki0ivgpfXThP6ZLN1cRqPeYKSUiiAdakMXkQJgHPB5i6Q+wBbP90IODvqIyDUislBEFnbn01bG52dSubeetXqDkVIqgvgd0EUkBXgB+LExpuUTmcXHKAdVj40xc4wxE40xE3v06NGxknbAhP6ZgLajK6Uii18BXURiscH8aWPMiz6yFAL9PN/7Atu6XrzO6Z+dRHZynAZ0pVRE8ecqFwEeAVYZY+5pJdurwKXuapcpQLkxpiiA5ewQEWF8/0yWbNaArpSKHP5c5TIV+B7wpYgsdcN+DuQDGGMeAt4ATgPWAdXA5YEvasdM6J/JOyt3ULpnH1nJccEujlJKdbt2A7oxZj6+28i9eQzww0AVKhDG59t29MWbdnPy8Lwgl0Yppbpf2N0p2mR033Rio4VF2uyilIoQYRvQE2KjGd47XU+MKqUiRtgGdICJ/TNZtqWMuobGYBdFKaW6XVgH9An9M9lb38hX21peNq+UUuEn7AM66A1GSqnIENYBPS8tgb6ZifrAC6VURAjrgA62lr5wUyn2ykqllApfERHQd1TsZWtZTbCLopRS3SrsA3rTDUbajq6UCndhH9CH9UwlOS5aA7pSKuyFfUCPiY5ibH6GBnSlVNgL+4AOMCE/k1VFFezZWx/soiilVLeJjIBekEWjgaVbyoJdFKWU6jYREdDH9stARE+MKqXCW0QE9PTEWIbkpmpAV0qFtYgI6AATCjJZvHk3jY16g5FSKjxFTkDPz6Sytp61O6uCXRSllOoWkRPQtaMupVSYi5iA3j87iZyUOBZuKg12UZRSqltETEAXEcblZ2rPi0qpsBUxAR1gXH4GG3dVs3vPvmAXRSmlAi6iAvrYfhkALC3UG4yUUuEnogL66L4ZRAks3awBXSkVfiIqoKfExzAkL5Ul2gWAUioMRVRAB9vssmxLmd5gpJQKOxEX0MflZ1BeU8eGXXuCXRSllAqoiAvoY/vZG4y0HV0pFW4iLqAPyk0hJT5Gu9JVSoWdiAvo0VHC6L7pLNmiNxgppcJLxAV0sCdGVxdVUrOvIdhFUUqpgInIgD4uP5P6RsOKbeXBLopSSgVMRAb05jtG9cSoUiqMRGRA75EaT9/MRD0xqpQKKxEZ0MHW0pds1hOjSqnwEdEBfVt5LTsqaoNdFKWUCoiIDejj8u0NRku0HV0pFSYiNqCP6J1GfEwUn63fFeyiKKVUQLQb0EXkURHZKSIrWkk/QUTKRWSpe/0q8MUMvITYaI45IpsPvt4Z7KIopVRA+FNDfxyY1U6ej40xY93rjq4X69A4cVguG3dVs6FEO+pSSoW+dgO6MeYjICyfrHzCkFwA3l+ttXSlVOgLVBv60SKyTETmiciIAP1mt8vPTmJQbgpvfrU92EVRSqkuC0RAXwz0N8aMAe4HXm4to4hcIyILRWRhcXFxACbddWeP68MXG0rZvKs62EVRSqku6XJAN8ZUGGOq3Oc3gFgRyWkl7xxjzERjzMQePXp0ddIBcfa4PojAC4sLg10UpZTqki4HdBHpKSLiPk92vxky1wL2zkhk6hE5PL+okLqGxmAXRymlOs2fyxafBT4FhopIoYhcKSLXisi1Lst5wAoRWQbcB3zXGBNSD+y87JgCtpbV8NrybcEuilJKdVpMexmMMRe2k/4X4C8BK1EQTB+Wy5C8FB58/xvOHNOHqCgJdpGUUqrDIvZOUa+oKOGHJw5i7c4qbUtXSoUsDejOt0f3Znx+Bn94czUVtXXBLo5SSnWYBnQnKkq4/YyR7Nqzjz+99XWwi6OUUh2mAd1jVN90Lju6gCc+3cRHaw6P6+SVUspfGtBbuPnUYQzOTeFn/1xGceXeYBdHKaX8pgG9hYTYaP783XFU1NZx3f8tYm99Q7CLpJRSftGA7sPw3mnc/Z0xLNy0m1tfXkGIXVavlIpQ7V6HHqlOH92bNdsrue+9dfTLTOJH0wcHu0hKKdUmDeht+PHJQyjcXcOf3llDWmIslx1TEOwiKaVUqzSgtyEqSrjrvNFU7q3ntle/Ijk+hvMm9A12sZRSyidtQ29HTHQU9184jmMH5XDT88t49ovNwS6SUkr5pAHdDwmx0Tx82USOH9KDW178kkfnbwh2kZRS6iAa0P2UEBvN3743gVkjenLHayu5799r9eoXpdRhRQN6B8THRPOXi8Zxzrg+3PPOGm558UvtQ10pddjQk6IdFBMdxZ/OH0OfzETuf28d28preeCicaQmxAa7aEqpCKc19E4QEX46cyh/OHcUn6wr4TsPfcq2sppgF0spFeE0oHfBBZPyeWz2JAp31/Dt++fz2fqQefKeUioMaUDvomlDevDyD6eSnhTLxQ9/zmOfbNCTpUqpoNCAHgCDclN45YdTOWlYLrf/ayU/eW4ZNfu0Uy+l1KGlAT1AUhNi+dslE/jJjCG8vHQrZz4wn6+3Vwa7WEqpCKIBPYCiooQbpg/micsnU7pnH2f8ZT5Pf75Jm2CUUoeEBvRuMG1ID9648TgmD8jiFy+t4IfPLKa8Rp9TqpTqXhrQu0luagJPXD6Zm08dxttf7eC0P3/MJ+tKgl0spVQY04DejaKihGuPP4J/Xns0cTFRXPzw5/zy5S/Zs7c+2EVTSoUhDeiHwLj8TN644TiuPHYAT3++mVl//kivWVdKBZwG9EMkMS6aW08fztxrjiZKhO/O+YzbXllBZa22rSulAkMD+iE2eUAW8248jtnHFPDkZ5uY/qcPeX15kV4Jo5TqMg3oQZAUF8OvzxjBSz+YSk5KPD98ZjGzH1vA5l3VwS6aUiqEaUAPorH9Mnj1+qncevpwFm4sZca9H/Lnd9fqXaZKqU7RgB5kMdFRXHnsAN796fFMPzKXe99dw/Q/fcArS7dqM4xSqkM0oB8meqUn8uDFE/jHNVPITI7jxn8s5ewH/8OiTbuDXTSlVIjQgH6YmTIwm1evP5a7zhvN1rIazv3rf7j+mcWsL64KdtGUUoc5Cdbf+okTJ5qFCxcGZdqhYs/eeh768Bse/ngD+xoaOW98X244eTB9MhKDXTSlVJCIyCJjzESfaRrQD387K2v56wff8PRnmwG46Kh8fnDiEeSmJgS5ZEqpQ00DepjYVlbD/e+t5bmFhcRGCxdN7s/V0wbQK11r7EpFCg3oYWZjyR7ue28tryzdRpTAOeP6cu0JRzAgJznYRVNKdTMN6GFqS2k1f/94PXMXbKGuoZFTR/XiuuOPYGSf9GAXTSnVTTSgh7niyr089skGnvp0E5V765kyMIvZxwxgxvA8oqMk2MVTSgVQlwK6iDwKnA7sNMaM9JEuwJ+B04BqYLYxZnF7hdKAHngVtXU8+/lmnvx0E1vLauiTkchlx/Tngon5pCfFBrt4SqkA6GpAnwZUAU+2EtBPA36EDehHAX82xhzVXqE0oHef+oZG3l21k8c+2cDnG0pJjI3m7PF9uGhyvjbHKBXi2groMe2NbIz5SEQK2shyJjbYG+AzEckQkV7GmKJOlVZ1WUx0FLNG9mTWyJ6s3FbB4//ZwPOLCnnm882M6pPOBZP6cebY3qQmaK1dqXASiDtF+wBbPN8L3bCDiMg1IrJQRBYWFxcHYNKqPcN7p3HXeWNY8POT+fW3h1PX0MgvX17B5N/+m5/9cxmLNpVqnzFKhYl2a+h+8HXWzWeEMMbMAeaAbXIJwLSVn9KTYpk9dQCXHVPA8sJy/rFgM68u3cbziwoZmJPMWeP6cObY3vTP1ksflQpVgQjohUA/z/e+wLYA/K7qBiLCmH4ZjOmXwS+/NZzXlxfx4pJC7nlnDfe8s4Zx+RmcPa4P3xrVi+yU+GAXVynVAX5dtuja0F9r5aTot4Dr2X9S9D5jzOT2flNPih5etpXV8Oqybby8ZCurt1cSEyUcNziHM8b25qRheaQnanu7UoeDrl7l8ixwApAD7ABuA2IBjDEPucsW/wLMwl62eLkxpt1IrQH98LV6ewUvL9nGq0u3sq28lthoYeqgHE4d2ZMZw3uSlRwX7CIqFbH0xiLVKY2NhmWFZcxbsZ15K4rYUlpDlMBRA7I5dVRPThnRk7w07SBMqUNJA7rqMmMMX22r4E0X3L8p3gPA6L7pnDQsl5OG5TKydzpRemeqUt1KA7oKuLU7Knnrq+28t3onS7aUYQz0SI3nxKE9OGlYLscO7kFKfCDOuSulvDSgq261q2ovH64p5t+rd/LRmmIqa+uJjRaOGpDN8UN6cOzgHIb1TMWeblFKdYUGdHXI1DU0snDjbt7/eifvrd7Jup320Xk5KXFMHZTD1EE5HDc4R/twV6qTNKCroNlWVsMn60qYv66ET9aVUFK1D4AjeiRz7KAcjh3cg8kFWdp5mFJ+0oCuDgvGGFZvr+STdSV8vLaELzaUUlPXgAgM65nGUQOyOGpAFpMGZJGjNzUp5ZMGdHVY2lvfwJLNZXyxoZTPN+xi0abd1NY1ArYGP3lANlMGZjF5QJY20SjlaEBXIWFffSMrtpXbAL9+Fws37qZybz0AfTISGd8/k/H5GYzPz+TIXmnExQSibzmlQosGdBWSGhoNq4oq+HxDKYs37Wbx5t0UldcCEB8Txag+6c1Bflx+pt7kpCKCBnQVNorKa1iyuaw5wK/YWsG+BttM0ycjkbH5GYzpm87IPvaVpn2+qzDTpQdcKHU46ZWeSK9RiZw2qhdg2+FXbqtg8eYylmzezZLNZby+fP+zVQbkJDOqT7p99U1nRO80fbCHClsa0FVIi4+JZlx+JuPyM4EBAJTu2ceXW8tZsbWc5YVlLNxYyqvLbI/OIjbIj+6zvxZ/ZM80vWxShQUN6CrsZCXHcfyQHhw/pEfzsJKqvTbIF5azfGs5n60v5eWl+7vt752ewJG90ppfw3qlUpCdTLT2TaNCiAZ0FRFyUuI5cWguJw7NbR62s7KWldsqWFVUyertFawqquCDNcU0NNrzSomx0QzpmcrwXqk2yPe0gV7b5dXhSk+KKuVRW9fAup1VrCyqYHVRJauKKli1vYKy6rrmPL3SExicl8rg3BSG5KUwKDeVwXkpGujVIaEnRZXyU0JsdHPbehNjDNsram2A317B2h1VrN1ZydOf72q+EQqgZ1oCg/NSGJybypC8FAa7YK9Pe1KHigZ0pdohIvbqmvREThy2v8mmodGwdXcNa3ZUsnZnFWvd+7NfbKamrqE5X15aPINzUxmQk8zAHskMyEnmiB4p9M5I1Db6w9WeXVC20b+8jY3wwhVQUdR+3iZTb4Dpv+pU0dqiAV2pToqOEvKzk8jPTuLk4XnNwxsbDVvL9gf6NTsq+WZnFS8v3UplbX1zvrjoKPpnJ7kgn8LAHskMzElmYI8UMpNitbvhQCtcCFu+aD+faYR3fw2Nde1mPcCAadDHZ0vIwfKP6dhv+0kDulIBFhUl9MtKol9WEtOP3B/ojTHs2rOP9cV72FBSxfriPawv2cO6nVW8t3ondQ37z2elJ8Y21+YLspPpn51EflYS/bOTIzPYf/YQbP5P58c3Bla92rFxJl8Dg072L29sIhQcZ6+LDSIN6EodIiJCTko8OSnxTB6QdUBafUMjhbtrWO8J9BuK9/DJuhJeXLz1gLyp8TH0y0qyQb4p0GfZoN8rPYGY6CD1cVO/Dyq2tp+vNaXfwGv/ZX/HyzTAnmL7ucewzv9+3kjbzNHvqPbzRsVAfErnpxUkGtCVOgzEREdRkJNMQU4yJ7WIWTX7Gtiyu5pNu6rZXFrN5l172FRazdc7Kvn3qp3NXR8AxEQJfTMTmwN+/6xk+mQm0icjkT6ZiWQnx3W+dl+xDTZ81Hr6B7+H3Rs699teo863NV6v6DiYdhOk5vkeRwEa0MPT8n/Cpk+CXQoVIInAEPcCIArIsa9GbMCvrK2jam89VbX1VO2tp7K4nqot9eyrb2Q3sBtYgW33T46PJjk+huS4GPseH938OTEumlbr98vnQl1124UdMA3GXNT5mc0aCPl+1KCVTxrQw8Ur18PWRfaETvFqOyw5t+1xVMiLApLd6wDRdmAj9iRtgzH23b0a9xgaqmy7fpNq94oSITpK3Ls9JxAtQlRsCg2TriNuwiW+m4olCtLzIUq7NQ4WDejhYN7NsOQpSOkJ/SZB7nCYfqut7aiIFuVere3oNfsa2FpWQ+HuaraW1bB1d03ze+HuGnZU1nLAvYfvQfL8NfRMT6BXeqJ7TyAvzb73rKmkV3piZJ64PQxoQA91H94Fn/8VEtLhhsUQd1BdTalWJcZFMyg3hUG5vk8A7qtvZGdlLdvLaykq97xX1FBUXssn60rYUVFLY4sbzuNiomyAd4E+Lz2B3NQEclPj7SvNfk6O1xAUSLo0Q9nOVfD+b+3nq9/XYK4CLi4mir6ZSfTNTGo1T31DIyVV+ygqr2FHRYvAX17Los272VG+94CTt02S4qJdkE+gR1p88+fc1Hh6pMaTm2a/a43fPxrQQ9Xc78Hq1+zZ/5+shuTsYJdIRaiY6Ch6pifQM731J0YZYyirrmNn5V52VtZSXLnXfq6w33dW7mXVtgo+rNxL1d76g8aPjRZ6pMTTIy3BvqfGkZ0cT3ZKHNkp8eQk2/fslDgyk+Ii9g5cDeihZMNHsP1L+3nNm9BvChz1fQ3m6rAnImQmx5GZHMfQnqlt5q3eV+8Cve/gX7i7mmWFZZTu2dfcM+aB04KspDgb7F3Qz0mJJ9sT9HM8aSnxMWFT+9eAHipqK+CZCzyXjQkc91MY7OedbEqFiKS4GApyYijIabsJsbHRUF5Tx649eymp2seuqn2ez3ubv6/cVkFJ1V4qag+u+YNtVspKiiMjKZbMpDiyku1n+x5HVnKsfU+ytf+M5FhSD9ODgAb0w135VnjmfHsHXl01zH4deo6yd7Jpm7mKYFFR+2v9g/y4QndvfQO799RRUrWXXXv2B/2Sqr3srt5H6Z46yqr3NXeXXFa976CTvU1iouTgYO/57D0gZCTFkpEYS3pibLffxasB/XC2fQU8NNV+7jcFhpwC/acGvb8IpUJRfEw0PdOj22zr92psNFTU1rG7uo7SPfsoq97n3usorbbfd++xn9eXVFG6yR4E6ls7CmC7bUhPiuXSo/tzzbQjAjVrzTSgH84+vtu+n3Y3TLpKA7lSh1CUq4VnJMUxoJ3mnybGGCr31lO2p87W+qv3UVFT52r8dZTV7KO8uo6e6Ynt/1gnaEA/XBkD6z+EmAQN5kqFCBEhLSGWtIRY8rNbv9Szu2hAP1yVb4GaUvjWnzSYK6X8op0uHK4WPmbfe40LbjmUUiFDA/rhqLEBlj4NCOSNCHZplFIhQptcDkflW6BqB3z7Poj174y8Ukr5VUMXkVki8rWIrBORm32kzxaRYhFZ6l5XBb6oEWTtO/Zde0tUSnVAuzV0EYkGHgBmAIXAAhF51RizskXWucaY67uhjJFlzy544yb7OffI4JZFKRVS/KmhTwbWGWPWG2P2Af8AzuzeYkWwFS8ABk79IyTnBLs0SqkQ4k9A7wNs8XwvdMNaOldElovI8yLSz9cPicg1IrJQRBYWFxd3orhhbs3bMO8miEu1154rpVQH+BPQfV0E3fLe1n8BBcaY0cC7wBO+fsgYM8cYM9EYM7FHjx4dK2m4K9sCz3zHfj7+v/UxXkqpDvMnahQC3hp3X2CbN4MxZpcxZq/7+ndgQmCKF0F2rrLv5z0GU28IblmUUiHJn4C+ABgsIgNEJA74LvCqN4OI9PJ8PQNYFbgiRoiSr+37wBOCWQqlVAhr9yoXY0y9iFwPvIV9lvijxpivROQOYKEx5lXgBhE5A6gHSoHZ3Vjm8FJXC5/eD6tfh6QcSMoKdomUUiHKrxuLjDFvAG+0GPYrz+dbgFsCW7QI8coPYcXz9vOYi4JbFqVUSNM7RYPtm/cgIR3+3ybthEsp1SV6KUUwLXrc9qh47H9pMFdKdZkG9GDZtgT+dSMg0Ht8sEujlAoD2uQSDFu+gEdm2IdX/HgFpOg1+UqprtMa+qFWtNwGc7APr9BgrpQKEA3oh1LpBvjbcfbzzN/CuEuCWx6lVFgJvSaXde/CW78Idik6p3i1fT/3ERh1XnDLopQKO6EX0OPToMfQYJeic3oMhUEzNJgrpbpF6AX0fpOh35PBLoVSSh12tA1dKaXChAZ0pZQKExrQlVIqTGhAV0qpMKEBXSmlwoQGdKWUChMa0JVSKkxoQFdKqTAhxpjgTFikGNjUydFzgJIAFicU6DxHBp3nyNCVee5vjPHZq1/QAnpXiMhCY8zEYJfjUNJ5jgw6z5Ghu+ZZm1yUUipMaEBXSqkwEaoBfU6wCxAEOs+RQec5MnTLPIdkG7pSSqmDhWoNXSmlVAsa0JVSKkyEXEAXkVki8rWIrBORm4NdnkARkX4i8r6IrBKRr0TkRjc8S0TeEZG17j3TDRcRuc8th+UiMj64c9A5IhItIktE5DX3fYCIfO7md66IxLnh8e77OpdeEMxyd4WIZIjI8yKy2q3vo8N5PYvIf7lteoWIPCsiCeG4nkXkURHZKSIrPMM6vF5F5DKXf62IXNaRMoRUQBeRaOAB4FRgOHChiAwPbqkCph74qTHmSGAK8EM3bzcD/zbGDAb+7b6DXQaD3esa4K+HvsgBcSOwyvP9D8C9bn53A1e64VcCu40xg4B7Xb5Q9WfgTWPMMGAMdv7Dcj2LSB/gBmCiMWYkEA18l/Bcz48Ds1oM69B6FZEs4DbgKGAycFvTQcAvxpiQeQFHA295vt8C3BLscnXTvL4CzAC+Bnq5Yb2Ar93nvwEXevI35wuVF9DXbeQnAa8Bgr17Lqbl+gbeAo52n2NcPgn2PHRintOADS3LHq7rGegDbAGy3Hp7DTglXNczUACs6Ox6BS4E/uYZfkC+9l4hVUNn/8bRpNANCyvub+Y44HMgzxhTBODec122cFgW/wv8N9DovmcDZcaYevfdO0/N8+vSy13+UDMQKAYec01ND4tIMmG6no0xW4G7gc1AEXa9LSL813OTjq7XLq3vUAvo4mNYWF13KSIpwAvAj40xFW1l9TEsZJaFiJwO7DTGLPIO9pHV+JEWSmKA8cBfjTHjgD3s/xvuS0jPt2suOBMYAPQGkrHNDS2F23puT2vz2aX5D7WAXgj083zvC2wLUlkCTkRiscH8aWPMi27wDhHp5dJ7ATvd8FBfFlOBM0RkI/APbLPL/wIZIhLj8njnqXl+XXo6UHooCxwghUChMeZz9/15bIAP1/V8MrDBGFNsjKkDXgSOIfzXc5OOrtcure9QC+gLgMHuDHkc9uTKq0EuU0CIiACPAKuMMfd4kl4Fms50X4ZtW28afqk7Wz4FKG/6axcKjDG3GGP6GmMKsOvxPWPMxcD7wHkuW8v5bVoO57n8IVdzM8ZsB7aIyFA3aDqwkjBdz9imlikikuS28ab5Dev17NHR9foWMFNEMt2/m5lumH+CfRKhEycdTgPWAN8Avwh2eQI4X8di/1otB5a612nY9sN/A2vde5bLL9grfr4BvsReRRD0+ejkvJ8AvOY+DwS+ANYB/wTi3fAE932dSx8Y7HJ3YX7HAgvdun4ZyAzn9QzcDqwGVgBPAfHhuJ6BZ7HnCeqwNe0rO7NegSvc/K8DLu9IGfTWf6WUChOh1uSilFKqFRrQlVIqTGhAV0qpMKEBXSmlwoQGdKWUChMa0JVSKkxoQFdKqTDx/wFRTCHS2+I5AgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 loss, accuracy:  3.242285770230519 0.35833333333333334\n",
      "1 번째 loss, accuracy:  3.18760706689806 0.35833333333333334\n",
      "2 번째 loss, accuracy:  3.13404848470208 0.35833333333333334\n",
      "3 번째 loss, accuracy:  3.081517264698192 0.35833333333333334\n",
      "4 번째 loss, accuracy:  3.029865397567186 0.35833333333333334\n",
      "5 번째 loss, accuracy:  2.979058282800912 0.35833333333333334\n",
      "6 번째 loss, accuracy:  2.929031679214735 0.35833333333333334\n",
      "7 번째 loss, accuracy:  2.8797681593735494 0.35833333333333334\n",
      "8 번째 loss, accuracy:  2.831257284618135 0.35833333333333334\n",
      "9 번째 loss, accuracy:  2.7834695536600313 0.35833333333333334\n",
      "10 번째 loss, accuracy:  2.7364404518120513 0.35833333333333334\n",
      "11 번째 loss, accuracy:  2.690147576150369 0.35833333333333334\n",
      "12 번째 loss, accuracy:  2.644600931881507 0.35833333333333334\n",
      "13 번째 loss, accuracy:  2.599828008778788 0.35833333333333334\n",
      "14 번째 loss, accuracy:  2.5558182088502357 0.35833333333333334\n",
      "15 번째 loss, accuracy:  2.5126074540705927 0.35833333333333334\n",
      "16 번째 loss, accuracy:  2.4701901314847317 0.35833333333333334\n",
      "17 번째 loss, accuracy:  2.4285715882375185 0.35833333333333334\n",
      "18 번째 loss, accuracy:  2.3878090327487227 0.35833333333333334\n",
      "19 번째 loss, accuracy:  2.347894344082189 0.35833333333333334\n",
      "20 번째 loss, accuracy:  2.308804393708066 0.35833333333333334\n",
      "21 번째 loss, accuracy:  2.2705460130330546 0.35833333333333334\n",
      "22 번째 loss, accuracy:  2.23315883118059 0.35833333333333334\n",
      "23 번째 loss, accuracy:  2.1966533671257364 0.35833333333333334\n",
      "24 번째 loss, accuracy:  2.1609978057200823 0.35833333333333334\n",
      "25 번째 loss, accuracy:  2.1261869335073436 0.35833333333333334\n",
      "26 번째 loss, accuracy:  2.092248184331616 0.35833333333333334\n",
      "27 번째 loss, accuracy:  2.059163227510874 0.35833333333333334\n",
      "28 번째 loss, accuracy:  2.0268888311403415 0.35833333333333334\n",
      "29 번째 loss, accuracy:  1.9955102248291547 0.35833333333333334\n",
      "30 번째 loss, accuracy:  1.9649268144489074 0.35833333333333334\n",
      "31 번째 loss, accuracy:  1.9352056804593185 0.35833333333333334\n",
      "32 번째 loss, accuracy:  1.9062664468067456 0.35833333333333334\n",
      "33 번째 loss, accuracy:  1.878095852452438 0.35833333333333334\n",
      "34 번째 loss, accuracy:  1.8507059535217987 0.35833333333333334\n",
      "35 번째 loss, accuracy:  1.8240600103390576 0.35833333333333334\n",
      "36 번째 loss, accuracy:  1.798184870555485 0.35833333333333334\n",
      "37 번째 loss, accuracy:  1.7730396082463973 0.35833333333333334\n",
      "38 번째 loss, accuracy:  1.7486123884911584 0.35833333333333334\n",
      "39 번째 loss, accuracy:  1.724878714368728 0.35833333333333334\n",
      "40 번째 loss, accuracy:  1.7018096398268936 0.35833333333333334\n",
      "41 번째 loss, accuracy:  1.6794091034063208 0.35833333333333334\n",
      "42 번째 loss, accuracy:  1.6576574255279717 0.35833333333333334\n",
      "43 번째 loss, accuracy:  1.6365026755529897 0.35833333333333334\n",
      "44 번째 loss, accuracy:  1.6159905364262392 0.35833333333333334\n",
      "45 번째 loss, accuracy:  1.5960600000186258 0.35833333333333334\n",
      "46 번째 loss, accuracy:  1.5767402005379731 0.35833333333333334\n",
      "47 번째 loss, accuracy:  1.5579888242318751 0.35833333333333334\n",
      "48 번째 loss, accuracy:  1.5398159842088621 0.35833333333333334\n",
      "49 번째 loss, accuracy:  1.522189665687166 0.35833333333333334\n",
      "50 번째 loss, accuracy:  1.5050858381256815 0.35833333333333334\n",
      "51 번째 loss, accuracy:  1.4885238949724835 0.35833333333333334\n",
      "52 번째 loss, accuracy:  1.4724697546211374 0.35833333333333334\n",
      "53 번째 loss, accuracy:  1.4569243071196447 0.35833333333333334\n",
      "54 번째 loss, accuracy:  1.441871229571635 0.35833333333333334\n",
      "55 번째 loss, accuracy:  1.4273038010096513 0.35833333333333334\n",
      "56 번째 loss, accuracy:  1.4131793752150539 0.35833333333333334\n",
      "57 번째 loss, accuracy:  1.3995219811275599 0.35833333333333334\n",
      "58 번째 loss, accuracy:  1.386324353718733 0.35833333333333334\n",
      "59 번째 loss, accuracy:  1.3735665968966002 0.35833333333333334\n",
      "60 번째 loss, accuracy:  1.3612422745106738 0.35833333333333334\n",
      "61 번째 loss, accuracy:  1.3493213182271406 0.35833333333333334\n",
      "62 번째 loss, accuracy:  1.3378129289535416 0.35833333333333334\n",
      "63 번째 loss, accuracy:  1.326699930543547 0.35833333333333334\n",
      "64 번째 loss, accuracy:  1.3159787706981272 0.35833333333333334\n",
      "65 번째 loss, accuracy:  1.3056395608284055 0.35833333333333334\n",
      "66 번째 loss, accuracy:  1.295658964745553 0.35833333333333334\n",
      "67 번째 loss, accuracy:  1.2860428476872372 0.35833333333333334\n",
      "68 번째 loss, accuracy:  1.2767779759373312 0.35833333333333334\n",
      "69 번째 loss, accuracy:  1.2678695189209503 0.35833333333333334\n",
      "70 번째 loss, accuracy:  1.2592819092121572 0.35833333333333334\n",
      "71 번째 loss, accuracy:  1.2510282760385736 0.35833333333333334\n",
      "72 번째 loss, accuracy:  1.2431009626388714 0.35833333333333334\n",
      "73 번째 loss, accuracy:  1.2354555888899625 0.35833333333333334\n",
      "74 번째 loss, accuracy:  1.2281302859480538 0.35833333333333334\n",
      "75 번째 loss, accuracy:  1.2210814212152994 0.35833333333333334\n",
      "76 번째 loss, accuracy:  1.2143137771996027 0.35833333333333334\n",
      "77 번째 loss, accuracy:  1.2078245383867958 0.35833333333333334\n",
      "78 번째 loss, accuracy:  1.20157416707897 0.35833333333333334\n",
      "79 번째 loss, accuracy:  1.1955922778136454 0.35833333333333334\n",
      "80 번째 loss, accuracy:  1.189855183624807 0.35833333333333334\n",
      "81 번째 loss, accuracy:  1.184373090842294 0.35833333333333334\n",
      "82 번째 loss, accuracy:  1.1791126220060155 0.35833333333333334\n",
      "83 번째 loss, accuracy:  1.1740818294119346 0.35833333333333334\n",
      "84 번째 loss, accuracy:  1.1692680113675764 0.35833333333333334\n",
      "85 번째 loss, accuracy:  1.1646568088505034 0.35833333333333334\n",
      "86 번째 loss, accuracy:  1.1602551620246464 0.35833333333333334\n",
      "87 번째 loss, accuracy:  1.1560381632916588 0.35833333333333334\n",
      "88 번째 loss, accuracy:  1.1520130587396975 0.35833333333333334\n",
      "89 번째 loss, accuracy:  1.1481661005228168 0.35833333333333334\n",
      "90 번째 loss, accuracy:  1.144488890322894 0.35833333333333334\n",
      "91 번째 loss, accuracy:  1.1409766468858678 0.35833333333333334\n",
      "92 번째 loss, accuracy:  1.1376213725050983 0.35833333333333334\n",
      "93 번째 loss, accuracy:  1.1344252107330037 0.35833333333333334\n",
      "94 번째 loss, accuracy:  1.1313730951838112 0.35833333333333334\n",
      "95 번째 loss, accuracy:  1.1284670780202586 0.35833333333333334\n",
      "96 번째 loss, accuracy:  1.1256913078069704 0.35833333333333334\n",
      "97 번째 loss, accuracy:  1.123038987142546 0.35833333333333334\n",
      "98 번째 loss, accuracy:  1.1205167657419206 0.35833333333333334\n",
      "99 번째 loss, accuracy:  1.11810867926729 0.35833333333333334\n",
      "100 번째 loss, accuracy:  1.1158030258821865 0.35833333333333334\n",
      "101 번째 loss, accuracy:  1.11361649932238 0.35833333333333334\n",
      "102 번째 loss, accuracy:  1.1115248366927604 0.35833333333333334\n",
      "103 번째 loss, accuracy:  1.1095207927453083 0.35833333333333334\n",
      "104 번째 loss, accuracy:  1.1076247513917403 0.35833333333333334\n",
      "105 번째 loss, accuracy:  1.1058096467256522 0.35833333333333334\n",
      "106 번째 loss, accuracy:  1.104074110006932 0.35\n",
      "107 번째 loss, accuracy:  1.1024199315470364 0.35\n",
      "108 번째 loss, accuracy:  1.1008415201527508 0.35\n",
      "109 번째 loss, accuracy:  1.0993385499043213 0.35\n",
      "110 번째 loss, accuracy:  1.0978952922978904 0.35\n",
      "111 번째 loss, accuracy:  1.0965200595279205 0.35\n",
      "112 번째 loss, accuracy:  1.095204901090486 0.35\n",
      "113 번째 loss, accuracy:  1.0939493996364036 0.35\n",
      "114 번째 loss, accuracy:  1.0927495508270277 0.35\n",
      "115 번째 loss, accuracy:  1.0916010720969553 0.35\n",
      "116 번째 loss, accuracy:  1.0905000752878702 0.35\n",
      "117 번째 loss, accuracy:  1.08944070957749 0.35\n",
      "118 번째 loss, accuracy:  1.0884316047509366 0.35\n",
      "119 번째 loss, accuracy:  1.0874611362205344 0.35\n",
      "120 번째 loss, accuracy:  1.0865271571557154 0.35\n",
      "121 번째 loss, accuracy:  1.0856269222020383 0.35\n",
      "122 번째 loss, accuracy:  1.0847659221011732 0.35\n",
      "123 번째 loss, accuracy:  1.0839297486440629 0.35\n",
      "124 번째 loss, accuracy:  1.0831263446068717 0.35\n",
      "125 번째 loss, accuracy:  1.0823459061567924 0.35\n",
      "126 번째 loss, accuracy:  1.081590907948818 0.35\n",
      "127 번째 loss, accuracy:  1.0808659373654725 0.35\n",
      "128 번째 loss, accuracy:  1.0801596094151873 0.35\n",
      "129 번째 loss, accuracy:  1.079471745898276 0.35\n",
      "130 번째 loss, accuracy:  1.078808700510629 0.35\n",
      "131 번째 loss, accuracy:  1.0781587006572728 0.3416666666666667\n",
      "132 번째 loss, accuracy:  1.0775264237167008 0.3416666666666667\n",
      "133 번째 loss, accuracy:  1.0769098903350984 0.3416666666666667\n",
      "134 번째 loss, accuracy:  1.0763030743399946 0.3416666666666667\n",
      "135 번째 loss, accuracy:  1.075714733581995 0.3416666666666667\n",
      "136 번째 loss, accuracy:  1.0751319396402947 0.3333333333333333\n",
      "137 번째 loss, accuracy:  1.0745626650240727 0.3333333333333333\n",
      "138 번째 loss, accuracy:  1.0740021689039758 0.3333333333333333\n",
      "139 번째 loss, accuracy:  1.0734503483178592 0.325\n",
      "140 번째 loss, accuracy:  1.0729061054386286 0.31666666666666665\n",
      "141 번째 loss, accuracy:  1.0723687654689276 0.31666666666666665\n",
      "142 번째 loss, accuracy:  1.0718364280236383 0.30833333333333335\n",
      "143 번째 loss, accuracy:  1.0713084627809484 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 번째 loss, accuracy:  1.0707878286375392 0.3\n",
      "145 번째 loss, accuracy:  1.070268979655334 0.3\n",
      "146 번째 loss, accuracy:  1.0697540230697233 0.2833333333333333\n",
      "147 번째 loss, accuracy:  1.0692397409799745 0.2833333333333333\n",
      "148 번째 loss, accuracy:  1.0687286296840932 0.2833333333333333\n",
      "149 번째 loss, accuracy:  1.0682185254181655 0.2833333333333333\n",
      "150 번째 loss, accuracy:  1.067708515441869 0.2833333333333333\n",
      "151 번째 loss, accuracy:  1.0672000402907968 0.275\n",
      "152 번째 loss, accuracy:  1.0666881195016964 0.275\n",
      "153 번째 loss, accuracy:  1.0661760481084974 0.26666666666666666\n",
      "154 번째 loss, accuracy:  1.0656626085481709 0.25\n",
      "155 번째 loss, accuracy:  1.0651469559482012 0.23333333333333334\n",
      "156 번째 loss, accuracy:  1.0646294008332338 0.21666666666666667\n",
      "157 번째 loss, accuracy:  1.0641113145452903 0.21666666666666667\n",
      "158 번째 loss, accuracy:  1.063586622211026 0.21666666666666667\n",
      "159 번째 loss, accuracy:  1.0630598106390161 0.21666666666666667\n",
      "160 번째 loss, accuracy:  1.0625299699601825 0.20833333333333334\n",
      "161 번째 loss, accuracy:  1.0619954922834938 0.20833333333333334\n",
      "162 번째 loss, accuracy:  1.0614557976203525 0.20833333333333334\n",
      "163 번째 loss, accuracy:  1.0609121340049141 0.19166666666666668\n",
      "164 번째 loss, accuracy:  1.0603641065802294 0.19166666666666668\n",
      "165 번째 loss, accuracy:  1.059810665819974 0.20833333333333334\n",
      "166 번째 loss, accuracy:  1.059251839593742 0.24166666666666667\n",
      "167 번째 loss, accuracy:  1.0586874072050603 0.275\n",
      "168 번째 loss, accuracy:  1.0581172066675062 0.275\n",
      "169 번째 loss, accuracy:  1.057540925943355 0.31666666666666665\n",
      "170 번째 loss, accuracy:  1.0569582637983348 0.38333333333333336\n",
      "171 번째 loss, accuracy:  1.0563687861883386 0.375\n",
      "172 번째 loss, accuracy:  1.0557722149521587 0.38333333333333336\n",
      "173 번째 loss, accuracy:  1.055169537437635 0.4083333333333333\n",
      "174 번째 loss, accuracy:  1.054559233081776 0.4083333333333333\n",
      "175 번째 loss, accuracy:  1.053941982376469 0.4083333333333333\n",
      "176 번째 loss, accuracy:  1.053317490385269 0.43333333333333335\n",
      "177 번째 loss, accuracy:  1.0526850372167653 0.43333333333333335\n",
      "178 번째 loss, accuracy:  1.052044292772216 0.43333333333333335\n",
      "179 번째 loss, accuracy:  1.0513957494819623 0.43333333333333335\n",
      "180 번째 loss, accuracy:  1.0507395010825764 0.43333333333333335\n",
      "181 번째 loss, accuracy:  1.0500738096805275 0.425\n",
      "182 번째 loss, accuracy:  1.0494006902811879 0.4166666666666667\n",
      "183 번째 loss, accuracy:  1.0487186414927183 0.4166666666666667\n",
      "184 번째 loss, accuracy:  1.0480280483800897 0.4166666666666667\n",
      "185 번째 loss, accuracy:  1.0473290456942972 0.4166666666666667\n",
      "186 번째 loss, accuracy:  1.0466209157916795 0.4166666666666667\n",
      "187 번째 loss, accuracy:  1.0459047914106312 0.425\n",
      "188 번째 loss, accuracy:  1.0451792529315767 0.425\n",
      "189 번째 loss, accuracy:  1.0444438150968023 0.425\n",
      "190 번째 loss, accuracy:  1.0436988581962752 0.4166666666666667\n",
      "191 번째 loss, accuracy:  1.0429451039617406 0.4166666666666667\n",
      "192 번째 loss, accuracy:  1.042182560286435 0.39166666666666666\n",
      "193 번째 loss, accuracy:  1.0414102233903242 0.39166666666666666\n",
      "194 번째 loss, accuracy:  1.0406283367557512 0.39166666666666666\n",
      "195 번째 loss, accuracy:  1.0398370395147287 0.39166666666666666\n",
      "196 번째 loss, accuracy:  1.0390353335778235 0.38333333333333336\n",
      "197 번째 loss, accuracy:  1.0382250101519135 0.38333333333333336\n",
      "198 번째 loss, accuracy:  1.0374050578398477 0.38333333333333336\n",
      "199 번째 loss, accuracy:  1.0365754275182661 0.38333333333333336\n",
      "200 번째 loss, accuracy:  1.035735710371474 0.38333333333333336\n",
      "201 번째 loss, accuracy:  1.0348865413056902 0.38333333333333336\n",
      "202 번째 loss, accuracy:  1.0340288481942668 0.38333333333333336\n",
      "203 번째 loss, accuracy:  1.033161534385734 0.38333333333333336\n",
      "204 번째 loss, accuracy:  1.0322833150235977 0.38333333333333336\n",
      "205 번째 loss, accuracy:  1.0313961962089138 0.38333333333333336\n",
      "206 번째 loss, accuracy:  1.0305005823274511 0.38333333333333336\n",
      "207 번째 loss, accuracy:  1.029593853422102 0.38333333333333336\n",
      "208 번째 loss, accuracy:  1.0286802764592544 0.38333333333333336\n",
      "209 번째 loss, accuracy:  1.0277577927937895 0.38333333333333336\n",
      "210 번째 loss, accuracy:  1.0268269383616506 0.38333333333333336\n",
      "211 번째 loss, accuracy:  1.0258851129753586 0.38333333333333336\n",
      "212 번째 loss, accuracy:  1.0249347623315201 0.38333333333333336\n",
      "213 번째 loss, accuracy:  1.0239768751194398 0.38333333333333336\n",
      "214 번째 loss, accuracy:  1.0230143366004092 0.38333333333333336\n",
      "215 번째 loss, accuracy:  1.0220407933192415 0.38333333333333336\n",
      "216 번째 loss, accuracy:  1.0210593820110108 0.38333333333333336\n",
      "217 번째 loss, accuracy:  1.0200689520964092 0.38333333333333336\n",
      "218 번째 loss, accuracy:  1.0190699731254795 0.39166666666666666\n",
      "219 번째 loss, accuracy:  1.0180658527646265 0.39166666666666666\n",
      "220 번째 loss, accuracy:  1.017054904796754 0.4\n",
      "221 번째 loss, accuracy:  1.0160355075965424 0.4083333333333333\n",
      "222 번째 loss, accuracy:  1.0150125776342784 0.4083333333333333\n",
      "223 번째 loss, accuracy:  1.0139797161011443 0.4083333333333333\n",
      "224 번째 loss, accuracy:  1.0129435426242377 0.4083333333333333\n",
      "225 번째 loss, accuracy:  1.0119019149840376 0.4166666666666667\n",
      "226 번째 loss, accuracy:  1.0108558788599495 0.4166666666666667\n",
      "227 번째 loss, accuracy:  1.0098039272911867 0.4166666666666667\n",
      "228 번째 loss, accuracy:  1.008747991286501 0.425\n",
      "229 번째 loss, accuracy:  1.0076881989008515 0.425\n",
      "230 번째 loss, accuracy:  1.0066250326838981 0.43333333333333335\n",
      "231 번째 loss, accuracy:  1.00555986133182 0.43333333333333335\n",
      "232 번째 loss, accuracy:  1.0044940185186726 0.44166666666666665\n",
      "233 번째 loss, accuracy:  1.0034220266125036 0.4583333333333333\n",
      "234 번째 loss, accuracy:  1.002348057761422 0.4583333333333333\n",
      "235 번째 loss, accuracy:  1.0012750063666662 0.49166666666666664\n",
      "236 번째 loss, accuracy:  1.0001991400835286 0.49166666666666664\n",
      "237 번째 loss, accuracy:  0.9991216026336148 0.49166666666666664\n",
      "238 번째 loss, accuracy:  0.9980438091011462 0.5083333333333333\n",
      "239 번째 loss, accuracy:  0.9969701295560264 0.5416666666666666\n",
      "240 번째 loss, accuracy:  0.9958921057629122 0.55\n",
      "241 번째 loss, accuracy:  0.9948161475438837 0.575\n",
      "242 번째 loss, accuracy:  0.9937376844436876 0.5916666666666667\n",
      "243 번째 loss, accuracy:  0.992661806919763 0.6083333333333333\n",
      "244 번째 loss, accuracy:  0.9915857399230442 0.625\n",
      "245 번째 loss, accuracy:  0.9905099440234998 0.625\n",
      "246 번째 loss, accuracy:  0.989438395141819 0.65\n",
      "247 번째 loss, accuracy:  0.9883695212683747 0.65\n",
      "248 번째 loss, accuracy:  0.9873000633690284 0.65\n",
      "249 번째 loss, accuracy:  0.9862340897693761 0.65\n",
      "250 번째 loss, accuracy:  0.9851702669627507 0.65\n",
      "251 번째 loss, accuracy:  0.9841121895361469 0.6583333333333333\n",
      "252 번째 loss, accuracy:  0.9830525360663 0.6666666666666666\n",
      "253 번째 loss, accuracy:  0.981996479417697 0.6833333333333333\n",
      "254 번째 loss, accuracy:  0.9809415249075963 0.6916666666666667\n",
      "255 번째 loss, accuracy:  0.9798884013634878 0.6916666666666667\n",
      "256 번째 loss, accuracy:  0.9788379812419025 0.6916666666666667\n",
      "257 번째 loss, accuracy:  0.9777931882901565 0.6916666666666667\n",
      "258 번째 loss, accuracy:  0.9767490746121327 0.6916666666666667\n",
      "259 번째 loss, accuracy:  0.9757084705248796 0.6916666666666667\n",
      "260 번째 loss, accuracy:  0.9746723077811578 0.6916666666666667\n",
      "261 번째 loss, accuracy:  0.973638100351656 0.6916666666666667\n",
      "262 번째 loss, accuracy:  0.9726066646555116 0.6916666666666667\n",
      "263 번째 loss, accuracy:  0.9715790388060682 0.6916666666666667\n",
      "264 번째 loss, accuracy:  0.9705549335083165 0.6916666666666667\n",
      "265 번째 loss, accuracy:  0.9695336629833043 0.6916666666666667\n",
      "266 번째 loss, accuracy:  0.9685188228333608 0.6916666666666667\n",
      "267 번째 loss, accuracy:  0.967501970400397 0.6916666666666667\n",
      "268 번째 loss, accuracy:  0.9664891876044539 0.6916666666666667\n",
      "269 번째 loss, accuracy:  0.9654791294832461 0.6916666666666667\n",
      "270 번째 loss, accuracy:  0.9644740279040446 0.6916666666666667\n",
      "271 번째 loss, accuracy:  0.9634703512264273 0.6916666666666667\n",
      "272 번째 loss, accuracy:  0.9624708458946892 0.6916666666666667\n",
      "273 번째 loss, accuracy:  0.9614740951588863 0.6916666666666667\n",
      "274 번째 loss, accuracy:  0.9604784054786767 0.6916666666666667\n",
      "275 번째 loss, accuracy:  0.9594853039348786 0.6916666666666667\n",
      "276 번째 loss, accuracy:  0.9584954079056445 0.6916666666666667\n",
      "277 번째 loss, accuracy:  0.9575086340061364 0.6916666666666667\n",
      "278 번째 loss, accuracy:  0.9565244019372462 0.6916666666666667\n",
      "279 번째 loss, accuracy:  0.9555440875165153 0.6916666666666667\n",
      "280 번째 loss, accuracy:  0.9545650239304401 0.6916666666666667\n",
      "281 번째 loss, accuracy:  0.9535887966233527 0.6916666666666667\n",
      "282 번째 loss, accuracy:  0.9526148930103676 0.6916666666666667\n",
      "283 번째 loss, accuracy:  0.9516440896734314 0.6916666666666667\n",
      "284 번째 loss, accuracy:  0.9506753234949966 0.6916666666666667\n",
      "285 번째 loss, accuracy:  0.9497095204903157 0.6916666666666667\n",
      "286 번째 loss, accuracy:  0.9487455177611414 0.6916666666666667\n",
      "287 번째 loss, accuracy:  0.9477842148549637 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 번째 loss, accuracy:  0.94682522599942 0.6916666666666667\n",
      "289 번째 loss, accuracy:  0.9458686729376299 0.6916666666666667\n",
      "290 번째 loss, accuracy:  0.944914579866181 0.6916666666666667\n",
      "291 번째 loss, accuracy:  0.9439630274570917 0.6916666666666667\n",
      "292 번째 loss, accuracy:  0.9430141562019807 0.6916666666666667\n",
      "293 번째 loss, accuracy:  0.9420681829652245 0.6916666666666667\n",
      "294 번째 loss, accuracy:  0.9411232789718492 0.6916666666666667\n",
      "295 번째 loss, accuracy:  0.9401808406995626 0.6916666666666667\n",
      "296 번째 loss, accuracy:  0.9392409352227435 0.6916666666666667\n",
      "297 번째 loss, accuracy:  0.9383024768481026 0.6916666666666667\n",
      "298 번째 loss, accuracy:  0.9373678144084562 0.6916666666666667\n",
      "299 번째 loss, accuracy:  0.9364336502608774 0.6916666666666667\n",
      "300 번째 loss, accuracy:  0.9355017069892806 0.6916666666666667\n",
      "301 번째 loss, accuracy:  0.9345724871236191 0.6916666666666667\n",
      "302 번째 loss, accuracy:  0.9336451489083508 0.6916666666666667\n",
      "303 번째 loss, accuracy:  0.9327199572632187 0.6916666666666667\n",
      "304 번째 loss, accuracy:  0.9317963017391994 0.6916666666666667\n",
      "305 번째 loss, accuracy:  0.9308752110513108 0.6916666666666667\n",
      "306 번째 loss, accuracy:  0.9299554894748954 0.6916666666666667\n",
      "307 번째 loss, accuracy:  0.9290377636837801 0.6916666666666667\n",
      "308 번째 loss, accuracy:  0.9281216283151466 0.6916666666666667\n",
      "309 번째 loss, accuracy:  0.9272080829176562 0.6916666666666667\n",
      "310 번째 loss, accuracy:  0.9262966243841646 0.6916666666666667\n",
      "311 번째 loss, accuracy:  0.9253871302854619 0.6916666666666667\n",
      "312 번째 loss, accuracy:  0.9244792150232742 0.6916666666666667\n",
      "313 번째 loss, accuracy:  0.9235728252627206 0.6916666666666667\n",
      "314 번째 loss, accuracy:  0.9226679621650923 0.6916666666666667\n",
      "315 번째 loss, accuracy:  0.921765932183596 0.6916666666666667\n",
      "316 번째 loss, accuracy:  0.9208654231071215 0.6916666666666667\n",
      "317 번째 loss, accuracy:  0.919966709136625 0.6916666666666667\n",
      "318 번째 loss, accuracy:  0.9190697440630264 0.6916666666666667\n",
      "319 번째 loss, accuracy:  0.9181745509444788 0.6916666666666667\n",
      "320 번째 loss, accuracy:  0.9172810458868672 0.6916666666666667\n",
      "321 번째 loss, accuracy:  0.9163891688851735 0.6916666666666667\n",
      "322 번째 loss, accuracy:  0.9154989849500768 0.6916666666666667\n",
      "323 번째 loss, accuracy:  0.9146104188532378 0.6916666666666667\n",
      "324 번째 loss, accuracy:  0.9137233128680385 0.6916666666666667\n",
      "325 번째 loss, accuracy:  0.9128382235394695 0.6916666666666667\n",
      "326 번째 loss, accuracy:  0.9119537950708653 0.6916666666666667\n",
      "327 번째 loss, accuracy:  0.9110716291231197 0.6916666666666667\n",
      "328 번째 loss, accuracy:  0.910190921844783 0.6916666666666667\n",
      "329 번째 loss, accuracy:  0.9093120134238654 0.6916666666666667\n",
      "330 번째 loss, accuracy:  0.9084343734491005 0.6916666666666667\n",
      "331 번째 loss, accuracy:  0.9075584192604471 0.6916666666666667\n",
      "332 번째 loss, accuracy:  0.906683920669326 0.6916666666666667\n",
      "333 번째 loss, accuracy:  0.9058110027249129 0.6916666666666667\n",
      "334 번째 loss, accuracy:  0.9049398446751871 0.6916666666666667\n",
      "335 번째 loss, accuracy:  0.9040699980227681 0.6916666666666667\n",
      "336 번째 loss, accuracy:  0.9032015718862189 0.6916666666666667\n",
      "337 번째 loss, accuracy:  0.9023346641365941 0.6916666666666667\n",
      "338 번째 loss, accuracy:  0.9014692048146199 0.6916666666666667\n",
      "339 번째 loss, accuracy:  0.9006046400974707 0.6916666666666667\n",
      "340 번째 loss, accuracy:  0.8997418912222753 0.6916666666666667\n",
      "341 번째 loss, accuracy:  0.8988808362595563 0.6916666666666667\n",
      "342 번째 loss, accuracy:  0.8980211313580442 0.6916666666666667\n",
      "343 번째 loss, accuracy:  0.8971626065539299 0.6916666666666667\n",
      "344 번째 loss, accuracy:  0.8963055079991012 0.6916666666666667\n",
      "345 번째 loss, accuracy:  0.8954502354761243 0.6916666666666667\n",
      "346 번째 loss, accuracy:  0.8945959890557609 0.6916666666666667\n",
      "347 번째 loss, accuracy:  0.8937431229277076 0.6916666666666667\n",
      "348 번째 loss, accuracy:  0.8928916681383179 0.6916666666666667\n",
      "349 번째 loss, accuracy:  0.8920414474859799 0.6916666666666667\n",
      "350 번째 loss, accuracy:  0.8911927035563457 0.6916666666666667\n",
      "351 번째 loss, accuracy:  0.8903456602979475 0.6916666666666667\n",
      "352 번째 loss, accuracy:  0.8894998523614881 0.6916666666666667\n",
      "353 번째 loss, accuracy:  0.8886549449487702 0.6916666666666667\n",
      "354 번째 loss, accuracy:  0.8878113903218758 0.6916666666666667\n",
      "355 번째 loss, accuracy:  0.8869691874928518 0.6916666666666667\n",
      "356 번째 loss, accuracy:  0.8861288520126676 0.6916666666666667\n",
      "357 번째 loss, accuracy:  0.8852896098477817 0.6916666666666667\n",
      "358 번째 loss, accuracy:  0.884451653361248 0.6916666666666667\n",
      "359 번째 loss, accuracy:  0.8836148340082581 0.6916666666666667\n",
      "360 번째 loss, accuracy:  0.8827796525298086 0.6916666666666667\n",
      "361 번째 loss, accuracy:  0.8819455984048491 0.6916666666666667\n",
      "362 번째 loss, accuracy:  0.8811130446569957 0.6916666666666667\n",
      "363 번째 loss, accuracy:  0.880281632601377 0.6916666666666667\n",
      "364 번째 loss, accuracy:  0.8794515829772397 0.6916666666666667\n",
      "365 번째 loss, accuracy:  0.8786230147165867 0.6916666666666667\n",
      "366 번째 loss, accuracy:  0.8777954090199269 0.6916666666666667\n",
      "367 번째 loss, accuracy:  0.8769691873037354 0.6916666666666667\n",
      "368 번째 loss, accuracy:  0.8761440392799602 0.6916666666666667\n",
      "369 번째 loss, accuracy:  0.8753202875159296 0.6916666666666667\n",
      "370 번째 loss, accuracy:  0.8744981321941012 0.6916666666666667\n",
      "371 번째 loss, accuracy:  0.8736769247368984 0.6916666666666667\n",
      "372 번째 loss, accuracy:  0.8728570774354996 0.6916666666666667\n",
      "373 번째 loss, accuracy:  0.8720388261588761 0.6916666666666667\n",
      "374 번째 loss, accuracy:  0.8712216589619161 0.6916666666666667\n",
      "375 번째 loss, accuracy:  0.8704054983979089 0.6916666666666667\n",
      "376 번째 loss, accuracy:  0.8695907094012874 0.6916666666666667\n",
      "377 번째 loss, accuracy:  0.8687772372005306 0.6916666666666667\n",
      "378 번째 loss, accuracy:  0.8679648900521644 0.6916666666666667\n",
      "379 번째 loss, accuracy:  0.8671537814922876 0.6916666666666667\n",
      "380 번째 loss, accuracy:  0.8663433928512032 0.6916666666666667\n",
      "381 번째 loss, accuracy:  0.8655347241842537 0.6916666666666667\n",
      "382 번째 loss, accuracy:  0.8647278244385146 0.6916666666666667\n",
      "383 번째 loss, accuracy:  0.8639216409248706 0.6916666666666667\n",
      "384 번째 loss, accuracy:  0.8631171977542278 0.6916666666666667\n",
      "385 번째 loss, accuracy:  0.8623138378739107 0.6916666666666667\n",
      "386 번째 loss, accuracy:  0.8615119395683782 0.6916666666666667\n",
      "387 번째 loss, accuracy:  0.8607110526331044 0.6916666666666667\n",
      "388 번째 loss, accuracy:  0.8599113193025542 0.6916666666666667\n",
      "389 번째 loss, accuracy:  0.8591127748058969 0.6916666666666667\n",
      "390 번째 loss, accuracy:  0.8583158022864112 0.6916666666666667\n",
      "391 번째 loss, accuracy:  0.857519779704765 0.6916666666666667\n",
      "392 번째 loss, accuracy:  0.8567253249520017 0.6916666666666667\n",
      "393 번째 loss, accuracy:  0.8559320816922982 0.6916666666666667\n",
      "394 번째 loss, accuracy:  0.8551403098056486 0.6916666666666667\n",
      "395 번째 loss, accuracy:  0.8543497094235931 0.6916666666666667\n",
      "396 번째 loss, accuracy:  0.8535603651812677 0.6916666666666667\n",
      "397 번째 loss, accuracy:  0.8527717632142002 0.6916666666666667\n",
      "398 번째 loss, accuracy:  0.8519847060164857 0.6916666666666667\n",
      "399 번째 loss, accuracy:  0.85119896322788 0.6916666666666667\n",
      "400 번째 loss, accuracy:  0.8504141458897233 0.6916666666666667\n",
      "401 번째 loss, accuracy:  0.8496309018772713 0.6916666666666667\n",
      "402 번째 loss, accuracy:  0.848848853240065 0.6916666666666667\n",
      "403 번째 loss, accuracy:  0.8480684558409594 0.6916666666666667\n",
      "404 번째 loss, accuracy:  0.8472887659213223 0.6916666666666667\n",
      "405 번째 loss, accuracy:  0.8465102476911232 0.6916666666666667\n",
      "406 번째 loss, accuracy:  0.845733415498642 0.6916666666666667\n",
      "407 번째 loss, accuracy:  0.8449576132873285 0.6916666666666667\n",
      "408 번째 loss, accuracy:  0.8441831843641048 0.6916666666666667\n",
      "409 번째 loss, accuracy:  0.8434100038711997 0.6916666666666667\n",
      "410 번째 loss, accuracy:  0.8426381650129637 0.6916666666666667\n",
      "411 번째 loss, accuracy:  0.8418678698227405 0.6916666666666667\n",
      "412 번째 loss, accuracy:  0.8410983295924982 0.6916666666666667\n",
      "413 번째 loss, accuracy:  0.8403302872704448 0.6916666666666667\n",
      "414 번째 loss, accuracy:  0.8395636745264745 0.6916666666666667\n",
      "415 번째 loss, accuracy:  0.8387980874021336 0.6916666666666667\n",
      "416 번째 loss, accuracy:  0.8380333800932481 0.6916666666666667\n",
      "417 번째 loss, accuracy:  0.8372705203035342 0.6916666666666667\n",
      "418 번째 loss, accuracy:  0.8365090026768167 0.6916666666666667\n",
      "419 번째 loss, accuracy:  0.8357483859130729 0.6916666666666667\n",
      "420 번째 loss, accuracy:  0.8349892690206223 0.6916666666666667\n",
      "421 번째 loss, accuracy:  0.8342313153067099 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422 번째 loss, accuracy:  0.8334747408643955 0.6916666666666667\n",
      "423 번째 loss, accuracy:  0.8327195757799143 0.6916666666666667\n",
      "424 번째 loss, accuracy:  0.8319653491928466 0.6916666666666667\n",
      "425 번째 loss, accuracy:  0.8312129158706102 0.6916666666666667\n",
      "426 번째 loss, accuracy:  0.8304617300048703 0.6916666666666667\n",
      "427 번째 loss, accuracy:  0.8297116468108494 0.6916666666666667\n",
      "428 번째 loss, accuracy:  0.8289626402743856 0.6916666666666667\n",
      "429 번째 loss, accuracy:  0.8282147647707941 0.6916666666666667\n",
      "430 번째 loss, accuracy:  0.8274683015291536 0.6916666666666667\n",
      "431 번째 loss, accuracy:  0.8267233946703139 0.6916666666666667\n",
      "432 번째 loss, accuracy:  0.8259797170735579 0.6916666666666667\n",
      "433 번째 loss, accuracy:  0.8252371341689858 0.6916666666666667\n",
      "434 번째 loss, accuracy:  0.8244959223453182 0.6916666666666667\n",
      "435 번째 loss, accuracy:  0.8237562069497028 0.6916666666666667\n",
      "436 번째 loss, accuracy:  0.8230174684754051 0.6916666666666667\n",
      "437 번째 loss, accuracy:  0.8222801571948994 0.6916666666666667\n",
      "438 번째 loss, accuracy:  0.8215444563007858 0.6916666666666667\n",
      "439 번째 loss, accuracy:  0.8208101565644138 0.6916666666666667\n",
      "440 번째 loss, accuracy:  0.820076494332955 0.6916666666666667\n",
      "441 번째 loss, accuracy:  0.819344144893978 0.6916666666666667\n",
      "442 번째 loss, accuracy:  0.8186135153055927 0.6916666666666667\n",
      "443 번째 loss, accuracy:  0.8178840284362202 0.6916666666666667\n",
      "444 번째 loss, accuracy:  0.8171560935603054 0.6916666666666667\n",
      "445 번째 loss, accuracy:  0.8164289136239667 0.6916666666666667\n",
      "446 번째 loss, accuracy:  0.8157031953845634 0.6916666666666667\n",
      "447 번째 loss, accuracy:  0.8149787592402816 0.6916666666666667\n",
      "448 번째 loss, accuracy:  0.8142556584919413 0.6916666666666667\n",
      "449 번째 loss, accuracy:  0.8135341894941035 0.6916666666666667\n",
      "450 번째 loss, accuracy:  0.8128131283489568 0.6916666666666667\n",
      "451 번째 loss, accuracy:  0.8120940751965123 0.6916666666666667\n",
      "452 번째 loss, accuracy:  0.8113766855731117 0.6916666666666667\n",
      "453 번째 loss, accuracy:  0.8106604603594273 0.6916666666666667\n",
      "454 번째 loss, accuracy:  0.8099449052680182 0.6916666666666667\n",
      "455 번째 loss, accuracy:  0.809231254163027 0.6916666666666667\n",
      "456 번째 loss, accuracy:  0.8085190371434836 0.6916666666666667\n",
      "457 번째 loss, accuracy:  0.8078081258104313 0.6916666666666667\n",
      "458 번째 loss, accuracy:  0.8070983375246801 0.6916666666666667\n",
      "459 번째 loss, accuracy:  0.80638991268817 0.6916666666666667\n",
      "460 번째 loss, accuracy:  0.8056827879240748 0.6916666666666667\n",
      "461 번째 loss, accuracy:  0.8049770378045539 0.6916666666666667\n",
      "462 번째 loss, accuracy:  0.8042727371831051 0.6916666666666667\n",
      "463 번째 loss, accuracy:  0.8035697401210962 0.6916666666666667\n",
      "464 번째 loss, accuracy:  0.8028680784143124 0.6916666666666667\n",
      "465 번째 loss, accuracy:  0.802167629556669 0.6916666666666667\n",
      "466 번째 loss, accuracy:  0.8014685896705788 0.6916666666666667\n",
      "467 번째 loss, accuracy:  0.8007704424839998 0.6916666666666667\n",
      "468 번째 loss, accuracy:  0.800073938216575 0.6916666666666667\n",
      "469 번째 loss, accuracy:  0.7993787143138182 0.6916666666666667\n",
      "470 번째 loss, accuracy:  0.7986847411179078 0.6916666666666667\n",
      "471 번째 loss, accuracy:  0.7979922729374287 0.6916666666666667\n",
      "472 번째 loss, accuracy:  0.7973011786698861 0.6916666666666667\n",
      "473 번째 loss, accuracy:  0.796611454633907 0.6916666666666667\n",
      "474 번째 loss, accuracy:  0.7959231188529714 0.6916666666666667\n",
      "475 번째 loss, accuracy:  0.7952359339569918 0.6916666666666667\n",
      "476 번째 loss, accuracy:  0.7945500325293727 0.6916666666666667\n",
      "477 번째 loss, accuracy:  0.7938656431924699 0.6916666666666667\n",
      "478 번째 loss, accuracy:  0.7931816710799268 0.6916666666666667\n",
      "479 번째 loss, accuracy:  0.7924997298122711 0.6916666666666667\n",
      "480 번째 loss, accuracy:  0.7918191041390382 0.6916666666666667\n",
      "481 번째 loss, accuracy:  0.7911399592992433 0.6916666666666667\n",
      "482 번째 loss, accuracy:  0.7904620963488451 0.6916666666666667\n",
      "483 번째 loss, accuracy:  0.789785696257839 0.6916666666666667\n",
      "484 번째 loss, accuracy:  0.7891105325375558 0.6916666666666667\n",
      "485 번째 loss, accuracy:  0.7884367963931685 0.6916666666666667\n",
      "486 번째 loss, accuracy:  0.7877644804157612 0.6916666666666667\n",
      "487 번째 loss, accuracy:  0.7870932665020053 0.6916666666666667\n",
      "488 번째 loss, accuracy:  0.7864235450944269 0.6916666666666667\n",
      "489 번째 loss, accuracy:  0.7857552412625225 0.6916666666666667\n",
      "490 번째 loss, accuracy:  0.7850881921495425 0.6916666666666667\n",
      "491 번째 loss, accuracy:  0.7844223074387051 0.6916666666666667\n",
      "492 번째 loss, accuracy:  0.783757982484167 0.6916666666666667\n",
      "493 번째 loss, accuracy:  0.7830951036639587 0.6916666666666667\n",
      "494 번째 loss, accuracy:  0.7824325967547251 0.6916666666666667\n",
      "495 번째 loss, accuracy:  0.7817716065354944 0.6916666666666667\n",
      "496 번째 loss, accuracy:  0.7811122631548033 0.6916666666666667\n",
      "497 번째 loss, accuracy:  0.7804543240696054 0.6916666666666667\n",
      "498 번째 loss, accuracy:  0.7797978310062041 0.6916666666666667\n",
      "499 번째 loss, accuracy:  0.7791427419273039 0.6916666666666667\n",
      "500 번째 loss, accuracy:  0.7784891813655682 0.6916666666666667\n",
      "501 번째 loss, accuracy:  0.777836742511146 0.6916666666666667\n",
      "502 번째 loss, accuracy:  0.7771852671470609 0.6916666666666667\n",
      "503 번째 loss, accuracy:  0.7765353515853164 0.6916666666666667\n",
      "504 번째 loss, accuracy:  0.7758869751118023 0.6916666666666667\n",
      "505 번째 loss, accuracy:  0.7752394702508589 0.6916666666666667\n",
      "506 번째 loss, accuracy:  0.7745933510043348 0.6916666666666667\n",
      "507 번째 loss, accuracy:  0.7739487908857607 0.6916666666666667\n",
      "508 번째 loss, accuracy:  0.7733055300485668 0.6916666666666667\n",
      "509 번째 loss, accuracy:  0.7726632915853463 0.6916666666666667\n",
      "510 번째 loss, accuracy:  0.7720226802914028 0.6916666666666667\n",
      "511 번째 loss, accuracy:  0.7713836161955072 0.6916666666666667\n",
      "512 번째 loss, accuracy:  0.7707457994467629 0.6916666666666667\n",
      "513 번째 loss, accuracy:  0.770109355450641 0.6916666666666667\n",
      "514 번째 loss, accuracy:  0.7694741748489423 0.6916666666666667\n",
      "515 번째 loss, accuracy:  0.7688402180861619 0.6916666666666667\n",
      "516 번째 loss, accuracy:  0.7682075470648388 0.6916666666666667\n",
      "517 번째 loss, accuracy:  0.7675764661327416 0.6916666666666667\n",
      "518 번째 loss, accuracy:  0.7669462212051618 0.6916666666666667\n",
      "519 번째 loss, accuracy:  0.7663174032259285 0.6916666666666667\n",
      "520 번째 loss, accuracy:  0.7656896649492481 0.6916666666666667\n",
      "521 번째 loss, accuracy:  0.7650631664547817 0.6916666666666667\n",
      "522 번째 loss, accuracy:  0.7644386044994127 0.6916666666666667\n",
      "523 번째 loss, accuracy:  0.7638154035419603 0.6916666666666667\n",
      "524 번째 loss, accuracy:  0.7631934622038052 0.6916666666666667\n",
      "525 번째 loss, accuracy:  0.7625728773504598 0.6916666666666667\n",
      "526 번째 loss, accuracy:  0.7619536637332748 0.6916666666666667\n",
      "527 번째 loss, accuracy:  0.7613353561258068 0.6916666666666667\n",
      "528 번째 loss, accuracy:  0.760718414792727 0.6916666666666667\n",
      "529 번째 loss, accuracy:  0.7601028966363672 0.6916666666666667\n",
      "530 번째 loss, accuracy:  0.7594887640003626 0.6916666666666667\n",
      "531 번째 loss, accuracy:  0.7588759833151629 0.6916666666666667\n",
      "532 번째 loss, accuracy:  0.7582645101722524 0.6916666666666667\n",
      "533 번째 loss, accuracy:  0.7576540689298831 0.6916666666666667\n",
      "534 번째 loss, accuracy:  0.757045313055846 0.6916666666666667\n",
      "535 번째 loss, accuracy:  0.7564378345426815 0.6916666666666667\n",
      "536 번째 loss, accuracy:  0.7558315909412574 0.6916666666666667\n",
      "537 번째 loss, accuracy:  0.7552267769162124 0.6916666666666667\n",
      "538 번째 loss, accuracy:  0.7546230102262647 0.6916666666666667\n",
      "539 번째 loss, accuracy:  0.7540206649188439 0.6916666666666667\n",
      "540 번째 loss, accuracy:  0.7534195937726921 0.6916666666666667\n",
      "541 번째 loss, accuracy:  0.7528199331338602 0.6916666666666667\n",
      "542 번째 loss, accuracy:  0.7522216740632656 0.6916666666666667\n",
      "543 번째 loss, accuracy:  0.7516244795441186 0.6916666666666667\n",
      "544 번째 loss, accuracy:  0.7510287548443959 0.6916666666666667\n",
      "545 번째 loss, accuracy:  0.7504343499139893 0.6916666666666667\n",
      "546 번째 loss, accuracy:  0.7498411982328101 0.6916666666666667\n",
      "547 번째 loss, accuracy:  0.749249312749684 0.6916666666666667\n",
      "548 번째 loss, accuracy:  0.7486588071172918 0.6916666666666667\n",
      "549 번째 loss, accuracy:  0.7480692975860405 0.6916666666666667\n",
      "550 번째 loss, accuracy:  0.747481276825863 0.6916666666666667\n",
      "551 번째 loss, accuracy:  0.746894388570093 0.6916666666666667\n",
      "552 번째 loss, accuracy:  0.7463087363671341 0.6916666666666667\n",
      "553 번째 loss, accuracy:  0.7457245214464538 0.6916666666666667\n",
      "554 번째 loss, accuracy:  0.7451417514194066 0.6916666666666667\n",
      "555 번째 loss, accuracy:  0.7445602339077861 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556 번째 loss, accuracy:  0.743980050607457 0.6916666666666667\n",
      "557 번째 loss, accuracy:  0.7434009299674589 0.6916666666666667\n",
      "558 번째 loss, accuracy:  0.7428233254934639 0.6916666666666667\n",
      "559 번째 loss, accuracy:  0.7422469925805488 0.6916666666666667\n",
      "560 번째 loss, accuracy:  0.7416715094120269 0.6916666666666667\n",
      "561 번째 loss, accuracy:  0.7410975491179518 0.6916666666666667\n",
      "562 번째 loss, accuracy:  0.740524982725113 0.6916666666666667\n",
      "563 번째 loss, accuracy:  0.7399536145532031 0.6916666666666667\n",
      "564 번째 loss, accuracy:  0.7393835972975669 0.6916666666666667\n",
      "565 번째 loss, accuracy:  0.7388148267320581 0.6916666666666667\n",
      "566 번째 loss, accuracy:  0.7382470355385252 0.6916666666666667\n",
      "567 번째 loss, accuracy:  0.7376807001796793 0.6916666666666667\n",
      "568 번째 loss, accuracy:  0.7371157867460838 0.6916666666666667\n",
      "569 번째 loss, accuracy:  0.7365520017481051 0.6916666666666667\n",
      "570 번째 loss, accuracy:  0.7359896226376549 0.6916666666666667\n",
      "571 번째 loss, accuracy:  0.7354284708815347 0.6916666666666667\n",
      "572 번째 loss, accuracy:  0.7348685144952458 0.6916666666666667\n",
      "573 번째 loss, accuracy:  0.7343098915396665 0.6916666666666667\n",
      "574 번째 loss, accuracy:  0.7337524682218882 0.6916666666666667\n",
      "575 번째 loss, accuracy:  0.7331962543445011 0.6916666666666667\n",
      "576 번째 loss, accuracy:  0.7326413610841875 0.6916666666666667\n",
      "577 번째 loss, accuracy:  0.7320875872203859 0.7\n",
      "578 번째 loss, accuracy:  0.7315349648873934 0.7\n",
      "579 번째 loss, accuracy:  0.730983839666187 0.7\n",
      "580 번째 loss, accuracy:  0.7304338103919648 0.7\n",
      "581 번째 loss, accuracy:  0.7298848198584352 0.7\n",
      "582 번째 loss, accuracy:  0.7293371661338156 0.7\n",
      "583 번째 loss, accuracy:  0.7287908513951312 0.7\n",
      "584 번째 loss, accuracy:  0.7282455786395324 0.7\n",
      "585 번째 loss, accuracy:  0.7277018249761656 0.7\n",
      "586 번째 loss, accuracy:  0.7271591996566011 0.7\n",
      "587 번째 loss, accuracy:  0.7266177943145661 0.7\n",
      "588 번째 loss, accuracy:  0.726077613231657 0.7\n",
      "589 번째 loss, accuracy:  0.7255388905123954 0.7\n",
      "590 번째 loss, accuracy:  0.7250011709903988 0.7\n",
      "591 번째 loss, accuracy:  0.7244646560298071 0.7\n",
      "592 번째 loss, accuracy:  0.7239294386283432 0.7\n",
      "593 번째 loss, accuracy:  0.7233955249759354 0.7\n",
      "594 번째 loss, accuracy:  0.7228626707791856 0.7\n",
      "595 번째 loss, accuracy:  0.7223311885543441 0.7\n",
      "596 번째 loss, accuracy:  0.721800707306232 0.7\n",
      "597 번째 loss, accuracy:  0.7212717050572847 0.7083333333333334\n",
      "598 번째 loss, accuracy:  0.7207437848597149 0.7083333333333334\n",
      "599 번째 loss, accuracy:  0.7202170849179496 0.7083333333333334\n",
      "600 번째 loss, accuracy:  0.719691703593746 0.7083333333333334\n",
      "601 번째 loss, accuracy:  0.7191674872247328 0.7083333333333334\n",
      "602 번째 loss, accuracy:  0.7186444608012615 0.7083333333333334\n",
      "603 번째 loss, accuracy:  0.7181226257829033 0.7083333333333334\n",
      "604 번째 loss, accuracy:  0.7176019762513536 0.7083333333333334\n",
      "605 번째 loss, accuracy:  0.717082577811658 0.7083333333333334\n",
      "606 번째 loss, accuracy:  0.7165640471564918 0.7083333333333334\n",
      "607 번째 loss, accuracy:  0.7160468244059178 0.7083333333333334\n",
      "608 번째 loss, accuracy:  0.7155309670522938 0.7083333333333334\n",
      "609 번째 loss, accuracy:  0.7150162809418591 0.7083333333333334\n",
      "610 번째 loss, accuracy:  0.7145026757921664 0.7083333333333334\n",
      "611 번째 loss, accuracy:  0.7139902158934783 0.7083333333333334\n",
      "612 번째 loss, accuracy:  0.7134790550782069 0.7083333333333334\n",
      "613 번째 loss, accuracy:  0.7129690217888943 0.7083333333333334\n",
      "614 번째 loss, accuracy:  0.7124602325960887 0.7083333333333334\n",
      "615 번째 loss, accuracy:  0.7119526417630266 0.7083333333333334\n",
      "616 번째 loss, accuracy:  0.7114462527070865 0.7083333333333334\n",
      "617 번째 loss, accuracy:  0.7109409366385141 0.7083333333333334\n",
      "618 번째 loss, accuracy:  0.7104369124323876 0.7083333333333334\n",
      "619 번째 loss, accuracy:  0.7099340010632915 0.7083333333333334\n",
      "620 번째 loss, accuracy:  0.7094322034500453 0.7083333333333334\n",
      "621 번째 loss, accuracy:  0.7089314364693587 0.7083333333333334\n",
      "622 번째 loss, accuracy:  0.7084315270674247 0.7083333333333334\n",
      "623 번째 loss, accuracy:  0.7079331422107534 0.7083333333333334\n",
      "624 번째 loss, accuracy:  0.7074360105556896 0.7083333333333334\n",
      "625 번째 loss, accuracy:  0.706940095783926 0.7083333333333334\n",
      "626 번째 loss, accuracy:  0.7064451712536001 0.7083333333333334\n",
      "627 번째 loss, accuracy:  0.7059515771900452 0.7083333333333334\n",
      "628 번째 loss, accuracy:  0.7054590754126548 0.7083333333333334\n",
      "629 번째 loss, accuracy:  0.7049676017566272 0.7083333333333334\n",
      "630 번째 loss, accuracy:  0.7044773390308967 0.7083333333333334\n",
      "631 번째 loss, accuracy:  0.7039881280962452 0.7083333333333334\n",
      "632 번째 loss, accuracy:  0.7035001902985428 0.7083333333333334\n",
      "633 번째 loss, accuracy:  0.7030130271573124 0.7083333333333334\n",
      "634 번째 loss, accuracy:  0.7025272737922713 0.7083333333333334\n",
      "635 번째 loss, accuracy:  0.7020427367667137 0.7083333333333334\n",
      "636 번째 loss, accuracy:  0.7015592601972972 0.7083333333333334\n",
      "637 번째 loss, accuracy:  0.701076929644003 0.7083333333333334\n",
      "638 번째 loss, accuracy:  0.7005956153693551 0.7083333333333334\n",
      "639 번째 loss, accuracy:  0.7001155554297772 0.7083333333333334\n",
      "640 번째 loss, accuracy:  0.6996365958315492 0.7083333333333334\n",
      "641 번째 loss, accuracy:  0.6991586275081818 0.7083333333333334\n",
      "642 번째 loss, accuracy:  0.6986819302913133 0.7083333333333334\n",
      "643 번째 loss, accuracy:  0.6982060982429575 0.7083333333333334\n",
      "644 번째 loss, accuracy:  0.6977315818677459 0.7083333333333334\n",
      "645 번째 loss, accuracy:  0.697258023422606 0.7083333333333334\n",
      "646 번째 loss, accuracy:  0.6967857906482643 0.7166666666666667\n",
      "647 번째 loss, accuracy:  0.6963145806080029 0.7166666666666667\n",
      "648 번째 loss, accuracy:  0.6958442851196551 0.7166666666666667\n",
      "649 번째 loss, accuracy:  0.6953753239111669 0.7166666666666667\n",
      "650 번째 loss, accuracy:  0.6949074428791204 0.7166666666666667\n",
      "651 번째 loss, accuracy:  0.694440576491111 0.7166666666666667\n",
      "652 번째 loss, accuracy:  0.6939749037051006 0.7166666666666667\n",
      "653 번째 loss, accuracy:  0.6935100678205465 0.7166666666666667\n",
      "654 번째 loss, accuracy:  0.6930463820328981 0.7166666666666667\n",
      "655 번째 loss, accuracy:  0.6925837858805796 0.7166666666666667\n",
      "656 번째 loss, accuracy:  0.6921224547475084 0.7166666666666667\n",
      "657 번째 loss, accuracy:  0.6916621157981531 0.7166666666666667\n",
      "658 번째 loss, accuracy:  0.6912028381790214 0.7166666666666667\n",
      "659 번째 loss, accuracy:  0.6907447418737571 0.7166666666666667\n",
      "660 번째 loss, accuracy:  0.6902875066496656 0.7166666666666667\n",
      "661 번째 loss, accuracy:  0.6898315891089316 0.7166666666666667\n",
      "662 번째 loss, accuracy:  0.689376668147259 0.7166666666666667\n",
      "663 번째 loss, accuracy:  0.6889227561693105 0.7166666666666667\n",
      "664 번째 loss, accuracy:  0.6884700068555036 0.7166666666666667\n",
      "665 번째 loss, accuracy:  0.6880182668566239 0.7166666666666667\n",
      "666 번째 loss, accuracy:  0.6875675640999667 0.7166666666666667\n",
      "667 번째 loss, accuracy:  0.6871179213986719 0.7166666666666667\n",
      "668 번째 loss, accuracy:  0.6866692759483467 0.7166666666666667\n",
      "669 번째 loss, accuracy:  0.6862217215846075 0.7166666666666667\n",
      "670 번째 loss, accuracy:  0.6857753061349904 0.7166666666666667\n",
      "671 번째 loss, accuracy:  0.6853299356687821 0.7166666666666667\n",
      "672 번째 loss, accuracy:  0.6848855918557769 0.7166666666666667\n",
      "673 번째 loss, accuracy:  0.6844422590087605 0.7166666666666667\n",
      "674 번째 loss, accuracy:  0.6839999686633934 0.7166666666666667\n",
      "675 번째 loss, accuracy:  0.6835587845818829 0.7166666666666667\n",
      "676 번째 loss, accuracy:  0.6831185312032019 0.7166666666666667\n",
      "677 번째 loss, accuracy:  0.6826793520094405 0.7166666666666667\n",
      "678 번째 loss, accuracy:  0.6822413285451638 0.7166666666666667\n",
      "679 번째 loss, accuracy:  0.6818041543262718 0.7166666666666667\n",
      "680 번째 loss, accuracy:  0.6813680876245793 0.7166666666666667\n",
      "681 번째 loss, accuracy:  0.6809331459920813 0.7166666666666667\n",
      "682 번째 loss, accuracy:  0.6804988573561992 0.7166666666666667\n",
      "683 번째 loss, accuracy:  0.6800657304589923 0.7166666666666667\n",
      "684 번째 loss, accuracy:  0.6796335020270005 0.7166666666666667\n",
      "685 번째 loss, accuracy:  0.6792023540921375 0.7166666666666667\n",
      "686 번째 loss, accuracy:  0.6787721679793712 0.7166666666666667\n",
      "687 번째 loss, accuracy:  0.6783432701294585 0.7166666666666667\n",
      "688 번째 loss, accuracy:  0.6779153481967413 0.7166666666666667\n",
      "689 번째 loss, accuracy:  0.6774883065106538 0.7166666666666667\n",
      "690 번째 loss, accuracy:  0.6770622812338861 0.7166666666666667\n",
      "691 번째 loss, accuracy:  0.6766372189470488 0.7166666666666667\n",
      "692 번째 loss, accuracy:  0.6762132575869084 0.7166666666666667\n",
      "693 번째 loss, accuracy:  0.6757903079542965 0.7166666666666667\n",
      "694 번째 loss, accuracy:  0.6753683097167104 0.7166666666666667\n",
      "695 번째 loss, accuracy:  0.6749473688343817 0.7166666666666667\n",
      "696 번째 loss, accuracy:  0.6745274363723441 0.7166666666666667\n",
      "697 번째 loss, accuracy:  0.6741085126009114 0.7166666666666667\n",
      "698 번째 loss, accuracy:  0.673690568110176 0.7166666666666667\n",
      "699 번째 loss, accuracy:  0.6732736002352264 0.7166666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 번째 loss, accuracy:  0.6728576082378865 0.7166666666666667\n",
      "701 번째 loss, accuracy:  0.6724426063983648 0.7166666666666667\n",
      "702 번째 loss, accuracy:  0.6720285486580628 0.7166666666666667\n",
      "703 번째 loss, accuracy:  0.6716153755031845 0.7166666666666667\n",
      "704 번째 loss, accuracy:  0.6712033255407064 0.7166666666666667\n",
      "705 번째 loss, accuracy:  0.6707921927691842 0.7166666666666667\n",
      "706 번째 loss, accuracy:  0.67038205563148 0.7166666666666667\n",
      "707 번째 loss, accuracy:  0.6699727041559124 0.7166666666666667\n",
      "708 번째 loss, accuracy:  0.6695644729750142 0.7166666666666667\n",
      "709 번째 loss, accuracy:  0.6691572154216726 0.7166666666666667\n",
      "710 번째 loss, accuracy:  0.6687509179422796 0.7166666666666667\n",
      "711 번째 loss, accuracy:  0.6683454332872336 0.7166666666666667\n",
      "712 번째 loss, accuracy:  0.6679410232533366 0.7166666666666667\n",
      "713 번째 loss, accuracy:  0.6675374975512685 0.7166666666666667\n",
      "714 번째 loss, accuracy:  0.6671348621758999 0.7166666666666667\n",
      "715 번째 loss, accuracy:  0.666733162228378 0.7166666666666667\n",
      "716 번째 loss, accuracy:  0.6663325243168109 0.7166666666666667\n",
      "717 번째 loss, accuracy:  0.6659328186915776 0.7166666666666667\n",
      "718 번째 loss, accuracy:  0.6655337583202725 0.7166666666666667\n",
      "719 번째 loss, accuracy:  0.6651358624751273 0.7166666666666667\n",
      "720 번째 loss, accuracy:  0.6647389939139333 0.7166666666666667\n",
      "721 번째 loss, accuracy:  0.6643430566083538 0.7166666666666667\n",
      "722 번째 loss, accuracy:  0.6639480339090637 0.7166666666666667\n",
      "723 번째 loss, accuracy:  0.6635539282920144 0.7166666666666667\n",
      "724 번째 loss, accuracy:  0.6631606907939747 0.7166666666666667\n",
      "725 번째 loss, accuracy:  0.6627682528567759 0.7166666666666667\n",
      "726 번째 loss, accuracy:  0.6623766829641561 0.7166666666666667\n",
      "727 번째 loss, accuracy:  0.6619862121027075 0.7166666666666667\n",
      "728 번째 loss, accuracy:  0.661596716387621 0.7166666666666667\n",
      "729 번째 loss, accuracy:  0.6612081272319786 0.725\n",
      "730 번째 loss, accuracy:  0.6608202624255438 0.725\n",
      "731 번째 loss, accuracy:  0.6604334023804682 0.725\n",
      "732 번째 loss, accuracy:  0.6600474787667453 0.725\n",
      "733 번째 loss, accuracy:  0.6596623585274258 0.725\n",
      "734 번째 loss, accuracy:  0.6592783130918515 0.725\n",
      "735 번째 loss, accuracy:  0.6588950669997743 0.725\n",
      "736 번째 loss, accuracy:  0.658512812730725 0.725\n",
      "737 번째 loss, accuracy:  0.6581314798908413 0.725\n",
      "738 번째 loss, accuracy:  0.657750997327179 0.725\n",
      "739 번째 loss, accuracy:  0.6573714434132873 0.725\n",
      "740 번째 loss, accuracy:  0.6569926963883446 0.725\n",
      "741 번째 loss, accuracy:  0.656614761921667 0.725\n",
      "742 번째 loss, accuracy:  0.6562378631311375 0.725\n",
      "743 번째 loss, accuracy:  0.6558617633196381 0.725\n",
      "744 번째 loss, accuracy:  0.655486574452227 0.725\n",
      "745 번째 loss, accuracy:  0.6551122609931238 0.725\n",
      "746 번째 loss, accuracy:  0.654738814985324 0.725\n",
      "747 번째 loss, accuracy:  0.6543662976388941 0.725\n",
      "748 번째 loss, accuracy:  0.6539944757684542 0.725\n",
      "749 번째 loss, accuracy:  0.6536234845458263 0.725\n",
      "750 번째 loss, accuracy:  0.6532534839318018 0.725\n",
      "751 번째 loss, accuracy:  0.6528843613305936 0.725\n",
      "752 번째 loss, accuracy:  0.6525161368023971 0.725\n",
      "753 번째 loss, accuracy:  0.6521487339620156 0.725\n",
      "754 번째 loss, accuracy:  0.6517821671892712 0.725\n",
      "755 번째 loss, accuracy:  0.6514164383957418 0.7333333333333333\n",
      "756 번째 loss, accuracy:  0.6510514114051756 0.7333333333333333\n",
      "757 번째 loss, accuracy:  0.6506874436051135 0.7333333333333333\n",
      "758 번째 loss, accuracy:  0.6503243584927276 0.7333333333333333\n",
      "759 번째 loss, accuracy:  0.6499620470750257 0.7333333333333333\n",
      "760 번째 loss, accuracy:  0.649600581742815 0.7333333333333333\n",
      "761 번째 loss, accuracy:  0.6492399389436656 0.7333333333333333\n",
      "762 번째 loss, accuracy:  0.6488801355289294 0.7333333333333333\n",
      "763 번째 loss, accuracy:  0.648521226616392 0.7333333333333333\n",
      "764 번째 loss, accuracy:  0.6481630919180154 0.7333333333333333\n",
      "765 번째 loss, accuracy:  0.6478058575610764 0.7333333333333333\n",
      "766 번째 loss, accuracy:  0.6474493945352066 0.7333333333333333\n",
      "767 번째 loss, accuracy:  0.6470938161013168 0.7333333333333333\n",
      "768 번째 loss, accuracy:  0.6467388827899712 0.7333333333333333\n",
      "769 번째 loss, accuracy:  0.6463848526745932 0.7333333333333333\n",
      "770 번째 loss, accuracy:  0.6460317086130162 0.7333333333333333\n",
      "771 번째 loss, accuracy:  0.6456793363875816 0.7333333333333333\n",
      "772 번째 loss, accuracy:  0.6453278287443518 0.7333333333333333\n",
      "773 번째 loss, accuracy:  0.6449771685115401 0.7333333333333333\n",
      "774 번째 loss, accuracy:  0.644627284795549 0.7333333333333333\n",
      "775 번째 loss, accuracy:  0.6442781924408716 0.7333333333333333\n",
      "776 번째 loss, accuracy:  0.6439298068674926 0.7333333333333333\n",
      "777 번째 loss, accuracy:  0.6435823428402939 0.7333333333333333\n",
      "778 번째 loss, accuracy:  0.6432357044084992 0.7333333333333333\n",
      "779 번째 loss, accuracy:  0.6428898926356524 0.7333333333333333\n",
      "780 번째 loss, accuracy:  0.6425447345808831 0.7333333333333333\n",
      "781 번째 loss, accuracy:  0.6422004251097108 0.7333333333333333\n",
      "782 번째 loss, accuracy:  0.6418569559802052 0.7333333333333333\n",
      "783 번째 loss, accuracy:  0.6415142032620558 0.7333333333333333\n",
      "784 번째 loss, accuracy:  0.6411722992762302 0.7333333333333333\n",
      "785 번째 loss, accuracy:  0.640831188770576 0.7333333333333333\n",
      "786 번째 loss, accuracy:  0.6404908889790162 0.7333333333333333\n",
      "787 번째 loss, accuracy:  0.6401513197139177 0.7333333333333333\n",
      "788 번째 loss, accuracy:  0.6398125722831819 0.7333333333333333\n",
      "789 번째 loss, accuracy:  0.639474558071008 0.7333333333333333\n",
      "790 번째 loss, accuracy:  0.6391373598412181 0.7333333333333333\n",
      "791 번째 loss, accuracy:  0.6388008852066908 0.7333333333333333\n",
      "792 번째 loss, accuracy:  0.6384652568500715 0.7333333333333333\n",
      "793 번째 loss, accuracy:  0.6381304425069644 0.7333333333333333\n",
      "794 번째 loss, accuracy:  0.6377963221778662 0.7333333333333333\n",
      "795 번째 loss, accuracy:  0.6374629665274016 0.7333333333333333\n",
      "796 번째 loss, accuracy:  0.6371304280932386 0.7333333333333333\n",
      "797 번째 loss, accuracy:  0.6367986873947729 0.7333333333333333\n",
      "798 번째 loss, accuracy:  0.6364675574651261 0.7333333333333333\n",
      "799 번째 loss, accuracy:  0.6361372455859635 0.7333333333333333\n",
      "800 번째 loss, accuracy:  0.6358077372768545 0.7333333333333333\n",
      "801 번째 loss, accuracy:  0.6354790095497883 0.7333333333333333\n",
      "802 번째 loss, accuracy:  0.6351510394953325 0.7416666666666667\n",
      "803 번째 loss, accuracy:  0.6348236688392627 0.7416666666666667\n",
      "804 번째 loss, accuracy:  0.6344971412058328 0.7416666666666667\n",
      "805 번째 loss, accuracy:  0.6341712101102196 0.7416666666666667\n",
      "806 번째 loss, accuracy:  0.6338460932174315 0.7416666666666667\n",
      "807 번째 loss, accuracy:  0.6335218332403184 0.7416666666666667\n",
      "808 번째 loss, accuracy:  0.6331982468379019 0.7416666666666667\n",
      "809 번째 loss, accuracy:  0.6328752781994804 0.7416666666666667\n",
      "810 번째 loss, accuracy:  0.632553224239043 0.7416666666666667\n",
      "811 번째 loss, accuracy:  0.6322318064620223 0.7416666666666667\n",
      "812 번째 loss, accuracy:  0.6319111505364152 0.7416666666666667\n",
      "813 번째 loss, accuracy:  0.6315913046989438 0.7416666666666667\n",
      "814 번째 loss, accuracy:  0.6312721011066746 0.7416666666666667\n",
      "815 번째 loss, accuracy:  0.6309537022661291 0.7416666666666667\n",
      "816 번째 loss, accuracy:  0.630635902003938 0.7416666666666667\n",
      "817 번째 loss, accuracy:  0.6303189074714268 0.7416666666666667\n",
      "818 번째 loss, accuracy:  0.6300025594424704 0.7416666666666667\n",
      "819 번째 loss, accuracy:  0.6296870570580089 0.7416666666666667\n",
      "820 번째 loss, accuracy:  0.6293722317539138 0.7416666666666667\n",
      "821 번째 loss, accuracy:  0.629058144959293 0.7416666666666667\n",
      "822 번째 loss, accuracy:  0.628744788290682 0.7416666666666667\n",
      "823 번째 loss, accuracy:  0.6284320756710386 0.7416666666666667\n",
      "824 번째 loss, accuracy:  0.6281201323264579 0.7416666666666667\n",
      "825 번째 loss, accuracy:  0.6278088443020685 0.7416666666666667\n",
      "826 번째 loss, accuracy:  0.6274982139208769 0.7416666666666667\n",
      "827 번째 loss, accuracy:  0.6271884058837507 0.7416666666666667\n",
      "828 번째 loss, accuracy:  0.6268792877257156 0.7416666666666667\n",
      "829 번째 loss, accuracy:  0.6265707731150078 0.7416666666666667\n",
      "830 번째 loss, accuracy:  0.6262630150387157 0.7416666666666667\n",
      "831 번째 loss, accuracy:  0.6259559593147498 0.7416666666666667\n",
      "832 번째 loss, accuracy:  0.6256495609247992 0.7416666666666667\n",
      "833 번째 loss, accuracy:  0.6253439101513767 0.7416666666666667\n",
      "834 번째 loss, accuracy:  0.6250389548112032 0.7416666666666667\n",
      "835 번째 loss, accuracy:  0.6247346582759462 0.7416666666666667\n",
      "836 번째 loss, accuracy:  0.6244309707843498 0.7416666666666667\n",
      "837 번째 loss, accuracy:  0.6241278757059943 0.7416666666666667\n",
      "838 번째 loss, accuracy:  0.6238255777869038 0.7416666666666667\n",
      "839 번째 loss, accuracy:  0.6235238365986197 0.7416666666666667\n",
      "840 번째 loss, accuracy:  0.6232229738330072 0.7416666666666667\n",
      "841 번째 loss, accuracy:  0.6229227898073354 0.7416666666666667\n",
      "842 번째 loss, accuracy:  0.6226232107931915 0.7416666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843 번째 loss, accuracy:  0.622324295692165 0.7416666666666667\n",
      "844 번째 loss, accuracy:  0.622026126852757 0.7416666666666667\n",
      "845 번째 loss, accuracy:  0.6217284778450566 0.7416666666666667\n",
      "846 번째 loss, accuracy:  0.6214315079327519 0.7416666666666667\n",
      "847 번째 loss, accuracy:  0.6211353076701385 0.7416666666666667\n",
      "848 번째 loss, accuracy:  0.6208398066773959 0.7416666666666667\n",
      "849 번째 loss, accuracy:  0.620544921227877 0.7416666666666667\n",
      "850 번째 loss, accuracy:  0.6202506695746267 0.7416666666666667\n",
      "851 번째 loss, accuracy:  0.6199571282694794 0.7416666666666667\n",
      "852 번째 loss, accuracy:  0.6196642648290012 0.7416666666666667\n",
      "853 번째 loss, accuracy:  0.6193719962085079 0.7416666666666667\n",
      "854 번째 loss, accuracy:  0.6190803722584128 0.7416666666666667\n",
      "855 번째 loss, accuracy:  0.6187893827139768 0.7416666666666667\n",
      "856 번째 loss, accuracy:  0.6184991097312661 0.7416666666666667\n",
      "857 번째 loss, accuracy:  0.6182094307313555 0.7416666666666667\n",
      "858 번째 loss, accuracy:  0.6179204376427698 0.7416666666666667\n",
      "859 번째 loss, accuracy:  0.6176320557169392 0.7416666666666667\n",
      "860 번째 loss, accuracy:  0.6173442564628019 0.7416666666666667\n",
      "861 번째 loss, accuracy:  0.6170569787232123 0.7416666666666667\n",
      "862 번째 loss, accuracy:  0.6167704539332689 0.7416666666666667\n",
      "863 번째 loss, accuracy:  0.6164845738779196 0.7416666666666667\n",
      "864 번째 loss, accuracy:  0.6161993998872061 0.7416666666666667\n",
      "865 번째 loss, accuracy:  0.6159148539213739 0.7416666666666667\n",
      "866 번째 loss, accuracy:  0.6156309317722685 0.7416666666666667\n",
      "867 번째 loss, accuracy:  0.6153476258200459 0.7416666666666667\n",
      "868 번째 loss, accuracy:  0.6150649454233715 0.7416666666666667\n",
      "869 번째 loss, accuracy:  0.6147829020507156 0.7416666666666667\n",
      "870 번째 loss, accuracy:  0.6145014597161621 0.7416666666666667\n",
      "871 번째 loss, accuracy:  0.6142206589502242 0.7416666666666667\n",
      "872 번째 loss, accuracy:  0.6139404600952709 0.7416666666666667\n",
      "873 번째 loss, accuracy:  0.6136608543839405 0.7416666666666667\n",
      "874 번째 loss, accuracy:  0.6133818853699107 0.7416666666666667\n",
      "875 번째 loss, accuracy:  0.6131035402939637 0.7416666666666667\n",
      "876 번째 loss, accuracy:  0.6128258336208446 0.7416666666666667\n",
      "877 번째 loss, accuracy:  0.6125486837535092 0.7416666666666667\n",
      "878 번째 loss, accuracy:  0.6122721549831589 0.7416666666666667\n",
      "879 번째 loss, accuracy:  0.61199624129679 0.7416666666666667\n",
      "880 번째 loss, accuracy:  0.6117209234144314 0.7416666666666667\n",
      "881 번째 loss, accuracy:  0.6114462422697439 0.7416666666666667\n",
      "882 번째 loss, accuracy:  0.6111721166362898 0.7416666666666667\n",
      "883 번째 loss, accuracy:  0.6108985055288992 0.7416666666666667\n",
      "884 번째 loss, accuracy:  0.6106255815608171 0.7416666666666667\n",
      "885 번째 loss, accuracy:  0.6103532988797213 0.7416666666666667\n",
      "886 번째 loss, accuracy:  0.6100816181719114 0.7416666666666667\n",
      "887 번째 loss, accuracy:  0.6098104964574875 0.7416666666666667\n",
      "888 번째 loss, accuracy:  0.6095398978907954 0.7416666666666667\n",
      "889 번째 loss, accuracy:  0.6092700037746723 0.7416666666666667\n",
      "890 번째 loss, accuracy:  0.6090006849428545 0.7416666666666667\n",
      "891 번째 loss, accuracy:  0.608731915321375 0.7416666666666667\n",
      "892 번째 loss, accuracy:  0.6084637055215586 0.7416666666666667\n",
      "893 번째 loss, accuracy:  0.6081961222335688 0.7416666666666667\n",
      "894 번째 loss, accuracy:  0.6079291210522937 0.7416666666666667\n",
      "895 번째 loss, accuracy:  0.6076627231226984 0.7416666666666667\n",
      "896 번째 loss, accuracy:  0.6073968932027677 0.7416666666666667\n",
      "897 번째 loss, accuracy:  0.6071316441685823 0.7416666666666667\n",
      "898 번째 loss, accuracy:  0.6068669764757008 0.7416666666666667\n",
      "899 번째 loss, accuracy:  0.6066027346022803 0.7416666666666667\n",
      "900 번째 loss, accuracy:  0.606339200555292 0.7416666666666667\n",
      "901 번째 loss, accuracy:  0.6060762381096014 0.7416666666666667\n",
      "902 번째 loss, accuracy:  0.6058138249204045 0.7416666666666667\n",
      "903 번째 loss, accuracy:  0.6055519948863174 0.7416666666666667\n",
      "904 번째 loss, accuracy:  0.6052906675829194 0.7416666666666667\n",
      "905 번째 loss, accuracy:  0.6050299074092728 0.7416666666666667\n",
      "906 번째 loss, accuracy:  0.6047697427206228 0.7416666666666667\n",
      "907 번째 loss, accuracy:  0.6045101730864144 0.7416666666666667\n",
      "908 번째 loss, accuracy:  0.6042510934463181 0.7416666666666667\n",
      "909 번째 loss, accuracy:  0.6039926029208845 0.7416666666666667\n",
      "910 번째 loss, accuracy:  0.603734697508735 0.7416666666666667\n",
      "911 번째 loss, accuracy:  0.6034773655017095 0.7416666666666667\n",
      "912 번째 loss, accuracy:  0.6032205800106522 0.7416666666666667\n",
      "913 번째 loss, accuracy:  0.6029643263092521 0.7416666666666667\n",
      "914 번째 loss, accuracy:  0.6027086336787836 0.7416666666666667\n",
      "915 번째 loss, accuracy:  0.6024534631667425 0.7416666666666667\n",
      "916 번째 loss, accuracy:  0.6021988676084828 0.7416666666666667\n",
      "917 번째 loss, accuracy:  0.6019447615590935 0.7416666666666667\n",
      "918 번째 loss, accuracy:  0.601691250923073 0.7416666666666667\n",
      "919 번째 loss, accuracy:  0.6014382116933082 0.7416666666666667\n",
      "920 번째 loss, accuracy:  0.6011857707743489 0.7416666666666667\n",
      "921 번째 loss, accuracy:  0.6009338753712817 0.7416666666666667\n",
      "922 번째 loss, accuracy:  0.6006825145551893 0.7416666666666667\n",
      "923 번째 loss, accuracy:  0.6004316957833462 0.7416666666666667\n",
      "924 번째 loss, accuracy:  0.6001813571063489 0.7416666666666667\n",
      "925 번째 loss, accuracy:  0.5999314966378632 0.7416666666666667\n",
      "926 번째 loss, accuracy:  0.5996822142661437 0.7416666666666667\n",
      "927 번째 loss, accuracy:  0.5994334618526819 0.7416666666666667\n",
      "928 번째 loss, accuracy:  0.5991852360395605 0.7416666666666667\n",
      "929 번째 loss, accuracy:  0.5989375775512246 0.7416666666666667\n",
      "930 번째 loss, accuracy:  0.5986904180729541 0.7416666666666667\n",
      "931 번째 loss, accuracy:  0.5984437967766555 0.7416666666666667\n",
      "932 번째 loss, accuracy:  0.5981977174781432 0.7416666666666667\n",
      "933 번째 loss, accuracy:  0.5979521834391626 0.7416666666666667\n",
      "934 번째 loss, accuracy:  0.597707137785989 0.7416666666666667\n",
      "935 번째 loss, accuracy:  0.5974626413666208 0.75\n",
      "936 번째 loss, accuracy:  0.5972185344865346 0.75\n",
      "937 번째 loss, accuracy:  0.5969750436532695 0.75\n",
      "938 번째 loss, accuracy:  0.5967320088889692 0.75\n",
      "939 번째 loss, accuracy:  0.5964895675001642 0.75\n",
      "940 번째 loss, accuracy:  0.5962476075764586 0.75\n",
      "941 번째 loss, accuracy:  0.5960061452858547 0.75\n",
      "942 번째 loss, accuracy:  0.5957652361566235 0.75\n",
      "943 번째 loss, accuracy:  0.5955247715670844 0.75\n",
      "944 번째 loss, accuracy:  0.5952847919678079 0.75\n",
      "945 번째 loss, accuracy:  0.5950453706286053 0.75\n",
      "946 번째 loss, accuracy:  0.5948063415608501 0.75\n",
      "947 번째 loss, accuracy:  0.5945679312475858 0.75\n",
      "948 번째 loss, accuracy:  0.5943299941047945 0.75\n",
      "949 번째 loss, accuracy:  0.5940925000064882 0.75\n",
      "950 번째 loss, accuracy:  0.5938555684447729 0.75\n",
      "951 번째 loss, accuracy:  0.5936190727457362 0.75\n",
      "952 번째 loss, accuracy:  0.5933830874131149 0.75\n",
      "953 번째 loss, accuracy:  0.5931476198808567 0.75\n",
      "954 번째 loss, accuracy:  0.5929125958462685 0.75\n",
      "955 번째 loss, accuracy:  0.5926781265119466 0.75\n",
      "956 번째 loss, accuracy:  0.5924441310086433 0.75\n",
      "957 번째 loss, accuracy:  0.5922106052054673 0.75\n",
      "958 번째 loss, accuracy:  0.5919775656118392 0.75\n",
      "959 번째 loss, accuracy:  0.5917450257220462 0.75\n",
      "960 번째 loss, accuracy:  0.591512942777152 0.75\n",
      "961 번째 loss, accuracy:  0.5912813795236211 0.75\n",
      "962 번째 loss, accuracy:  0.5910502918049116 0.75\n",
      "963 번째 loss, accuracy:  0.5908196467161173 0.75\n",
      "964 번째 loss, accuracy:  0.5905894400027232 0.75\n",
      "965 번째 loss, accuracy:  0.5903597050017896 0.75\n",
      "966 번째 loss, accuracy:  0.590130458202099 0.75\n",
      "967 번째 loss, accuracy:  0.5899017462848728 0.75\n",
      "968 번째 loss, accuracy:  0.5896734224123861 0.75\n",
      "969 번째 loss, accuracy:  0.589445606056663 0.75\n",
      "970 번째 loss, accuracy:  0.5892182790132295 0.75\n",
      "971 번째 loss, accuracy:  0.5889914127035946 0.75\n",
      "972 번째 loss, accuracy:  0.5887650360382035 0.75\n",
      "973 번째 loss, accuracy:  0.5885391257568096 0.75\n",
      "974 번째 loss, accuracy:  0.5883136735846335 0.75\n",
      "975 번째 loss, accuracy:  0.5880886441162223 0.75\n",
      "976 번째 loss, accuracy:  0.5878641368096055 0.75\n",
      "977 번째 loss, accuracy:  0.5876400580852418 0.75\n",
      "978 번째 loss, accuracy:  0.5874164435390549 0.75\n",
      "979 번째 loss, accuracy:  0.5871932685754103 0.75\n",
      "980 번째 loss, accuracy:  0.5869704896928428 0.75\n",
      "981 번째 loss, accuracy:  0.5867482095253759 0.75\n",
      "982 번째 loss, accuracy:  0.5865264160174546 0.75\n",
      "983 번째 loss, accuracy:  0.5863050309944595 0.75\n",
      "984 번째 loss, accuracy:  0.5860840979712613 0.75\n",
      "985 번째 loss, accuracy:  0.5858635900545203 0.75\n",
      "986 번째 loss, accuracy:  0.5856435957393721 0.75\n",
      "987 번째 loss, accuracy:  0.5854239643284435 0.75\n",
      "988 번째 loss, accuracy:  0.5852048487766801 0.75\n",
      "989 번째 loss, accuracy:  0.5849861050887191 0.75\n",
      "990 번째 loss, accuracy:  0.5847678873397435 0.75\n",
      "991 번째 loss, accuracy:  0.5845500604194916 0.75\n",
      "992 번째 loss, accuracy:  0.5843326469965828 0.75\n",
      "993 번째 loss, accuracy:  0.5841156943041302 0.7583333333333333\n",
      "994 번째 loss, accuracy:  0.5838991747651794 0.7583333333333333\n",
      "995 번째 loss, accuracy:  0.5836831138818226 0.7583333333333333\n",
      "996 번째 loss, accuracy:  0.5834674798660676 0.7583333333333333\n",
      "997 번째 loss, accuracy:  0.5832523175851734 0.7583333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998 번째 loss, accuracy:  0.5830375422951664 0.7583333333333333\n",
      "999 번째 loss, accuracy:  0.5828231594064894 0.7583333333333333\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 loss, accuracy:  1.1887849864095021 0.3333333333333333\n",
      "1 번째 loss, accuracy:  1.184768093822469 0.3333333333333333\n",
      "2 번째 loss, accuracy:  1.1809197508395435 0.3333333333333333\n",
      "3 번째 loss, accuracy:  1.1772440109352647 0.3333333333333333\n",
      "4 번째 loss, accuracy:  1.1737282918355576 0.3333333333333333\n",
      "5 번째 loss, accuracy:  1.1703671764796075 0.3333333333333333\n",
      "6 번째 loss, accuracy:  1.1671569946225517 0.3333333333333333\n",
      "7 번째 loss, accuracy:  1.1640810764917107 0.3333333333333333\n",
      "8 번째 loss, accuracy:  1.1611473716016107 0.3333333333333333\n",
      "9 번째 loss, accuracy:  1.1583416338437857 0.3333333333333333\n",
      "10 번째 loss, accuracy:  1.155659707403415 0.3333333333333333\n",
      "11 번째 loss, accuracy:  1.1530951187901641 0.3333333333333333\n",
      "12 번째 loss, accuracy:  1.150645205915268 0.3333333333333333\n",
      "13 번째 loss, accuracy:  1.148300926347349 0.3333333333333333\n",
      "14 번째 loss, accuracy:  1.1460655499281303 0.3333333333333333\n",
      "15 번째 loss, accuracy:  1.1439219247010874 0.3333333333333333\n",
      "16 번째 loss, accuracy:  1.141878378574331 0.3333333333333333\n",
      "17 번째 loss, accuracy:  1.1399256906387014 0.3333333333333333\n",
      "18 번째 loss, accuracy:  1.1380590570804523 0.3333333333333333\n",
      "19 번째 loss, accuracy:  1.1362734168007498 0.3333333333333333\n",
      "20 번째 loss, accuracy:  1.1345648063123213 0.3333333333333333\n",
      "21 번째 loss, accuracy:  1.1329297421341635 0.3333333333333333\n",
      "22 번째 loss, accuracy:  1.1313650120250938 0.3333333333333333\n",
      "23 번째 loss, accuracy:  1.1298666764027978 0.3333333333333333\n",
      "24 번째 loss, accuracy:  1.1284422332714887 0.3333333333333333\n",
      "25 번째 loss, accuracy:  1.1270808576834468 0.3333333333333333\n",
      "26 번째 loss, accuracy:  1.1257791359154063 0.3333333333333333\n",
      "27 번째 loss, accuracy:  1.124531329003228 0.3333333333333333\n",
      "28 번째 loss, accuracy:  1.1233402800645829 0.3333333333333333\n",
      "29 번째 loss, accuracy:  1.1221991049924804 0.3333333333333333\n",
      "30 번째 loss, accuracy:  1.1211079189907656 0.3333333333333333\n",
      "31 번째 loss, accuracy:  1.1200620203095784 0.3333333333333333\n",
      "32 번째 loss, accuracy:  1.1190624783626988 0.3333333333333333\n",
      "33 번째 loss, accuracy:  1.1181096524775178 0.3333333333333333\n",
      "34 번째 loss, accuracy:  1.1171986760890538 0.3333333333333333\n",
      "35 번째 loss, accuracy:  1.1163262966394079 0.3333333333333333\n",
      "36 번째 loss, accuracy:  1.1154906334238917 0.3333333333333333\n",
      "37 번째 loss, accuracy:  1.1146918120770921 0.3333333333333333\n",
      "38 번째 loss, accuracy:  1.113929176319087 0.3333333333333333\n",
      "39 번째 loss, accuracy:  1.113197639781112 0.3333333333333333\n",
      "40 번째 loss, accuracy:  1.1124986627432705 0.3333333333333333\n",
      "41 번째 loss, accuracy:  1.1118260270316769 0.3333333333333333\n",
      "42 번째 loss, accuracy:  1.1111842551397308 0.3333333333333333\n",
      "43 번째 loss, accuracy:  1.1105698713227383 0.3333333333333333\n",
      "44 번째 loss, accuracy:  1.1099803738264908 0.3333333333333333\n",
      "45 번째 loss, accuracy:  1.109417433064992 0.3333333333333333\n",
      "46 번째 loss, accuracy:  1.1088806646495533 0.3333333333333333\n",
      "47 번째 loss, accuracy:  1.1083683124457977 0.3333333333333333\n",
      "48 번째 loss, accuracy:  1.107876895521305 0.3333333333333333\n",
      "49 번째 loss, accuracy:  1.10740567052033 0.3333333333333333\n",
      "50 번째 loss, accuracy:  1.1069539457594832 0.3333333333333333\n",
      "51 번째 loss, accuracy:  1.1065211733989337 0.3333333333333333\n",
      "52 번째 loss, accuracy:  1.1061090622984875 0.3333333333333333\n",
      "53 번째 loss, accuracy:  1.1057137487469386 0.3333333333333333\n",
      "54 번째 loss, accuracy:  1.1053345007131605 0.3333333333333333\n",
      "55 번째 loss, accuracy:  1.1049730636051416 0.3333333333333333\n",
      "56 번째 loss, accuracy:  1.1046251246882761 0.3333333333333333\n",
      "57 번째 loss, accuracy:  1.1042919011341297 0.3333333333333333\n",
      "58 번째 loss, accuracy:  1.103973318239847 0.3333333333333333\n",
      "59 번째 loss, accuracy:  1.1036677309508938 0.3333333333333333\n",
      "60 번째 loss, accuracy:  1.1033741154175938 0.3333333333333333\n",
      "61 번째 loss, accuracy:  1.103094500361034 0.3333333333333333\n",
      "62 번째 loss, accuracy:  1.102823760232117 0.3333333333333333\n",
      "63 번째 loss, accuracy:  1.1025674279050213 0.3333333333333333\n",
      "64 번째 loss, accuracy:  1.1023216622872136 0.3333333333333333\n",
      "65 번째 loss, accuracy:  1.102085414868561 0.3333333333333333\n",
      "66 번째 loss, accuracy:  1.1018593519592363 0.3333333333333333\n",
      "67 번째 loss, accuracy:  1.1016438906590735 0.3333333333333333\n",
      "68 번째 loss, accuracy:  1.101436331489261 0.3333333333333333\n",
      "69 번째 loss, accuracy:  1.1012353375072774 0.3333333333333333\n",
      "70 번째 loss, accuracy:  1.1010438871109056 0.3333333333333333\n",
      "71 번째 loss, accuracy:  1.1008614240023926 0.3333333333333333\n",
      "72 번째 loss, accuracy:  1.1006862427454593 0.3333333333333333\n",
      "73 번째 loss, accuracy:  1.100516746949288 0.3333333333333333\n",
      "74 번째 loss, accuracy:  1.1003565157737538 0.3333333333333333\n",
      "75 번째 loss, accuracy:  1.100200990207677 0.3333333333333333\n",
      "76 번째 loss, accuracy:  1.1000522468596368 0.3333333333333333\n",
      "77 번째 loss, accuracy:  1.0999106978787732 0.3333333333333333\n",
      "78 번째 loss, accuracy:  1.0997749610889216 0.3333333333333333\n",
      "79 번째 loss, accuracy:  1.0996447881014353 0.3333333333333333\n",
      "80 번째 loss, accuracy:  1.09951979546766 0.3333333333333333\n",
      "81 번째 loss, accuracy:  1.0993982617829943 0.3333333333333333\n",
      "82 번째 loss, accuracy:  1.099282134163144 0.3333333333333333\n",
      "83 번째 loss, accuracy:  1.0991704493035641 0.3333333333333333\n",
      "84 번째 loss, accuracy:  1.099063208482229 0.3333333333333333\n",
      "85 번째 loss, accuracy:  1.098961529263247 0.3333333333333333\n",
      "86 번째 loss, accuracy:  1.0988642566263747 0.3333333333333333\n",
      "87 번째 loss, accuracy:  1.0987698827600014 0.3333333333333333\n",
      "88 번째 loss, accuracy:  1.0986803206823745 0.3333333333333333\n",
      "89 번째 loss, accuracy:  1.0985942881500848 0.3333333333333333\n",
      "90 번째 loss, accuracy:  1.098511427078292 0.3333333333333333\n",
      "91 번째 loss, accuracy:  1.098431676966371 0.3333333333333333\n",
      "92 번째 loss, accuracy:  1.0983556818912632 0.3333333333333333\n",
      "93 번째 loss, accuracy:  1.0982832621875669 0.3333333333333333\n",
      "94 번째 loss, accuracy:  1.0982135456708637 0.3333333333333333\n",
      "95 번째 loss, accuracy:  1.098146478571312 0.3333333333333333\n",
      "96 번째 loss, accuracy:  1.098081852560663 0.3333333333333333\n",
      "97 번째 loss, accuracy:  1.098019575334284 0.3333333333333333\n",
      "98 번째 loss, accuracy:  1.0979600710834234 0.3333333333333333\n",
      "99 번째 loss, accuracy:  1.0979028419059655 0.3333333333333333\n",
      "100 번째 loss, accuracy:  1.0978473690105044 0.3333333333333333\n",
      "101 번째 loss, accuracy:  1.0977952713147567 0.3333333333333333\n",
      "102 번째 loss, accuracy:  1.0977450137896547 0.3333333333333333\n",
      "103 번째 loss, accuracy:  1.0976960603098973 0.3333333333333333\n",
      "104 번째 loss, accuracy:  1.09764928294372 0.3333333333333333\n",
      "105 번째 loss, accuracy:  1.0976049430668728 0.3333333333333333\n",
      "106 번째 loss, accuracy:  1.097561700636469 0.3333333333333333\n",
      "107 번째 loss, accuracy:  1.0975212283258478 0.3333333333333333\n",
      "108 번째 loss, accuracy:  1.0974811215561686 0.3333333333333333\n",
      "109 번째 loss, accuracy:  1.097443544035726 0.3333333333333333\n",
      "110 번째 loss, accuracy:  1.0974064223192312 0.3333333333333333\n",
      "111 번째 loss, accuracy:  1.0973703353108297 0.3333333333333333\n",
      "112 번째 loss, accuracy:  1.0973356813392057 0.3333333333333333\n",
      "113 번째 loss, accuracy:  1.0973032113106482 0.3333333333333333\n",
      "114 번째 loss, accuracy:  1.0972732878419857 0.3333333333333333\n",
      "115 번째 loss, accuracy:  1.097242955648466 0.3333333333333333\n",
      "116 번째 loss, accuracy:  1.0972140536987074 0.3333333333333333\n",
      "117 번째 loss, accuracy:  1.0971863313143444 0.3333333333333333\n",
      "118 번째 loss, accuracy:  1.0971593300173057 0.3333333333333333\n",
      "119 번째 loss, accuracy:  1.0971342063002483 0.3333333333333333\n",
      "120 번째 loss, accuracy:  1.0971093143581725 0.325\n",
      "121 번째 loss, accuracy:  1.097085235962654 0.45\n",
      "122 번째 loss, accuracy:  1.0970613024197007 0.475\n",
      "123 번째 loss, accuracy:  1.0970394926684415 0.45\n",
      "124 번째 loss, accuracy:  1.0970185136657167 0.425\n",
      "125 번째 loss, accuracy:  1.0969985130652857 0.4083333333333333\n",
      "126 번째 loss, accuracy:  1.096978685794228 0.39166666666666666\n",
      "127 번째 loss, accuracy:  1.0969594143771209 0.38333333333333336\n",
      "128 번째 loss, accuracy:  1.0969407022954627 0.375\n",
      "129 번째 loss, accuracy:  1.0969232044741792 0.375\n",
      "130 번째 loss, accuracy:  1.096906557541183 0.36666666666666664\n",
      "131 번째 loss, accuracy:  1.0968905553449464 0.36666666666666664\n",
      "132 번째 loss, accuracy:  1.0968745909351427 0.36666666666666664\n",
      "133 번째 loss, accuracy:  1.0968588911693662 0.36666666666666664\n",
      "134 번째 loss, accuracy:  1.0968441451412025 0.36666666666666664\n",
      "135 번째 loss, accuracy:  1.0968300928091912 0.36666666666666664\n",
      "136 번째 loss, accuracy:  1.0968166249987694 0.36666666666666664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 번째 loss, accuracy:  1.0968033006998963 0.35833333333333334\n",
      "138 번째 loss, accuracy:  1.0967905233080786 0.35833333333333334\n",
      "139 번째 loss, accuracy:  1.0967780878699804 0.35833333333333334\n",
      "140 번째 loss, accuracy:  1.0967661614792175 0.35833333333333334\n",
      "141 번째 loss, accuracy:  1.0967545630976578 0.35833333333333334\n",
      "142 번째 loss, accuracy:  1.0967432237937862 0.35833333333333334\n",
      "143 번째 loss, accuracy:  1.0967325983785396 0.35833333333333334\n",
      "144 번째 loss, accuracy:  1.0967222224154691 0.35833333333333334\n",
      "145 번째 loss, accuracy:  1.09671221350283 0.35833333333333334\n",
      "146 번째 loss, accuracy:  1.0967023070874482 0.35833333333333334\n",
      "147 번째 loss, accuracy:  1.096692843207063 0.35833333333333334\n",
      "148 번째 loss, accuracy:  1.0966838656910296 0.35833333333333334\n",
      "149 번째 loss, accuracy:  1.0966753212835034 0.35833333333333334\n",
      "150 번째 loss, accuracy:  1.0966664367249137 0.35833333333333334\n",
      "151 번째 loss, accuracy:  1.0966585683761874 0.35833333333333334\n",
      "152 번째 loss, accuracy:  1.096650491137132 0.35833333333333334\n",
      "153 번째 loss, accuracy:  1.096642802337469 0.35833333333333334\n",
      "154 번째 loss, accuracy:  1.096635396269421 0.35833333333333334\n",
      "155 번째 loss, accuracy:  1.0966279741147948 0.35833333333333334\n",
      "156 번째 loss, accuracy:  1.0966209672153162 0.35833333333333334\n",
      "157 번째 loss, accuracy:  1.0966143258627032 0.35833333333333334\n",
      "158 번째 loss, accuracy:  1.0966078321270738 0.35833333333333334\n",
      "159 번째 loss, accuracy:  1.0966013971094144 0.35833333333333334\n",
      "160 번째 loss, accuracy:  1.0965952680280537 0.35833333333333334\n",
      "161 번째 loss, accuracy:  1.0965893593090217 0.35833333333333334\n",
      "162 번째 loss, accuracy:  1.096583305783156 0.35833333333333334\n",
      "163 번째 loss, accuracy:  1.0965778725830617 0.35833333333333334\n",
      "164 번째 loss, accuracy:  1.0965721436748244 0.35833333333333334\n",
      "165 번째 loss, accuracy:  1.0965669162012868 0.35833333333333334\n",
      "166 번째 loss, accuracy:  1.0965616622529801 0.35833333333333334\n",
      "167 번째 loss, accuracy:  1.0965565521699645 0.35833333333333334\n",
      "168 번째 loss, accuracy:  1.0965516522425673 0.35833333333333334\n",
      "169 번째 loss, accuracy:  1.0965467026634252 0.35833333333333334\n",
      "170 번째 loss, accuracy:  1.0965417950821472 0.35833333333333334\n",
      "171 번째 loss, accuracy:  1.096537302252427 0.35833333333333334\n",
      "172 번째 loss, accuracy:  1.0965326073495558 0.35833333333333334\n",
      "173 번째 loss, accuracy:  1.0965284524638892 0.35833333333333334\n",
      "174 번째 loss, accuracy:  1.096524155829286 0.35833333333333334\n",
      "175 번째 loss, accuracy:  1.096519828320982 0.35833333333333334\n",
      "176 번째 loss, accuracy:  1.09651583447357 0.35833333333333334\n",
      "177 번째 loss, accuracy:  1.096511741183742 0.35833333333333334\n",
      "178 번째 loss, accuracy:  1.0965075767752857 0.35833333333333334\n",
      "179 번째 loss, accuracy:  1.096503548220187 0.35833333333333334\n",
      "180 번째 loss, accuracy:  1.0964996433129424 0.35833333333333334\n",
      "181 번째 loss, accuracy:  1.0964959646824894 0.35833333333333334\n",
      "182 번째 loss, accuracy:  1.0964923308796142 0.35833333333333334\n",
      "183 번째 loss, accuracy:  1.0964887473285694 0.35833333333333334\n",
      "184 번째 loss, accuracy:  1.0964852739182953 0.35833333333333334\n",
      "185 번째 loss, accuracy:  1.0964817951694148 0.35833333333333334\n",
      "186 번째 loss, accuracy:  1.0964783557471602 0.35833333333333334\n",
      "187 번째 loss, accuracy:  1.0964750993213845 0.35833333333333334\n",
      "188 번째 loss, accuracy:  1.0964715598673678 0.35833333333333334\n",
      "189 번째 loss, accuracy:  1.0964684800689986 0.35833333333333334\n",
      "190 번째 loss, accuracy:  1.0964653233188022 0.35833333333333334\n",
      "191 번째 loss, accuracy:  1.0964621947725888 0.35833333333333334\n",
      "192 번째 loss, accuracy:  1.0964590879558005 0.35833333333333334\n",
      "193 번째 loss, accuracy:  1.096456083027086 0.35833333333333334\n",
      "194 번째 loss, accuracy:  1.0964531180675194 0.35833333333333334\n",
      "195 번째 loss, accuracy:  1.0964501901318024 0.35833333333333334\n",
      "196 번째 loss, accuracy:  1.096447301906726 0.35833333333333334\n",
      "197 번째 loss, accuracy:  1.0964444793624968 0.35833333333333334\n",
      "198 번째 loss, accuracy:  1.0964416209000303 0.35833333333333334\n",
      "199 번째 loss, accuracy:  1.096438818164593 0.35833333333333334\n",
      "200 번째 loss, accuracy:  1.096436147487517 0.35833333333333334\n",
      "201 번째 loss, accuracy:  1.096433458784266 0.35833333333333334\n",
      "202 번째 loss, accuracy:  1.0964306831205979 0.35833333333333334\n",
      "203 번째 loss, accuracy:  1.0964280530073642 0.35833333333333334\n",
      "204 번째 loss, accuracy:  1.096425423283777 0.35833333333333334\n",
      "205 번째 loss, accuracy:  1.0964227971259548 0.35833333333333334\n",
      "206 번째 loss, accuracy:  1.0964201371003932 0.35833333333333334\n",
      "207 번째 loss, accuracy:  1.096417607924827 0.35833333333333334\n",
      "208 번째 loss, accuracy:  1.0964150817878542 0.35833333333333334\n",
      "209 번째 loss, accuracy:  1.096412632742461 0.35833333333333334\n",
      "210 번째 loss, accuracy:  1.096410239356281 0.35833333333333334\n",
      "211 번째 loss, accuracy:  1.0964076523683162 0.35833333333333334\n",
      "212 번째 loss, accuracy:  1.0964051812478794 0.35833333333333334\n",
      "213 번째 loss, accuracy:  1.096402710060549 0.35833333333333334\n",
      "214 번째 loss, accuracy:  1.0964003464314347 0.35833333333333334\n",
      "215 번째 loss, accuracy:  1.0963979427941173 0.35833333333333334\n",
      "216 번째 loss, accuracy:  1.0963955324588095 0.35833333333333334\n",
      "217 번째 loss, accuracy:  1.096393176458343 0.35833333333333334\n",
      "218 번째 loss, accuracy:  1.0963907989577903 0.35833333333333334\n",
      "219 번째 loss, accuracy:  1.0963884486807625 0.35833333333333334\n",
      "220 번째 loss, accuracy:  1.0963860651646427 0.35833333333333334\n",
      "221 번째 loss, accuracy:  1.0963838321709891 0.35833333333333334\n",
      "222 번째 loss, accuracy:  1.0963814519276056 0.35833333333333334\n",
      "223 번째 loss, accuracy:  1.0963791884485032 0.35833333333333334\n",
      "224 번째 loss, accuracy:  1.0963768595111492 0.35833333333333334\n",
      "225 번째 loss, accuracy:  1.0963745841624708 0.35833333333333334\n",
      "226 번째 loss, accuracy:  1.0963723304808515 0.35833333333333334\n",
      "227 번째 loss, accuracy:  1.0963700681652921 0.35833333333333334\n",
      "228 번째 loss, accuracy:  1.0963677992911924 0.35833333333333334\n",
      "229 번째 loss, accuracy:  1.096365588800976 0.35833333333333334\n",
      "230 번째 loss, accuracy:  1.0963632908423961 0.35833333333333334\n",
      "231 번째 loss, accuracy:  1.0963610389499285 0.35833333333333334\n",
      "232 번째 loss, accuracy:  1.0963588162269882 0.35833333333333334\n",
      "233 번째 loss, accuracy:  1.0963565732844747 0.35833333333333334\n",
      "234 번째 loss, accuracy:  1.0963543704120615 0.35833333333333334\n",
      "235 번째 loss, accuracy:  1.0963521459264223 0.35833333333333334\n",
      "236 번째 loss, accuracy:  1.0963498672660166 0.35833333333333334\n",
      "237 번째 loss, accuracy:  1.0963476277618567 0.35833333333333334\n",
      "238 번째 loss, accuracy:  1.096345402611217 0.35833333333333334\n",
      "239 번째 loss, accuracy:  1.0963431480218149 0.35833333333333334\n",
      "240 번째 loss, accuracy:  1.096340907757901 0.35833333333333334\n",
      "241 번째 loss, accuracy:  1.0963386807626818 0.35833333333333334\n",
      "242 번째 loss, accuracy:  1.0963364855254563 0.35833333333333334\n",
      "243 번째 loss, accuracy:  1.096334212313 0.35833333333333334\n",
      "244 번째 loss, accuracy:  1.0963320000930465 0.35833333333333334\n",
      "245 번째 loss, accuracy:  1.0963297981574498 0.35833333333333334\n",
      "246 번째 loss, accuracy:  1.0963275868459799 0.35833333333333334\n",
      "247 번째 loss, accuracy:  1.0963253839980902 0.35833333333333334\n",
      "248 번째 loss, accuracy:  1.0963232155395601 0.35833333333333334\n",
      "249 번째 loss, accuracy:  1.096320983431764 0.35833333333333334\n",
      "250 번째 loss, accuracy:  1.0963187846370492 0.35833333333333334\n",
      "251 번째 loss, accuracy:  1.0963165927027454 0.35833333333333334\n",
      "252 번째 loss, accuracy:  1.096314357644519 0.35833333333333334\n",
      "253 번째 loss, accuracy:  1.0963121392457653 0.35833333333333334\n",
      "254 번째 loss, accuracy:  1.0963099259523694 0.35833333333333334\n",
      "255 번째 loss, accuracy:  1.096307710191714 0.35833333333333334\n",
      "256 번째 loss, accuracy:  1.096305491737884 0.35833333333333334\n",
      "257 번째 loss, accuracy:  1.0963032927010503 0.35833333333333334\n",
      "258 번째 loss, accuracy:  1.0963010534490725 0.35833333333333334\n",
      "259 번째 loss, accuracy:  1.0962988201548773 0.35833333333333334\n",
      "260 번째 loss, accuracy:  1.0962965563302705 0.35833333333333334\n",
      "261 번째 loss, accuracy:  1.096294311539019 0.35833333333333334\n",
      "262 번째 loss, accuracy:  1.0962920908548095 0.35833333333333334\n",
      "263 번째 loss, accuracy:  1.0962898398388419 0.35833333333333334\n",
      "264 번째 loss, accuracy:  1.096287612709282 0.35833333333333334\n",
      "265 번째 loss, accuracy:  1.0962853740048009 0.35833333333333334\n",
      "266 번째 loss, accuracy:  1.0962831051983306 0.35833333333333334\n",
      "267 번째 loss, accuracy:  1.0962808278683955 0.35833333333333334\n",
      "268 번째 loss, accuracy:  1.0962785597579345 0.35833333333333334\n",
      "269 번째 loss, accuracy:  1.0962763058955065 0.35833333333333334\n",
      "270 번째 loss, accuracy:  1.0962740303609313 0.35833333333333334\n",
      "271 번째 loss, accuracy:  1.0962717790952856 0.35833333333333334\n",
      "272 번째 loss, accuracy:  1.0962694949834677 0.35833333333333334\n",
      "273 번째 loss, accuracy:  1.0962672107148301 0.35833333333333334\n",
      "274 번째 loss, accuracy:  1.0962649278103214 0.35833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 번째 loss, accuracy:  1.0962626179873056 0.35833333333333334\n",
      "276 번째 loss, accuracy:  1.0962603244796154 0.35833333333333334\n",
      "277 번째 loss, accuracy:  1.0962580123834598 0.35833333333333334\n",
      "278 번째 loss, accuracy:  1.0962556915646766 0.35833333333333334\n",
      "279 번째 loss, accuracy:  1.0962533656589382 0.35833333333333334\n",
      "280 번째 loss, accuracy:  1.096251058388782 0.35833333333333334\n",
      "281 번째 loss, accuracy:  1.0962487264149445 0.35833333333333334\n",
      "282 번째 loss, accuracy:  1.0962464053091137 0.35833333333333334\n",
      "283 번째 loss, accuracy:  1.0962440871300638 0.35833333333333334\n",
      "284 번째 loss, accuracy:  1.0962417487569753 0.35833333333333334\n",
      "285 번째 loss, accuracy:  1.0962393989388566 0.35833333333333334\n",
      "286 번째 loss, accuracy:  1.0962370371064745 0.35833333333333334\n",
      "287 번째 loss, accuracy:  1.0962346698114567 0.35833333333333334\n",
      "288 번째 loss, accuracy:  1.0962322920052183 0.35833333333333334\n",
      "289 번째 loss, accuracy:  1.0962299309437014 0.35833333333333334\n",
      "290 번째 loss, accuracy:  1.0962275536146584 0.35833333333333334\n",
      "291 번째 loss, accuracy:  1.0962251783162609 0.35833333333333334\n",
      "292 번째 loss, accuracy:  1.096222791803559 0.35833333333333334\n",
      "293 번째 loss, accuracy:  1.0962203889262847 0.35833333333333334\n",
      "294 번째 loss, accuracy:  1.0962179807379797 0.35833333333333334\n",
      "295 번째 loss, accuracy:  1.0962155879709934 0.35833333333333334\n",
      "296 번째 loss, accuracy:  1.0962131769775654 0.35833333333333334\n",
      "297 번째 loss, accuracy:  1.0962107677740023 0.35833333333333334\n",
      "298 번째 loss, accuracy:  1.0962083405991017 0.35833333333333334\n",
      "299 번째 loss, accuracy:  1.096205920010599 0.35833333333333334\n",
      "300 번째 loss, accuracy:  1.0962034887621854 0.35833333333333334\n",
      "301 번째 loss, accuracy:  1.0962010445857244 0.35833333333333334\n",
      "302 번째 loss, accuracy:  1.0961985811834856 0.35833333333333334\n",
      "303 번째 loss, accuracy:  1.0961961253228476 0.35833333333333334\n",
      "304 번째 loss, accuracy:  1.0961936536901398 0.35833333333333334\n",
      "305 번째 loss, accuracy:  1.0961911773793112 0.35833333333333334\n",
      "306 번째 loss, accuracy:  1.0961886909010132 0.35833333333333334\n",
      "307 번째 loss, accuracy:  1.0961861922062732 0.35833333333333334\n",
      "308 번째 loss, accuracy:  1.0961836947639962 0.35833333333333334\n",
      "309 번째 loss, accuracy:  1.0961811850533678 0.35833333333333334\n",
      "310 번째 loss, accuracy:  1.0961786728291234 0.35833333333333334\n",
      "311 번째 loss, accuracy:  1.096176150382479 0.35833333333333334\n",
      "312 번째 loss, accuracy:  1.0961736176910066 0.35833333333333334\n",
      "313 번째 loss, accuracy:  1.096171078042855 0.35833333333333334\n",
      "314 번째 loss, accuracy:  1.0961685374183654 0.35833333333333334\n",
      "315 번째 loss, accuracy:  1.0961659822733234 0.35833333333333334\n",
      "316 번째 loss, accuracy:  1.0961634151427524 0.35833333333333334\n",
      "317 번째 loss, accuracy:  1.0961608468171742 0.35833333333333334\n",
      "318 번째 loss, accuracy:  1.0961582803224237 0.35833333333333334\n",
      "319 번째 loss, accuracy:  1.0961556937745272 0.35833333333333334\n",
      "320 번째 loss, accuracy:  1.0961531051228504 0.35833333333333334\n",
      "321 번째 loss, accuracy:  1.0961504997413622 0.35833333333333334\n",
      "322 번째 loss, accuracy:  1.09614788488948 0.35833333333333334\n",
      "323 번째 loss, accuracy:  1.0961452746136409 0.35833333333333334\n",
      "324 번째 loss, accuracy:  1.0961426574476116 0.35833333333333334\n",
      "325 번째 loss, accuracy:  1.0961400226177134 0.35833333333333334\n",
      "326 번째 loss, accuracy:  1.0961373759051933 0.35833333333333334\n",
      "327 번째 loss, accuracy:  1.0961347258170653 0.35833333333333334\n",
      "328 번째 loss, accuracy:  1.0961320640326495 0.35833333333333334\n",
      "329 번째 loss, accuracy:  1.0961293873596398 0.35833333333333334\n",
      "330 번째 loss, accuracy:  1.0961267112763542 0.35833333333333334\n",
      "331 번째 loss, accuracy:  1.096124026437473 0.35833333333333334\n",
      "332 번째 loss, accuracy:  1.096121331436445 0.35833333333333334\n",
      "333 번째 loss, accuracy:  1.096118631003945 0.35833333333333334\n",
      "334 번째 loss, accuracy:  1.0961159161653447 0.35833333333333334\n",
      "335 번째 loss, accuracy:  1.096113197391909 0.35833333333333334\n",
      "336 번째 loss, accuracy:  1.0961104681381793 0.35833333333333334\n",
      "337 번째 loss, accuracy:  1.0961077295788821 0.35833333333333334\n",
      "338 번째 loss, accuracy:  1.0961049901067041 0.35833333333333334\n",
      "339 번째 loss, accuracy:  1.0961022310003374 0.35833333333333334\n",
      "340 번째 loss, accuracy:  1.0960994650809712 0.35833333333333334\n",
      "341 번째 loss, accuracy:  1.0960966941505415 0.35833333333333334\n",
      "342 번째 loss, accuracy:  1.0960939102806035 0.35833333333333334\n",
      "343 번째 loss, accuracy:  1.0960911094512922 0.35833333333333334\n",
      "344 번째 loss, accuracy:  1.0960883006929507 0.35833333333333334\n",
      "345 번째 loss, accuracy:  1.0960854813334309 0.35833333333333334\n",
      "346 번째 loss, accuracy:  1.0960826493383278 0.35833333333333334\n",
      "347 번째 loss, accuracy:  1.0960798086607433 0.35833333333333334\n",
      "348 번째 loss, accuracy:  1.0960769551612892 0.35833333333333334\n",
      "349 번째 loss, accuracy:  1.0960740983107307 0.35833333333333334\n",
      "350 번째 loss, accuracy:  1.096071230286934 0.35833333333333334\n",
      "351 번째 loss, accuracy:  1.0960683503918958 0.35833333333333334\n",
      "352 번째 loss, accuracy:  1.09606546570475 0.35833333333333334\n",
      "353 번째 loss, accuracy:  1.0960625621755824 0.35833333333333334\n",
      "354 번째 loss, accuracy:  1.0960596565525458 0.35833333333333334\n",
      "355 번째 loss, accuracy:  1.0960567387769555 0.35833333333333334\n",
      "356 번째 loss, accuracy:  1.0960538139294405 0.35833333333333334\n",
      "357 번째 loss, accuracy:  1.0960508762816417 0.35833333333333334\n",
      "358 번째 loss, accuracy:  1.0960479211906435 0.35833333333333334\n",
      "359 번째 loss, accuracy:  1.096044960432887 0.35833333333333334\n",
      "360 번째 loss, accuracy:  1.0960419878841299 0.35833333333333334\n",
      "361 번째 loss, accuracy:  1.0960390057276277 0.35833333333333334\n",
      "362 번째 loss, accuracy:  1.0960360116583383 0.35833333333333334\n",
      "363 번째 loss, accuracy:  1.0960330064714905 0.35833333333333334\n",
      "364 번째 loss, accuracy:  1.0960299911100404 0.35833333333333334\n",
      "365 번째 loss, accuracy:  1.0960269666807918 0.35833333333333334\n",
      "366 번째 loss, accuracy:  1.0960239261987845 0.35833333333333334\n",
      "367 번째 loss, accuracy:  1.0960208723269256 0.35833333333333334\n",
      "368 번째 loss, accuracy:  1.0960178088585903 0.35833333333333334\n",
      "369 번째 loss, accuracy:  1.0960147366221469 0.35833333333333334\n",
      "370 번째 loss, accuracy:  1.0960116496976224 0.35833333333333334\n",
      "371 번째 loss, accuracy:  1.0960085527455519 0.35833333333333334\n",
      "372 번째 loss, accuracy:  1.0960054440623035 0.35833333333333334\n",
      "373 번째 loss, accuracy:  1.0960023262522585 0.35833333333333334\n",
      "374 번째 loss, accuracy:  1.0959991926479462 0.35833333333333334\n",
      "375 번째 loss, accuracy:  1.0959960473803492 0.35833333333333334\n",
      "376 번째 loss, accuracy:  1.0959928926154296 0.35833333333333334\n",
      "377 번째 loss, accuracy:  1.0959897242543841 0.35833333333333334\n",
      "378 번째 loss, accuracy:  1.0959865444266355 0.35833333333333334\n",
      "379 번째 loss, accuracy:  1.0959833503150327 0.35833333333333334\n",
      "380 번째 loss, accuracy:  1.0959801478534288 0.35833333333333334\n",
      "381 번째 loss, accuracy:  1.0959769283733027 0.35833333333333334\n",
      "382 번째 loss, accuracy:  1.09597369632037 0.35833333333333334\n",
      "383 번째 loss, accuracy:  1.0959704551061364 0.35833333333333334\n",
      "384 번째 loss, accuracy:  1.0959671983100823 0.35833333333333334\n",
      "385 번째 loss, accuracy:  1.095963929584111 0.35833333333333334\n",
      "386 번째 loss, accuracy:  1.0959606466904854 0.35833333333333334\n",
      "387 번째 loss, accuracy:  1.0959573536372196 0.35833333333333334\n",
      "388 번째 loss, accuracy:  1.095954045538058 0.35833333333333334\n",
      "389 번째 loss, accuracy:  1.095950725420395 0.35833333333333334\n",
      "390 번째 loss, accuracy:  1.0959473928346422 0.35833333333333334\n",
      "391 번째 loss, accuracy:  1.0959440435935173 0.35833333333333334\n",
      "392 번째 loss, accuracy:  1.0959406801086249 0.35833333333333334\n",
      "393 번째 loss, accuracy:  1.095937305224468 0.35833333333333334\n",
      "394 번째 loss, accuracy:  1.0959339187302999 0.35833333333333334\n",
      "395 번째 loss, accuracy:  1.09593051552005 0.35833333333333334\n",
      "396 번째 loss, accuracy:  1.0959270991981065 0.35833333333333334\n",
      "397 번째 loss, accuracy:  1.0959236683566833 0.35833333333333334\n",
      "398 번째 loss, accuracy:  1.0959202227728013 0.35833333333333334\n",
      "399 번째 loss, accuracy:  1.095916764199204 0.35833333333333334\n",
      "400 번째 loss, accuracy:  1.0959132917107188 0.35833333333333334\n",
      "401 번째 loss, accuracy:  1.0959098047029545 0.35833333333333334\n",
      "402 번째 loss, accuracy:  1.0959063031815703 0.35833333333333334\n",
      "403 번째 loss, accuracy:  1.0959027883403807 0.35833333333333334\n",
      "404 번째 loss, accuracy:  1.0958992580385627 0.35833333333333334\n",
      "405 번째 loss, accuracy:  1.095895712117937 0.35833333333333334\n",
      "406 번째 loss, accuracy:  1.0958921512440873 0.35833333333333334\n",
      "407 번째 loss, accuracy:  1.0958885730372283 0.35833333333333334\n",
      "408 번째 loss, accuracy:  1.0958849817913916 0.35833333333333334\n",
      "409 번째 loss, accuracy:  1.095881376292352 0.35833333333333334\n",
      "410 번째 loss, accuracy:  1.0958777558963952 0.35833333333333334\n",
      "411 번째 loss, accuracy:  1.09587412006892 0.35833333333333334\n",
      "412 번째 loss, accuracy:  1.0958704685758103 0.35833333333333334\n",
      "413 번째 loss, accuracy:  1.0958667992208182 0.35833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414 번째 loss, accuracy:  1.0958631173789717 0.35833333333333334\n",
      "415 번째 loss, accuracy:  1.0958594167374565 0.35833333333333334\n",
      "416 번째 loss, accuracy:  1.0958557027903084 0.35833333333333334\n",
      "417 번째 loss, accuracy:  1.0958519729584935 0.35833333333333334\n",
      "418 번째 loss, accuracy:  1.095848224976037 0.35833333333333334\n",
      "419 번째 loss, accuracy:  1.095844461002751 0.35833333333333334\n",
      "420 번째 loss, accuracy:  1.0958406809395218 0.35833333333333334\n",
      "421 번째 loss, accuracy:  1.0958368829599412 0.35833333333333334\n",
      "422 번째 loss, accuracy:  1.0958330696019123 0.35833333333333334\n",
      "423 번째 loss, accuracy:  1.095829237670236 0.35833333333333334\n",
      "424 번째 loss, accuracy:  1.0958253905472122 0.35833333333333334\n",
      "425 번째 loss, accuracy:  1.0958215251564538 0.35833333333333334\n",
      "426 번째 loss, accuracy:  1.0958176444537806 0.35833333333333334\n",
      "427 번째 loss, accuracy:  1.0958137464958255 0.35833333333333334\n",
      "428 번째 loss, accuracy:  1.0958098291883673 0.35833333333333334\n",
      "429 번째 loss, accuracy:  1.0958058964202357 0.35833333333333334\n",
      "430 번째 loss, accuracy:  1.0958019443786093 0.35833333333333334\n",
      "431 번째 loss, accuracy:  1.095797974433441 0.35833333333333334\n",
      "432 번째 loss, accuracy:  1.0957939868295719 0.35833333333333334\n",
      "433 번째 loss, accuracy:  1.0957899810196916 0.35833333333333334\n",
      "434 번째 loss, accuracy:  1.0957859582751017 0.35833333333333334\n",
      "435 번째 loss, accuracy:  1.0957819176144725 0.35833333333333334\n",
      "436 번째 loss, accuracy:  1.0957778579782207 0.35833333333333334\n",
      "437 번째 loss, accuracy:  1.0957737783992807 0.35833333333333334\n",
      "438 번째 loss, accuracy:  1.0957696836926147 0.35833333333333334\n",
      "439 번째 loss, accuracy:  1.0957655660525527 0.35833333333333334\n",
      "440 번째 loss, accuracy:  1.0957614311388428 0.35833333333333334\n",
      "441 번째 loss, accuracy:  1.0957572754104499 0.35833333333333334\n",
      "442 번째 loss, accuracy:  1.095753102177597 0.35833333333333334\n",
      "443 번째 loss, accuracy:  1.0957489071528352 0.35833333333333334\n",
      "444 번째 loss, accuracy:  1.0957446924725884 0.35833333333333334\n",
      "445 번째 loss, accuracy:  1.0957404584409665 0.35833333333333334\n",
      "446 번째 loss, accuracy:  1.0957362069083278 0.35833333333333334\n",
      "447 번째 loss, accuracy:  1.0957319317959673 0.35833333333333334\n",
      "448 번째 loss, accuracy:  1.0957276382399257 0.35833333333333334\n",
      "449 번째 loss, accuracy:  1.0957233249328613 0.35833333333333334\n",
      "450 번째 loss, accuracy:  1.095718990015969 0.35833333333333334\n",
      "451 번째 loss, accuracy:  1.0957146333725047 0.35833333333333334\n",
      "452 번째 loss, accuracy:  1.0957102574315472 0.35833333333333334\n",
      "453 번째 loss, accuracy:  1.095705862035358 0.35833333333333334\n",
      "454 번째 loss, accuracy:  1.0957014427034233 0.35833333333333334\n",
      "455 번째 loss, accuracy:  1.0956970009385538 0.35833333333333334\n",
      "456 번째 loss, accuracy:  1.0956925405360871 0.35833333333333334\n",
      "457 번째 loss, accuracy:  1.0956880581262984 0.35833333333333334\n",
      "458 번째 loss, accuracy:  1.0956835548592387 0.35833333333333334\n",
      "459 번째 loss, accuracy:  1.0956790271079258 0.35833333333333334\n",
      "460 번째 loss, accuracy:  1.0956744785240566 0.35833333333333334\n",
      "461 번째 loss, accuracy:  1.0956699064706636 0.35833333333333334\n",
      "462 번째 loss, accuracy:  1.09566531131886 0.35833333333333334\n",
      "463 번째 loss, accuracy:  1.09566069417842 0.35833333333333334\n",
      "464 번째 loss, accuracy:  1.095656054250636 0.35833333333333334\n",
      "465 번째 loss, accuracy:  1.0956513915341455 0.35833333333333334\n",
      "466 번째 loss, accuracy:  1.0956467042059246 0.35833333333333334\n",
      "467 번째 loss, accuracy:  1.0956419930261077 0.35833333333333334\n",
      "468 번째 loss, accuracy:  1.0956372590811823 0.35833333333333334\n",
      "469 번째 loss, accuracy:  1.095632502845265 0.35833333333333334\n",
      "470 번째 loss, accuracy:  1.0956277210671597 0.35833333333333334\n",
      "471 번째 loss, accuracy:  1.0956229123836294 0.35833333333333334\n",
      "472 번째 loss, accuracy:  1.0956180799761919 0.35833333333333334\n",
      "473 번째 loss, accuracy:  1.0956132246636598 0.35833333333333334\n",
      "474 번째 loss, accuracy:  1.0956083435334363 0.35833333333333334\n",
      "475 번째 loss, accuracy:  1.0956034394811682 0.35833333333333334\n",
      "476 번째 loss, accuracy:  1.0955985085492648 0.35833333333333334\n",
      "477 번째 loss, accuracy:  1.0955935535798176 0.35833333333333334\n",
      "478 번째 loss, accuracy:  1.095588570764517 0.35833333333333334\n",
      "479 번째 loss, accuracy:  1.0955835630736324 0.35833333333333334\n",
      "480 번째 loss, accuracy:  1.0955785260201785 0.35833333333333334\n",
      "481 번째 loss, accuracy:  1.0955734643816777 0.35833333333333334\n",
      "482 번째 loss, accuracy:  1.0955683767593378 0.35833333333333334\n",
      "483 번째 loss, accuracy:  1.0955632613402135 0.35833333333333334\n",
      "484 번째 loss, accuracy:  1.0955581200961833 0.35833333333333334\n",
      "485 번째 loss, accuracy:  1.0955529502108836 0.35833333333333334\n",
      "486 번째 loss, accuracy:  1.0955477543297634 0.35833333333333334\n",
      "487 번째 loss, accuracy:  1.0955425311351719 0.35833333333333334\n",
      "488 번째 loss, accuracy:  1.0955372795801102 0.35833333333333334\n",
      "489 번째 loss, accuracy:  1.0955319997660942 0.35833333333333334\n",
      "490 번째 loss, accuracy:  1.09552669169448 0.35833333333333334\n",
      "491 번째 loss, accuracy:  1.0955213546834268 0.35833333333333334\n",
      "492 번째 loss, accuracy:  1.0955159875824985 0.35833333333333334\n",
      "493 번째 loss, accuracy:  1.0955105920824206 0.35833333333333334\n",
      "494 번째 loss, accuracy:  1.0955051682539259 0.35833333333333334\n",
      "495 번째 loss, accuracy:  1.0954997138020621 0.35833333333333334\n",
      "496 번째 loss, accuracy:  1.0954942286746525 0.35833333333333334\n",
      "497 번째 loss, accuracy:  1.095488713556946 0.35833333333333334\n",
      "498 번째 loss, accuracy:  1.0954831671728509 0.35833333333333334\n",
      "499 번째 loss, accuracy:  1.095477592164994 0.35833333333333334\n",
      "500 번째 loss, accuracy:  1.0954719866349503 0.35833333333333334\n",
      "501 번째 loss, accuracy:  1.0954663475469422 0.35833333333333334\n",
      "502 번째 loss, accuracy:  1.0954606768979167 0.35833333333333334\n",
      "503 번째 loss, accuracy:  1.0954549756437018 0.35833333333333334\n",
      "504 번째 loss, accuracy:  1.0954492402047813 0.35833333333333334\n",
      "505 번째 loss, accuracy:  1.0954434724895212 0.35833333333333334\n",
      "506 번째 loss, accuracy:  1.0954376730211095 0.35833333333333334\n",
      "507 번째 loss, accuracy:  1.0954318421996951 0.35833333333333334\n",
      "508 번째 loss, accuracy:  1.0954259756868396 0.35833333333333334\n",
      "509 번째 loss, accuracy:  1.095420076400682 0.35833333333333334\n",
      "510 번째 loss, accuracy:  1.095414144376727 0.35833333333333334\n",
      "511 번째 loss, accuracy:  1.0954081748555349 0.35833333333333334\n",
      "512 번째 loss, accuracy:  1.0954021729184362 0.35833333333333334\n",
      "513 번째 loss, accuracy:  1.0953961362136477 0.35833333333333334\n",
      "514 번째 loss, accuracy:  1.0953900632772609 0.35833333333333334\n",
      "515 번째 loss, accuracy:  1.0953839558943368 0.35833333333333334\n",
      "516 번째 loss, accuracy:  1.0953778103694785 0.35833333333333334\n",
      "517 번째 loss, accuracy:  1.095371631418978 0.35833333333333334\n",
      "518 번째 loss, accuracy:  1.095365415330685 0.35833333333333334\n",
      "519 번째 loss, accuracy:  1.0953591608414988 0.35833333333333334\n",
      "520 번째 loss, accuracy:  1.0953528701171193 0.35833333333333334\n",
      "521 번째 loss, accuracy:  1.0953465418592185 0.35833333333333334\n",
      "522 번째 loss, accuracy:  1.0953401763766708 0.35833333333333334\n",
      "523 번째 loss, accuracy:  1.0953337729884027 0.35833333333333334\n",
      "524 번째 loss, accuracy:  1.0953273316283714 0.35833333333333334\n",
      "525 번째 loss, accuracy:  1.0953208498338143 0.35833333333333334\n",
      "526 번째 loss, accuracy:  1.0953143295149574 0.35833333333333334\n",
      "527 번째 loss, accuracy:  1.0953077693611002 0.35833333333333334\n",
      "528 번째 loss, accuracy:  1.0953011696918231 0.35833333333333334\n",
      "529 번째 loss, accuracy:  1.0952945291179077 0.35833333333333334\n",
      "530 번째 loss, accuracy:  1.0952878457045798 0.35833333333333334\n",
      "531 번째 loss, accuracy:  1.0952811224802166 0.35833333333333334\n",
      "532 번째 loss, accuracy:  1.0952743582655828 0.35833333333333334\n",
      "533 번째 loss, accuracy:  1.0952675520786028 0.35833333333333334\n",
      "534 번째 loss, accuracy:  1.0952607032103865 0.35833333333333334\n",
      "535 번째 loss, accuracy:  1.0952538138190333 0.35833333333333334\n",
      "536 번째 loss, accuracy:  1.0952468782533462 0.35833333333333334\n",
      "537 번째 loss, accuracy:  1.0952398988946677 0.35833333333333334\n",
      "538 번째 loss, accuracy:  1.0952328776142901 0.35833333333333334\n",
      "539 번째 loss, accuracy:  1.0952258080711867 0.35833333333333334\n",
      "540 번째 loss, accuracy:  1.0952186941165138 0.35833333333333334\n",
      "541 번째 loss, accuracy:  1.0952115356568943 0.35833333333333334\n",
      "542 번째 loss, accuracy:  1.095204334052901 0.35833333333333334\n",
      "543 번째 loss, accuracy:  1.0951970827474184 0.35833333333333334\n",
      "544 번째 loss, accuracy:  1.0951897857773114 0.35833333333333334\n",
      "545 번째 loss, accuracy:  1.0951824390346614 0.35833333333333334\n",
      "546 번째 loss, accuracy:  1.0951750477043987 0.35833333333333334\n",
      "547 번째 loss, accuracy:  1.0951676091930138 0.35833333333333334\n",
      "548 번째 loss, accuracy:  1.095160120207668 0.35833333333333334\n",
      "549 번째 loss, accuracy:  1.0951525841516196 0.35833333333333334\n",
      "550 번째 loss, accuracy:  1.0951449924624592 0.35833333333333334\n",
      "551 번째 loss, accuracy:  1.0951373529149693 0.35833333333333334\n",
      "552 번째 loss, accuracy:  1.0951296625794167 0.35833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553 번째 loss, accuracy:  1.0951219196968995 0.35833333333333334\n",
      "554 번째 loss, accuracy:  1.095114126984879 0.35833333333333334\n",
      "555 번째 loss, accuracy:  1.095106281676334 0.35833333333333334\n",
      "556 번째 loss, accuracy:  1.0950983851288265 0.35833333333333334\n",
      "557 번째 loss, accuracy:  1.0950904330737847 0.35833333333333334\n",
      "558 번째 loss, accuracy:  1.0950824274463178 0.35833333333333334\n",
      "559 번째 loss, accuracy:  1.0950743677280093 0.35833333333333334\n",
      "560 번째 loss, accuracy:  1.0950662536582265 0.35833333333333334\n",
      "561 번째 loss, accuracy:  1.0950580848072886 0.35833333333333334\n",
      "562 번째 loss, accuracy:  1.0950498604029812 0.35833333333333334\n",
      "563 번째 loss, accuracy:  1.0950415773359765 0.35833333333333334\n",
      "564 번째 loss, accuracy:  1.095033238040559 0.35833333333333334\n",
      "565 번째 loss, accuracy:  1.0950248381435737 0.35833333333333334\n",
      "566 번째 loss, accuracy:  1.0950163782582423 0.35833333333333334\n",
      "567 번째 loss, accuracy:  1.0950078593253605 0.35833333333333334\n",
      "568 번째 loss, accuracy:  1.0949992828315753 0.35833333333333334\n",
      "569 번째 loss, accuracy:  1.0949906437673513 0.35833333333333334\n",
      "570 번째 loss, accuracy:  1.0949819447705134 0.35833333333333334\n",
      "571 번째 loss, accuracy:  1.0949731821920707 0.35833333333333334\n",
      "572 번째 loss, accuracy:  1.0949643602433667 0.35833333333333334\n",
      "573 번째 loss, accuracy:  1.094955475995721 0.35833333333333334\n",
      "574 번째 loss, accuracy:  1.0949465227368205 0.35833333333333334\n",
      "575 번째 loss, accuracy:  1.0949375055559505 0.35833333333333334\n",
      "576 번째 loss, accuracy:  1.0949284227111844 0.35833333333333334\n",
      "577 번째 loss, accuracy:  1.0949192764318956 0.35833333333333334\n",
      "578 번째 loss, accuracy:  1.0949100620642491 0.35833333333333334\n",
      "579 번째 loss, accuracy:  1.094900780278206 0.35833333333333334\n",
      "580 번째 loss, accuracy:  1.0948914302650872 0.35833333333333334\n",
      "581 번째 loss, accuracy:  1.0948820080578907 0.35833333333333334\n",
      "582 번째 loss, accuracy:  1.0948725172090725 0.35833333333333334\n",
      "583 번째 loss, accuracy:  1.094862957950846 0.35833333333333334\n",
      "584 번째 loss, accuracy:  1.0948533262910383 0.35833333333333334\n",
      "585 번째 loss, accuracy:  1.0948436228988063 0.35833333333333334\n",
      "586 번째 loss, accuracy:  1.0948338463209408 0.35833333333333334\n",
      "587 번째 loss, accuracy:  1.0948239954150012 0.35833333333333334\n",
      "588 번째 loss, accuracy:  1.094814068557228 0.35833333333333334\n",
      "589 번째 loss, accuracy:  1.0948040672998574 0.35833333333333334\n",
      "590 번째 loss, accuracy:  1.0947939903393946 0.35833333333333334\n",
      "591 번째 loss, accuracy:  1.094783835227137 0.35833333333333334\n",
      "592 번째 loss, accuracy:  1.0947735994539618 0.35833333333333334\n",
      "593 번째 loss, accuracy:  1.0947632867392543 0.35833333333333334\n",
      "594 번째 loss, accuracy:  1.0947528912971285 0.35833333333333334\n",
      "595 번째 loss, accuracy:  1.0947424156677352 0.35833333333333334\n",
      "596 번째 loss, accuracy:  1.0947318597626743 0.35833333333333334\n",
      "597 번째 loss, accuracy:  1.0947212201768874 0.35833333333333334\n",
      "598 번째 loss, accuracy:  1.094710496931996 0.35833333333333334\n",
      "599 번째 loss, accuracy:  1.0946996903896713 0.35833333333333334\n",
      "600 번째 loss, accuracy:  1.094688794914963 0.35833333333333334\n",
      "601 번째 loss, accuracy:  1.0946778095036958 0.35833333333333334\n",
      "602 번째 loss, accuracy:  1.0946667389502225 0.35833333333333334\n",
      "603 번째 loss, accuracy:  1.0946555771471955 0.35833333333333334\n",
      "604 번째 loss, accuracy:  1.094644324761201 0.35833333333333334\n",
      "605 번째 loss, accuracy:  1.094632982456809 0.35833333333333334\n",
      "606 번째 loss, accuracy:  1.0946215459386521 0.35833333333333334\n",
      "607 번째 loss, accuracy:  1.094610015737111 0.35833333333333334\n",
      "608 번째 loss, accuracy:  1.0945983913872317 0.35833333333333334\n",
      "609 번째 loss, accuracy:  1.0945866663515316 0.35833333333333334\n",
      "610 번째 loss, accuracy:  1.0945748467608267 0.35833333333333334\n",
      "611 번째 loss, accuracy:  1.0945629324597335 0.35833333333333334\n",
      "612 번째 loss, accuracy:  1.0945509150228385 0.35833333333333334\n",
      "613 번째 loss, accuracy:  1.0945387945423735 0.35833333333333334\n",
      "614 번째 loss, accuracy:  1.0945265739092802 0.35833333333333334\n",
      "615 번째 loss, accuracy:  1.0945142478940117 0.35833333333333334\n",
      "616 번째 loss, accuracy:  1.094501816719626 0.35833333333333334\n",
      "617 번째 loss, accuracy:  1.0944892810039817 0.35833333333333334\n",
      "618 번째 loss, accuracy:  1.0944766383962257 0.35833333333333334\n",
      "619 번째 loss, accuracy:  1.094463885549878 0.35833333333333334\n",
      "620 번째 loss, accuracy:  1.0944510212426075 0.35833333333333334\n",
      "621 번째 loss, accuracy:  1.094438046229841 0.35833333333333334\n",
      "622 번째 loss, accuracy:  1.0944249595195612 0.35833333333333334\n",
      "623 번째 loss, accuracy:  1.0944117524805417 0.35833333333333334\n",
      "624 번째 loss, accuracy:  1.0943984308375534 0.35833333333333334\n",
      "625 번째 loss, accuracy:  1.0943849912650307 0.35833333333333334\n",
      "626 번째 loss, accuracy:  1.0943714268949205 0.35833333333333334\n",
      "627 번째 loss, accuracy:  1.0943577476723225 0.35833333333333334\n",
      "628 번째 loss, accuracy:  1.094343940804705 0.35833333333333334\n",
      "629 번째 loss, accuracy:  1.0943300103820477 0.35833333333333334\n",
      "630 번째 loss, accuracy:  1.094315956632734 0.35833333333333334\n",
      "631 번째 loss, accuracy:  1.0943017750547714 0.35833333333333334\n",
      "632 번째 loss, accuracy:  1.0942874635041284 0.35833333333333334\n",
      "633 번째 loss, accuracy:  1.0942730167876216 0.35833333333333334\n",
      "634 번째 loss, accuracy:  1.0942584401601987 0.35833333333333334\n",
      "635 번째 loss, accuracy:  1.0942437269598846 0.35833333333333334\n",
      "636 번째 loss, accuracy:  1.0942288787981795 0.35833333333333334\n",
      "637 번째 loss, accuracy:  1.0942138897981397 0.35833333333333334\n",
      "638 번째 loss, accuracy:  1.0941987601265908 0.35833333333333334\n",
      "639 번째 loss, accuracy:  1.094183485905073 0.35833333333333334\n",
      "640 번째 loss, accuracy:  1.0941680677405614 0.35833333333333334\n",
      "641 번째 loss, accuracy:  1.094152501453639 0.35833333333333334\n",
      "642 번째 loss, accuracy:  1.0941367868015737 0.35833333333333334\n",
      "643 번째 loss, accuracy:  1.0941209234140845 0.35833333333333334\n",
      "644 번째 loss, accuracy:  1.0941049089335602 0.35833333333333334\n",
      "645 번째 loss, accuracy:  1.09408873665739 0.35833333333333334\n",
      "646 번째 loss, accuracy:  1.0940724004246085 0.35833333333333334\n",
      "647 번째 loss, accuracy:  1.0940559076140546 0.35833333333333334\n",
      "648 번째 loss, accuracy:  1.0940392460490533 0.35833333333333334\n",
      "649 번째 loss, accuracy:  1.094022428286587 0.35833333333333334\n",
      "650 번째 loss, accuracy:  1.0940054416450593 0.35833333333333334\n",
      "651 번째 loss, accuracy:  1.093988285020049 0.35833333333333334\n",
      "652 번째 loss, accuracy:  1.0939709542149818 0.35833333333333334\n",
      "653 번째 loss, accuracy:  1.0939534437936371 0.35833333333333334\n",
      "654 번째 loss, accuracy:  1.0939357610384282 0.35833333333333334\n",
      "655 번째 loss, accuracy:  1.0939178947364658 0.35833333333333334\n",
      "656 번째 loss, accuracy:  1.0938998451794895 0.35833333333333334\n",
      "657 번째 loss, accuracy:  1.0938816079270783 0.35833333333333334\n",
      "658 번째 loss, accuracy:  1.0938631808094637 0.35833333333333334\n",
      "659 번째 loss, accuracy:  1.0938445631817761 0.35833333333333334\n",
      "660 번째 loss, accuracy:  1.0938257557248898 0.35833333333333334\n",
      "661 번째 loss, accuracy:  1.0938067444343602 0.35833333333333334\n",
      "662 번째 loss, accuracy:  1.093787534620941 0.35833333333333334\n",
      "663 번째 loss, accuracy:  1.0937681179116747 0.35833333333333334\n",
      "664 번째 loss, accuracy:  1.0937484975616187 0.35833333333333334\n",
      "665 번째 loss, accuracy:  1.0937286651069051 0.35833333333333334\n",
      "666 번째 loss, accuracy:  1.093708621063332 0.35833333333333334\n",
      "667 번째 loss, accuracy:  1.0936883606850183 0.35833333333333334\n",
      "668 번째 loss, accuracy:  1.0936678752004503 0.35833333333333334\n",
      "669 번째 loss, accuracy:  1.0936471741244163 0.35833333333333334\n",
      "670 번째 loss, accuracy:  1.0936262345999241 0.35833333333333334\n",
      "671 번째 loss, accuracy:  1.093605071789299 0.35833333333333334\n",
      "672 번째 loss, accuracy:  1.0935836670798438 0.35833333333333334\n",
      "673 번째 loss, accuracy:  1.0935620206998182 0.35833333333333334\n",
      "674 번째 loss, accuracy:  1.0935401296093596 0.35833333333333334\n",
      "675 번째 loss, accuracy:  1.09351799853705 0.35833333333333334\n",
      "676 번째 loss, accuracy:  1.0934956081130116 0.35833333333333334\n",
      "677 번째 loss, accuracy:  1.093472957582918 0.35833333333333334\n",
      "678 번째 loss, accuracy:  1.0934500468818045 0.35833333333333334\n",
      "679 번째 loss, accuracy:  1.0934268792321535 0.35833333333333334\n",
      "680 번째 loss, accuracy:  1.0934034402376531 0.35833333333333334\n",
      "681 번째 loss, accuracy:  1.0933797273529486 0.35833333333333334\n",
      "682 번째 loss, accuracy:  1.0933557343308145 0.35833333333333334\n",
      "683 번째 loss, accuracy:  1.0933314633713052 0.35833333333333334\n",
      "684 번째 loss, accuracy:  1.0933069067911783 0.35833333333333334\n",
      "685 번째 loss, accuracy:  1.0932820409989064 0.35833333333333334\n",
      "686 번째 loss, accuracy:  1.0932568859544796 0.35833333333333334\n",
      "687 번째 loss, accuracy:  1.0932314322955714 0.35833333333333334\n",
      "688 번째 loss, accuracy:  1.093205653723677 0.35833333333333334\n",
      "689 번째 loss, accuracy:  1.0931795722949493 0.35833333333333334\n",
      "690 번째 loss, accuracy:  1.093153169000587 0.35833333333333334\n",
      "691 번째 loss, accuracy:  1.093126440801697 0.35833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692 번째 loss, accuracy:  1.093099380036849 0.35833333333333334\n",
      "693 번째 loss, accuracy:  1.0930719817411165 0.35833333333333334\n",
      "694 번째 loss, accuracy:  1.093044227514806 0.35833333333333334\n",
      "695 번째 loss, accuracy:  1.0930161254004536 0.35833333333333334\n",
      "696 번째 loss, accuracy:  1.0929876606727187 0.35833333333333334\n",
      "697 번째 loss, accuracy:  1.0929588328274669 0.35833333333333334\n",
      "698 번째 loss, accuracy:  1.0929296399516928 0.35833333333333334\n",
      "699 번째 loss, accuracy:  1.092900059953132 0.35833333333333334\n",
      "700 번째 loss, accuracy:  1.092870100107262 0.35833333333333334\n",
      "701 번째 loss, accuracy:  1.0928397542936132 0.35833333333333334\n",
      "702 번째 loss, accuracy:  1.0928090105176753 0.36666666666666664\n",
      "703 번째 loss, accuracy:  1.0927778587669712 0.36666666666666664\n",
      "704 번째 loss, accuracy:  1.092746280998637 0.36666666666666664\n",
      "705 번째 loss, accuracy:  1.0927142834112966 0.36666666666666664\n",
      "706 번째 loss, accuracy:  1.0926818457813183 0.375\n",
      "707 번째 loss, accuracy:  1.0926489770594698 0.375\n",
      "708 번째 loss, accuracy:  1.0926156428456315 0.375\n",
      "709 번째 loss, accuracy:  1.0925818592679062 0.375\n",
      "710 번째 loss, accuracy:  1.0925476063789858 0.375\n",
      "711 번째 loss, accuracy:  1.0925128778720166 0.375\n",
      "712 번째 loss, accuracy:  1.0924776617191176 0.375\n",
      "713 번째 loss, accuracy:  1.09244194466764 0.375\n",
      "714 번째 loss, accuracy:  1.092405688138162 0.375\n",
      "715 번째 loss, accuracy:  1.0923689544919488 0.375\n",
      "716 번째 loss, accuracy:  1.09233167699473 0.375\n",
      "717 번째 loss, accuracy:  1.0922938647688405 0.375\n",
      "718 번째 loss, accuracy:  1.0922555049453382 0.375\n",
      "719 번째 loss, accuracy:  1.0922165587089494 0.375\n",
      "720 번째 loss, accuracy:  1.0921770518508633 0.375\n",
      "721 번째 loss, accuracy:  1.0921369571430053 0.375\n",
      "722 번째 loss, accuracy:  1.0920962600552413 0.375\n",
      "723 번째 loss, accuracy:  1.092054948538358 0.375\n",
      "724 번째 loss, accuracy:  1.0920129698253036 0.375\n",
      "725 번째 loss, accuracy:  1.091970381542193 0.375\n",
      "726 번째 loss, accuracy:  1.0919271328379139 0.375\n",
      "727 번째 loss, accuracy:  1.0918832046416422 0.375\n",
      "728 번째 loss, accuracy:  1.091838615665814 0.375\n",
      "729 번째 loss, accuracy:  1.0917933098329586 0.375\n",
      "730 번째 loss, accuracy:  1.091747292781997 0.375\n",
      "731 번째 loss, accuracy:  1.0917005418560812 0.375\n",
      "732 번째 loss, accuracy:  1.091653046579469 0.375\n",
      "733 번째 loss, accuracy:  1.0916047597636875 0.375\n",
      "734 번째 loss, accuracy:  1.0915556927986454 0.375\n",
      "735 번째 loss, accuracy:  1.0915058195655107 0.375\n",
      "736 번째 loss, accuracy:  1.0914551198640197 0.375\n",
      "737 번째 loss, accuracy:  1.0914035811085838 0.375\n",
      "738 번째 loss, accuracy:  1.0913511586295124 0.375\n",
      "739 번째 loss, accuracy:  1.09129783065615 0.375\n",
      "740 번째 loss, accuracy:  1.0912435993339962 0.375\n",
      "741 번째 loss, accuracy:  1.091188444917237 0.375\n",
      "742 번째 loss, accuracy:  1.0911323392620027 0.375\n",
      "743 번째 loss, accuracy:  1.0910752514628441 0.375\n",
      "744 번째 loss, accuracy:  1.0910171680194527 0.375\n",
      "745 번째 loss, accuracy:  1.0909580591748742 0.375\n",
      "746 번째 loss, accuracy:  1.0908978842717112 0.375\n",
      "747 번째 loss, accuracy:  1.0908366429067433 0.375\n",
      "748 번째 loss, accuracy:  1.0907742931575437 0.375\n",
      "749 번째 loss, accuracy:  1.0907108107515535 0.375\n",
      "750 번째 loss, accuracy:  1.0906461587603105 0.375\n",
      "751 번째 loss, accuracy:  1.0905803070329791 0.375\n",
      "752 번째 loss, accuracy:  1.0905132017494694 0.375\n",
      "753 번째 loss, accuracy:  1.0904448674272993 0.375\n",
      "754 번째 loss, accuracy:  1.0903752084514329 0.375\n",
      "755 번째 loss, accuracy:  1.0903041992076197 0.375\n",
      "756 번째 loss, accuracy:  1.0902318543391591 0.375\n",
      "757 번째 loss, accuracy:  1.0901580805948756 0.375\n",
      "758 번째 loss, accuracy:  1.0900828177825743 0.375\n",
      "759 번째 loss, accuracy:  1.090006086401695 0.375\n",
      "760 번째 loss, accuracy:  1.0899278342764096 0.375\n",
      "761 번째 loss, accuracy:  1.0898479629950661 0.375\n",
      "762 번째 loss, accuracy:  1.0897664909909561 0.375\n",
      "763 번째 loss, accuracy:  1.0896833441777267 0.375\n",
      "764 번째 loss, accuracy:  1.0895985270668225 0.375\n",
      "765 번째 loss, accuracy:  1.0895119216366926 0.375\n",
      "766 번째 loss, accuracy:  1.0894234927614121 0.375\n",
      "767 번째 loss, accuracy:  1.0893331571482396 0.375\n",
      "768 번째 loss, accuracy:  1.0892409393511604 0.375\n",
      "769 번째 loss, accuracy:  1.0891467126203815 0.375\n",
      "770 번째 loss, accuracy:  1.0890504534562804 0.38333333333333336\n",
      "771 번째 loss, accuracy:  1.0889521036691296 0.38333333333333336\n",
      "772 번째 loss, accuracy:  1.0888515229849318 0.38333333333333336\n",
      "773 번째 loss, accuracy:  1.0887486755910787 0.38333333333333336\n",
      "774 번째 loss, accuracy:  1.0886434913274574 0.38333333333333336\n",
      "775 번째 loss, accuracy:  1.0885359641915964 0.38333333333333336\n",
      "776 번째 loss, accuracy:  1.0884259892021924 0.39166666666666666\n",
      "777 번째 loss, accuracy:  1.0883134567165764 0.39166666666666666\n",
      "778 번째 loss, accuracy:  1.0881983003183264 0.39166666666666666\n",
      "779 번째 loss, accuracy:  1.088080325091594 0.39166666666666666\n",
      "780 번째 loss, accuracy:  1.087959590707779 0.39166666666666666\n",
      "781 번째 loss, accuracy:  1.0878359677285983 0.4\n",
      "782 번째 loss, accuracy:  1.0877092525442087 0.4166666666666667\n",
      "783 번째 loss, accuracy:  1.087579494987557 0.4166666666666667\n",
      "784 번째 loss, accuracy:  1.0874464878962324 0.4166666666666667\n",
      "785 번째 loss, accuracy:  1.0873102119058695 0.425\n",
      "786 번째 loss, accuracy:  1.0871704688936812 0.425\n",
      "787 번째 loss, accuracy:  1.0870271625043313 0.425\n",
      "788 번째 loss, accuracy:  1.0868800815254827 0.425\n",
      "789 번째 loss, accuracy:  1.0867292461111917 0.425\n",
      "790 번째 loss, accuracy:  1.0865744409302092 0.43333333333333335\n",
      "791 번째 loss, accuracy:  1.0864155957221806 0.43333333333333335\n",
      "792 번째 loss, accuracy:  1.0862524847691157 0.43333333333333335\n",
      "793 번째 loss, accuracy:  1.0860848570534902 0.43333333333333335\n",
      "794 번째 loss, accuracy:  1.0859127958294674 0.44166666666666665\n",
      "795 번째 loss, accuracy:  1.08573594047726 0.44166666666666665\n",
      "796 번째 loss, accuracy:  1.0855541122588181 0.45\n",
      "797 번째 loss, accuracy:  1.0853671732488974 0.4583333333333333\n",
      "798 번째 loss, accuracy:  1.0851749007649247 0.4583333333333333\n",
      "799 번째 loss, accuracy:  1.0849772554616115 0.4583333333333333\n",
      "800 번째 loss, accuracy:  1.0847738096135624 0.4583333333333333\n",
      "801 번째 loss, accuracy:  1.084564336514552 0.4583333333333333\n",
      "802 번째 loss, accuracy:  1.084348654842323 0.4583333333333333\n",
      "803 번째 loss, accuracy:  1.084126621696196 0.4583333333333333\n",
      "804 번째 loss, accuracy:  1.0838978135389674 0.4583333333333333\n",
      "805 번째 loss, accuracy:  1.083662087321205 0.4583333333333333\n",
      "806 번째 loss, accuracy:  1.0834191657350982 0.4583333333333333\n",
      "807 번째 loss, accuracy:  1.0831687229080713 0.4583333333333333\n",
      "808 번째 loss, accuracy:  1.082910234850399 0.4583333333333333\n",
      "809 번째 loss, accuracy:  1.0826436935366894 0.4583333333333333\n",
      "810 번째 loss, accuracy:  1.0823684164862784 0.4583333333333333\n",
      "811 번째 loss, accuracy:  1.0820843472012769 0.4583333333333333\n",
      "812 번째 loss, accuracy:  1.081791051530253 0.475\n",
      "813 번째 loss, accuracy:  1.0814879921289562 0.48333333333333334\n",
      "814 번째 loss, accuracy:  1.0811747842950234 0.5\n",
      "815 번째 loss, accuracy:  1.0808511561074425 0.5083333333333333\n",
      "816 번째 loss, accuracy:  1.0805167800200373 0.5083333333333333\n",
      "817 번째 loss, accuracy:  1.0801707472321604 0.5083333333333333\n",
      "818 번째 loss, accuracy:  1.0798129000358336 0.5083333333333333\n",
      "819 번째 loss, accuracy:  1.0794426095633254 0.5083333333333333\n",
      "820 번째 loss, accuracy:  1.0790589854691721 0.525\n",
      "821 번째 loss, accuracy:  1.0786618233328797 0.5333333333333333\n",
      "822 번째 loss, accuracy:  1.0782503661063996 0.5333333333333333\n",
      "823 번째 loss, accuracy:  1.0778245221074314 0.5333333333333333\n",
      "824 번째 loss, accuracy:  1.0773826611169153 0.55\n",
      "825 번째 loss, accuracy:  1.07692489719932 0.55\n",
      "826 번째 loss, accuracy:  1.0764503715993492 0.5583333333333333\n",
      "827 번째 loss, accuracy:  1.0759586743698786 0.5583333333333333\n",
      "828 번째 loss, accuracy:  1.0754483931466345 0.575\n",
      "829 번째 loss, accuracy:  1.0749189713871823 0.5916666666666667\n",
      "830 번째 loss, accuracy:  1.0743694910841315 0.6\n",
      "831 번째 loss, accuracy:  1.0737987967156972 0.6\n",
      "832 번째 loss, accuracy:  1.0732066080487597 0.6\n",
      "833 번째 loss, accuracy:  1.0725921998873378 0.6\n",
      "834 번째 loss, accuracy:  1.0719539076835116 0.6166666666666667\n",
      "835 번째 loss, accuracy:  1.0712916353901591 0.6166666666666667\n",
      "836 번째 loss, accuracy:  1.07060354396246 0.6166666666666667\n",
      "837 번째 loss, accuracy:  1.0698900176826223 0.625\n",
      "838 번째 loss, accuracy:  1.0691485859629433 0.6333333333333333\n",
      "839 번째 loss, accuracy:  1.0683786837410696 0.6416666666666667\n",
      "840 번째 loss, accuracy:  1.0675793967941012 0.6416666666666667\n",
      "841 번째 loss, accuracy:  1.0667502518771654 0.6416666666666667\n",
      "842 번째 loss, accuracy:  1.065889509702559 0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843 번째 loss, accuracy:  1.0649965095528697 0.6583333333333333\n",
      "844 번째 loss, accuracy:  1.064071003738501 0.675\n",
      "845 번째 loss, accuracy:  1.063111258430538 0.675\n",
      "846 번째 loss, accuracy:  1.0621170881225217 0.675\n",
      "847 번째 loss, accuracy:  1.0610873706083792 0.675\n",
      "848 번째 loss, accuracy:  1.0600217998947923 0.6833333333333333\n",
      "849 번째 loss, accuracy:  1.0589193906895817 0.6916666666666667\n",
      "850 번째 loss, accuracy:  1.057781242719733 0.6916666666666667\n",
      "851 번째 loss, accuracy:  1.056605273678026 0.6916666666666667\n",
      "852 번째 loss, accuracy:  1.0553928136983608 0.6916666666666667\n",
      "853 번째 loss, accuracy:  1.0541427005152237 0.6916666666666667\n",
      "854 번째 loss, accuracy:  1.0528568082628211 0.6916666666666667\n",
      "855 번째 loss, accuracy:  1.051535576503237 0.6916666666666667\n",
      "856 번째 loss, accuracy:  1.0501789200823972 0.6916666666666667\n",
      "857 번째 loss, accuracy:  1.0487881122773124 0.6916666666666667\n",
      "858 번째 loss, accuracy:  1.0473646554427527 0.6916666666666667\n",
      "859 번째 loss, accuracy:  1.0459082684899461 0.6916666666666667\n",
      "860 번째 loss, accuracy:  1.0444230394690484 0.6916666666666667\n",
      "861 번째 loss, accuracy:  1.042909948892397 0.6916666666666667\n",
      "862 번째 loss, accuracy:  1.0413695542067158 0.6916666666666667\n",
      "863 번째 loss, accuracy:  1.0398071620460625 0.6916666666666667\n",
      "864 번째 loss, accuracy:  1.0382238868998224 0.6916666666666667\n",
      "865 번째 loss, accuracy:  1.036620699091109 0.6916666666666667\n",
      "866 번째 loss, accuracy:  1.0350036145536896 0.6916666666666667\n",
      "867 번째 loss, accuracy:  1.0333731284686172 0.6916666666666667\n",
      "868 번째 loss, accuracy:  1.0317319919702916 0.6916666666666667\n",
      "869 번째 loss, accuracy:  1.0300854165397872 0.6916666666666667\n",
      "870 번째 loss, accuracy:  1.028435378398817 0.6916666666666667\n",
      "871 번째 loss, accuracy:  1.026783959851861 0.6916666666666667\n",
      "872 번째 loss, accuracy:  1.0251355024264437 0.6916666666666667\n",
      "873 번째 loss, accuracy:  1.0234899892214897 0.6916666666666667\n",
      "874 번째 loss, accuracy:  1.0218521264247515 0.6916666666666667\n",
      "875 번째 loss, accuracy:  1.0202259029671832 0.6916666666666667\n",
      "876 번째 loss, accuracy:  1.018611528378188 0.6916666666666667\n",
      "877 번째 loss, accuracy:  1.0170102159558683 0.6916666666666667\n",
      "878 번째 loss, accuracy:  1.0154269846603248 0.6916666666666667\n",
      "879 번째 loss, accuracy:  1.0138598836987247 0.6916666666666667\n",
      "880 번째 loss, accuracy:  1.012313131551001 0.6916666666666667\n",
      "881 번째 loss, accuracy:  1.0107871374395876 0.6916666666666667\n",
      "882 번째 loss, accuracy:  1.0092821904547444 0.6916666666666667\n",
      "883 번째 loss, accuracy:  1.0077997694396927 0.6916666666666667\n",
      "884 번째 loss, accuracy:  1.006340184995048 0.6916666666666667\n",
      "885 번째 loss, accuracy:  1.0049049351446506 0.6916666666666667\n",
      "886 번째 loss, accuracy:  1.0034925073938132 0.6916666666666667\n",
      "887 번째 loss, accuracy:  1.0021043672913625 0.6916666666666667\n",
      "888 번째 loss, accuracy:  1.0007396172509715 0.6916666666666667\n",
      "889 번째 loss, accuracy:  0.9994003177160102 0.6916666666666667\n",
      "890 번째 loss, accuracy:  0.9980832056622636 0.6916666666666667\n",
      "891 번째 loss, accuracy:  0.9967893682086378 0.6916666666666667\n",
      "892 번째 loss, accuracy:  0.9955183187732376 0.6916666666666667\n",
      "893 번째 loss, accuracy:  0.9942705992898397 0.6916666666666667\n",
      "894 번째 loss, accuracy:  0.9930420521096539 0.6916666666666667\n",
      "895 번째 loss, accuracy:  0.9918356191585266 0.6916666666666667\n",
      "896 번째 loss, accuracy:  0.9906513690975148 0.6916666666666667\n",
      "897 번째 loss, accuracy:  0.9894873866977302 0.6916666666666667\n",
      "898 번째 loss, accuracy:  0.9883422490548147 0.6916666666666667\n",
      "899 번째 loss, accuracy:  0.9872157005102872 0.6916666666666667\n",
      "900 번째 loss, accuracy:  0.9861066859060662 0.6916666666666667\n",
      "901 번째 loss, accuracy:  0.9850162193995552 0.6916666666666667\n",
      "902 번째 loss, accuracy:  0.9839423211453086 0.6916666666666667\n",
      "903 번째 loss, accuracy:  0.9828850590811091 0.6916666666666667\n",
      "904 번째 loss, accuracy:  0.981842238927451 0.6916666666666667\n",
      "905 번째 loss, accuracy:  0.9808154907977029 0.6916666666666667\n",
      "906 번째 loss, accuracy:  0.9798028778790664 0.6916666666666667\n",
      "907 번째 loss, accuracy:  0.9788045298652743 0.6916666666666667\n",
      "908 번째 loss, accuracy:  0.9778189490560958 0.6916666666666667\n",
      "909 번째 loss, accuracy:  0.9768460675149064 0.6916666666666667\n",
      "910 번째 loss, accuracy:  0.9758857608927374 0.6916666666666667\n",
      "911 번째 loss, accuracy:  0.9749373361352809 0.6916666666666667\n",
      "912 번째 loss, accuracy:  0.9740001459674263 0.6916666666666667\n",
      "913 번째 loss, accuracy:  0.9730732252031162 0.6916666666666667\n",
      "914 번째 loss, accuracy:  0.9721567879185059 0.6916666666666667\n",
      "915 번째 loss, accuracy:  0.9712490235791225 0.6916666666666667\n",
      "916 번째 loss, accuracy:  0.9703513206816463 0.6916666666666667\n",
      "917 번째 loss, accuracy:  0.9694625460234719 0.6916666666666667\n",
      "918 번째 loss, accuracy:  0.9685824741992639 0.6916666666666667\n",
      "919 번째 loss, accuracy:  0.9677110278325621 0.6916666666666667\n",
      "920 번째 loss, accuracy:  0.9668474251646202 0.6916666666666667\n",
      "921 번째 loss, accuracy:  0.965991745790354 0.6916666666666667\n",
      "922 번째 loss, accuracy:  0.9651432619407729 0.6916666666666667\n",
      "923 번째 loss, accuracy:  0.9643018141893566 0.6916666666666667\n",
      "924 번째 loss, accuracy:  0.9634667965744614 0.6916666666666667\n",
      "925 번째 loss, accuracy:  0.9626386219015195 0.6916666666666667\n",
      "926 번째 loss, accuracy:  0.9618162849116809 0.6916666666666667\n",
      "927 번째 loss, accuracy:  0.9610003301963322 0.6916666666666667\n",
      "928 번째 loss, accuracy:  0.960190031266792 0.6916666666666667\n",
      "929 번째 loss, accuracy:  0.9593852467688017 0.6916666666666667\n",
      "930 번째 loss, accuracy:  0.9585857900576287 0.6916666666666667\n",
      "931 번째 loss, accuracy:  0.957790998565582 0.6916666666666667\n",
      "932 번째 loss, accuracy:  0.9570015704675308 0.6916666666666667\n",
      "933 번째 loss, accuracy:  0.9562169923465625 0.6916666666666667\n",
      "934 번째 loss, accuracy:  0.9554368256940549 0.6916666666666667\n",
      "935 번째 loss, accuracy:  0.9546614151312094 0.6916666666666667\n",
      "936 번째 loss, accuracy:  0.9538899722272105 0.6916666666666667\n",
      "937 번째 loss, accuracy:  0.9531229428376844 0.6916666666666667\n",
      "938 번째 loss, accuracy:  0.9523598640086538 0.6916666666666667\n",
      "939 번째 loss, accuracy:  0.9516007055204572 0.6916666666666667\n",
      "940 번째 loss, accuracy:  0.9508452799673253 0.6916666666666667\n",
      "941 번째 loss, accuracy:  0.9500934904317572 0.6916666666666667\n",
      "942 번째 loss, accuracy:  0.9493451740529693 0.6916666666666667\n",
      "943 번째 loss, accuracy:  0.9486003724163825 0.6916666666666667\n",
      "944 번째 loss, accuracy:  0.9478587342783524 0.6916666666666667\n",
      "945 번째 loss, accuracy:  0.947120520198862 0.6916666666666667\n",
      "946 번째 loss, accuracy:  0.9463854237800895 0.6916666666666667\n",
      "947 번째 loss, accuracy:  0.945653197671699 0.6916666666666667\n",
      "948 번째 loss, accuracy:  0.9449240922923058 0.6916666666666667\n",
      "949 번째 loss, accuracy:  0.9441979145125787 0.6916666666666667\n",
      "950 번째 loss, accuracy:  0.9434745085463031 0.6916666666666667\n",
      "951 번째 loss, accuracy:  0.9427538002433244 0.6916666666666667\n",
      "952 번째 loss, accuracy:  0.9420357044106495 0.6916666666666667\n",
      "953 번째 loss, accuracy:  0.9413202835592978 0.6916666666666667\n",
      "954 번째 loss, accuracy:  0.9406072856352414 0.6916666666666667\n",
      "955 번째 loss, accuracy:  0.9398968236426616 0.6916666666666667\n",
      "956 번째 loss, accuracy:  0.9391887851009338 0.6916666666666667\n",
      "957 번째 loss, accuracy:  0.9384830950760104 0.6916666666666667\n",
      "958 번째 loss, accuracy:  0.9377796800563966 0.6916666666666667\n",
      "959 번째 loss, accuracy:  0.9370784687479912 0.6916666666666667\n",
      "960 번째 loss, accuracy:  0.9363793453975172 0.6916666666666667\n",
      "961 번째 loss, accuracy:  0.935682468078692 0.6916666666666667\n",
      "962 번째 loss, accuracy:  0.9349877371344697 0.6916666666666667\n",
      "963 번째 loss, accuracy:  0.934295056659801 0.6916666666666667\n",
      "964 번째 loss, accuracy:  0.9336044591301459 0.6916666666666667\n",
      "965 번째 loss, accuracy:  0.9329158159187432 0.6916666666666667\n",
      "966 번째 loss, accuracy:  0.9322290959909908 0.6916666666666667\n",
      "967 번째 loss, accuracy:  0.9315443589634456 0.6916666666666667\n",
      "968 번째 loss, accuracy:  0.930861383563847 0.6916666666666667\n",
      "969 번째 loss, accuracy:  0.9301803194853621 0.6916666666666667\n",
      "970 번째 loss, accuracy:  0.9295009854266294 0.6916666666666667\n",
      "971 번째 loss, accuracy:  0.9288234858661607 0.6916666666666667\n",
      "972 번째 loss, accuracy:  0.9281477377088732 0.6916666666666667\n",
      "973 번째 loss, accuracy:  0.9274737832560064 0.6916666666666667\n",
      "974 번째 loss, accuracy:  0.9268013703377604 0.6916666666666667\n",
      "975 번째 loss, accuracy:  0.926130666599779 0.6916666666666667\n",
      "976 번째 loss, accuracy:  0.9254615441990381 0.6916666666666667\n",
      "977 번째 loss, accuracy:  0.9247941680871079 0.6916666666666667\n",
      "978 번째 loss, accuracy:  0.9241282375931864 0.6916666666666667\n",
      "979 번째 loss, accuracy:  0.9234638767374387 0.6916666666666667\n",
      "980 번째 loss, accuracy:  0.92280118674257 0.6916666666666667\n",
      "981 번째 loss, accuracy:  0.9221399134117798 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "982 번째 loss, accuracy:  0.921480131335035 0.6916666666666667\n",
      "983 번째 loss, accuracy:  0.9208218620776435 0.6916666666666667\n",
      "984 번째 loss, accuracy:  0.9201650407636282 0.6916666666666667\n",
      "985 번째 loss, accuracy:  0.9195097171067651 0.6916666666666667\n",
      "986 번째 loss, accuracy:  0.9188557460200094 0.6916666666666667\n",
      "987 번째 loss, accuracy:  0.9182030961922476 0.6916666666666667\n",
      "988 번째 loss, accuracy:  0.917551888100973 0.6916666666666667\n",
      "989 번째 loss, accuracy:  0.9169020617654835 0.6916666666666667\n",
      "990 번째 loss, accuracy:  0.9162536667541014 0.6916666666666667\n",
      "991 번째 loss, accuracy:  0.915606418181849 0.6916666666666667\n",
      "992 번째 loss, accuracy:  0.9149608004808554 0.6916666666666667\n",
      "993 번째 loss, accuracy:  0.9143164036937148 0.6916666666666667\n",
      "994 번째 loss, accuracy:  0.9136732991873404 0.6916666666666667\n",
      "995 번째 loss, accuracy:  0.9130314065793871 0.6916666666666667\n",
      "996 번째 loss, accuracy:  0.9123910055107919 0.6916666666666667\n",
      "997 번째 loss, accuracy:  0.9117517265245331 0.6916666666666667\n",
      "998 번째 loss, accuracy:  0.9111135832189554 0.6916666666666667\n",
      "999 번째 loss, accuracy:  0.9104767914412338 0.6916666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 loss, accuracy:  1.3093753026119577 0.35833333333333334\n",
      "1 번째 loss, accuracy:  1.3062603137896467 0.35833333333333334\n",
      "2 번째 loss, accuracy:  1.3031737071682234 0.35833333333333334\n",
      "3 번째 loss, accuracy:  1.3001155175778336 0.35833333333333334\n",
      "4 번째 loss, accuracy:  1.2970857760738645 0.35833333333333334\n",
      "5 번째 loss, accuracy:  1.2940845098418157 0.35833333333333334\n",
      "6 번째 loss, accuracy:  1.2911117421071447 0.35833333333333334\n",
      "7 번째 loss, accuracy:  1.2881674920498143 0.35833333333333334\n",
      "8 번째 loss, accuracy:  1.285251774723802 0.35833333333333334\n",
      "9 번째 loss, accuracy:  1.2823646009815122 0.35833333333333334\n",
      "10 번째 loss, accuracy:  1.2795059774030642 0.35833333333333334\n",
      "11 번째 loss, accuracy:  1.2766759062304982 0.35833333333333334\n",
      "12 번째 loss, accuracy:  1.2738743853068797 0.35833333333333334\n",
      "13 번째 loss, accuracy:  1.2711014080203775 0.35833333333333334\n",
      "14 번째 loss, accuracy:  1.268356963253194 0.35833333333333334\n",
      "15 번째 loss, accuracy:  1.2656410353354237 0.35833333333333334\n",
      "16 번째 loss, accuracy:  1.2629536040038252 0.35833333333333334\n",
      "17 번째 loss, accuracy:  1.260294644365497 0.35833333333333334\n",
      "18 번째 loss, accuracy:  1.25766412686641 0.35833333333333334\n",
      "19 번째 loss, accuracy:  1.2550620172647484 0.35833333333333334\n",
      "20 번째 loss, accuracy:  1.2524882766092038 0.35833333333333334\n",
      "21 번째 loss, accuracy:  1.2499428612219141 0.35833333333333334\n",
      "22 번째 loss, accuracy:  1.247425722686288 0.35833333333333334\n",
      "23 번째 loss, accuracy:  1.2449368078394896 0.35833333333333334\n",
      "24 번째 loss, accuracy:  1.2424760587696397 0.35833333333333334\n",
      "25 번째 loss, accuracy:  1.2400434128176414 0.35833333333333334\n",
      "26 번째 loss, accuracy:  1.2376388025835752 0.35833333333333334\n",
      "27 번째 loss, accuracy:  1.235262155937675 0.35833333333333334\n",
      "28 번째 loss, accuracy:  1.2329133960356964 0.35833333333333334\n",
      "29 번째 loss, accuracy:  1.2305924413387572 0.35833333333333334\n",
      "30 번째 loss, accuracy:  1.228299205637441 0.35833333333333334\n",
      "31 번째 loss, accuracy:  1.2260335980802384 0.35833333333333334\n",
      "32 번째 loss, accuracy:  1.2237955232060613 0.35833333333333334\n",
      "33 번째 loss, accuracy:  1.2215848809809313 0.35833333333333334\n",
      "34 번째 loss, accuracy:  1.2194015668386498 0.35833333333333334\n",
      "35 번째 loss, accuracy:  1.2172454717253416 0.35833333333333334\n",
      "36 번째 loss, accuracy:  1.2151164821478595 0.35833333333333334\n",
      "37 번째 loss, accuracy:  1.2130144802258978 0.35833333333333334\n",
      "38 번째 loss, accuracy:  1.2109393437476854 0.35833333333333334\n",
      "39 번째 loss, accuracy:  1.2088909462292066 0.35833333333333334\n",
      "40 번째 loss, accuracy:  1.2068691569768144 0.35833333333333334\n",
      "41 번째 loss, accuracy:  1.2048738411531044 0.35833333333333334\n",
      "42 번째 loss, accuracy:  1.2029048598459855 0.35833333333333334\n",
      "43 번째 loss, accuracy:  1.200962070140729 0.35833333333333334\n",
      "44 번째 loss, accuracy:  1.199045325194996 0.35833333333333334\n",
      "45 번째 loss, accuracy:  1.197154474316668 0.35833333333333334\n",
      "46 번째 loss, accuracy:  1.1952893630442518 0.35833333333333334\n",
      "47 번째 loss, accuracy:  1.1934498332299224 0.35833333333333334\n",
      "48 번째 loss, accuracy:  1.1916357231249748 0.35833333333333334\n",
      "49 번째 loss, accuracy:  1.1898468674674243 0.35833333333333334\n",
      "50 번째 loss, accuracy:  1.1880830975719296 0.35833333333333334\n",
      "51 번째 loss, accuracy:  1.1863442414215941 0.35833333333333334\n",
      "52 번째 loss, accuracy:  1.1846301237616934 0.35833333333333334\n",
      "53 번째 loss, accuracy:  1.182940566195126 0.35833333333333334\n",
      "54 번째 loss, accuracy:  1.1812753872794992 0.35833333333333334\n",
      "55 번째 loss, accuracy:  1.1796344026256458 0.35833333333333334\n",
      "56 번째 loss, accuracy:  1.1780174249974806 0.35833333333333334\n",
      "57 번째 loss, accuracy:  1.1764242644130982 0.35833333333333334\n",
      "58 번째 loss, accuracy:  1.1748547282468589 0.35833333333333334\n",
      "59 번째 loss, accuracy:  1.1733086213325017 0.35833333333333334\n",
      "60 번째 loss, accuracy:  1.1717857460669827 0.35833333333333334\n",
      "61 번째 loss, accuracy:  1.1702859025149919 0.35833333333333334\n",
      "62 번째 loss, accuracy:  1.1688088885140502 0.35833333333333334\n",
      "63 번째 loss, accuracy:  1.1673544997799932 0.35833333333333334\n",
      "64 번째 loss, accuracy:  1.165922530012747 0.35833333333333334\n",
      "65 번째 loss, accuracy:  1.164512771002262 0.35833333333333334\n",
      "66 번째 loss, accuracy:  1.1631250127345147 0.35833333333333334\n",
      "67 번째 loss, accuracy:  1.1617590434974001 0.35833333333333334\n",
      "68 번째 loss, accuracy:  1.1604146499864645 0.35833333333333334\n",
      "69 번째 loss, accuracy:  1.1590916174103325 0.35833333333333334\n",
      "70 번째 loss, accuracy:  1.1577897295956847 0.35833333333333334\n",
      "71 번째 loss, accuracy:  1.1565087690917943 0.35833333333333334\n",
      "72 번째 loss, accuracy:  1.1552485172743432 0.35833333333333334\n",
      "73 번째 loss, accuracy:  1.1540087544485755 0.35833333333333334\n",
      "74 번째 loss, accuracy:  1.1527892599516358 0.35833333333333334\n",
      "75 번째 loss, accuracy:  1.1515898122539443 0.35833333333333334\n",
      "76 번째 loss, accuracy:  1.1504101890596194 0.35833333333333334\n",
      "77 번째 loss, accuracy:  1.1492501674057762 0.35833333333333334\n",
      "78 번째 loss, accuracy:  1.1481095237606398 0.35833333333333334\n",
      "79 번째 loss, accuracy:  1.1469880341204572 0.35833333333333334\n",
      "80 번째 loss, accuracy:  1.1458854741050226 0.35833333333333334\n",
      "81 번째 loss, accuracy:  1.1448016190518382 0.35833333333333334\n",
      "82 번째 loss, accuracy:  1.143736244108786 0.35833333333333334\n",
      "83 번째 loss, accuracy:  1.1426891243252701 0.35833333333333334\n",
      "84 번째 loss, accuracy:  1.1416600347417902 0.35833333333333334\n",
      "85 번째 loss, accuracy:  1.1406487504778318 0.35833333333333334\n",
      "86 번째 loss, accuracy:  1.1396550468180549 0.35833333333333334\n",
      "87 번째 loss, accuracy:  1.1386786992967717 0.35833333333333334\n",
      "88 번째 loss, accuracy:  1.1377194837806017 0.35833333333333334\n",
      "89 번째 loss, accuracy:  1.1367771765492527 0.35833333333333334\n",
      "90 번째 loss, accuracy:  1.1358515543745136 0.35833333333333334\n",
      "91 번째 loss, accuracy:  1.134942394597246 0.35833333333333334\n",
      "92 번째 loss, accuracy:  1.1340494752025194 0.35833333333333334\n",
      "93 번째 loss, accuracy:  1.1331725748927004 0.35833333333333334\n",
      "94 번째 loss, accuracy:  1.1323114731586061 0.35833333333333334\n",
      "95 번째 loss, accuracy:  1.1314659503486513 0.35833333333333334\n",
      "96 번째 loss, accuracy:  1.1306357877359126 0.35833333333333334\n",
      "97 번째 loss, accuracy:  1.1298207675832146 0.35833333333333334\n",
      "98 번째 loss, accuracy:  1.1290206732061256 0.35833333333333334\n",
      "99 번째 loss, accuracy:  1.128235289033913 0.35833333333333334\n",
      "100 번째 loss, accuracy:  1.1274644006684222 0.35833333333333334\n",
      "101 번째 loss, accuracy:  1.1267077949408948 0.35833333333333334\n",
      "102 번째 loss, accuracy:  1.1259652599667447 0.35833333333333334\n",
      "103 번째 loss, accuracy:  1.1252365851982116 0.35833333333333334\n",
      "104 번째 loss, accuracy:  1.124521561475041 0.35833333333333334\n",
      "105 번째 loss, accuracy:  1.1238199810730607 0.35833333333333334\n",
      "106 번째 loss, accuracy:  1.1231316377507319 0.35833333333333334\n",
      "107 번째 loss, accuracy:  1.1224563267936996 0.35833333333333334\n",
      "108 번째 loss, accuracy:  1.1217938450573135 0.35833333333333334\n",
      "109 번째 loss, accuracy:  1.1211439910071848 0.35833333333333334\n",
      "110 번째 loss, accuracy:  1.1205065647577386 0.35833333333333334\n",
      "111 번째 loss, accuracy:  1.1198813681088347 0.35833333333333334\n",
      "112 번째 loss, accuracy:  1.119268204580497 0.35833333333333334\n",
      "113 번째 loss, accuracy:  1.1186668794456789 0.35833333333333334\n",
      "114 번째 loss, accuracy:  1.1180771997612022 0.35833333333333334\n",
      "115 번째 loss, accuracy:  1.1174989743968253 0.35833333333333334\n",
      "116 번째 loss, accuracy:  1.1169320140624994 0.35833333333333334\n",
      "117 번째 loss, accuracy:  1.1163761313338385 0.35833333333333334\n",
      "118 번째 loss, accuracy:  1.1158311406758306 0.35833333333333334\n",
      "119 번째 loss, accuracy:  1.1152968584648124 0.35833333333333334\n",
      "120 번째 loss, accuracy:  1.1147731030087802 0.35833333333333334\n",
      "121 번째 loss, accuracy:  1.1142596945660075 0.35833333333333334\n",
      "122 번째 loss, accuracy:  1.113756455362065 0.35833333333333334\n",
      "123 번째 loss, accuracy:  1.1132632096052446 0.35833333333333334\n",
      "124 번째 loss, accuracy:  1.1127797835004267 0.35833333333333334\n",
      "125 번째 loss, accuracy:  1.1123060052614389 0.35833333333333334\n",
      "126 번째 loss, accuracy:  1.111841705121928 0.35833333333333334\n",
      "127 번째 loss, accuracy:  1.1113867153448107 0.35833333333333334\n",
      "128 번째 loss, accuracy:  1.1109408702302979 0.35833333333333334\n",
      "129 번째 loss, accuracy:  1.110504006122573 0.35833333333333334\n",
      "130 번째 loss, accuracy:  1.1100759614151248 0.35833333333333334\n",
      "131 번째 loss, accuracy:  1.109656576554802 0.35833333333333334\n",
      "132 번째 loss, accuracy:  1.1092456940446236 0.35833333333333334\n",
      "133 번째 loss, accuracy:  1.1088431584453657 0.35833333333333334\n",
      "134 번째 loss, accuracy:  1.1084488163759785 0.35833333333333334\n",
      "135 번째 loss, accuracy:  1.1080625165128708 0.35833333333333334\n",
      "136 번째 loss, accuracy:  1.1076841095880936 0.35833333333333334\n",
      "137 번째 loss, accuracy:  1.1073134483864566 0.35833333333333334\n",
      "138 번째 loss, accuracy:  1.1069503877416287 0.35833333333333334\n",
      "139 번째 loss, accuracy:  1.1065947845312547 0.35833333333333334\n",
      "140 번째 loss, accuracy:  1.1062464976711273 0.35833333333333334\n",
      "141 번째 loss, accuracy:  1.1059053881084278 0.35833333333333334\n",
      "142 번째 loss, accuracy:  1.1055713188141056 0.35833333333333334\n",
      "143 번째 loss, accuracy:  1.1052441547744072 0.35833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 번째 loss, accuracy:  1.1049237629816122 0.35833333333333334\n",
      "145 번째 loss, accuracy:  1.1046100124239828 0.35833333333333334\n",
      "146 번째 loss, accuracy:  1.1043027740749845 0.35833333333333334\n",
      "147 번째 loss, accuracy:  1.104001920881797 0.35833333333333334\n",
      "148 번째 loss, accuracy:  1.1037073277531604 0.35833333333333334\n",
      "149 번째 loss, accuracy:  1.1034188715465774 0.35833333333333334\n",
      "150 번째 loss, accuracy:  1.1031364310548988 0.35833333333333334\n",
      "151 번째 loss, accuracy:  1.1028598869923607 0.35833333333333334\n",
      "152 번째 loss, accuracy:  1.1025891219800328 0.35833333333333334\n",
      "153 번째 loss, accuracy:  1.1023240205308027 0.35833333333333334\n",
      "154 번째 loss, accuracy:  1.1020644690338215 0.35833333333333334\n",
      "155 번째 loss, accuracy:  1.1018103557385297 0.35833333333333334\n",
      "156 번째 loss, accuracy:  1.10156157073822 0.35833333333333334\n",
      "157 번째 loss, accuracy:  1.1013180059532308 0.35833333333333334\n",
      "158 번째 loss, accuracy:  1.1010795551137254 0.35833333333333334\n",
      "159 번째 loss, accuracy:  1.100846113742151 0.35833333333333334\n",
      "160 번째 loss, accuracy:  1.1006175791353419 0.35833333333333334\n",
      "161 번째 loss, accuracy:  1.1003938503463522 0.35833333333333334\n",
      "162 번째 loss, accuracy:  1.100174828165976 0.35833333333333334\n",
      "163 번째 loss, accuracy:  1.099960415104035 0.35833333333333334\n",
      "164 번째 loss, accuracy:  1.099750515370421 0.35833333333333334\n",
      "165 번째 loss, accuracy:  1.099545034855933 0.35833333333333334\n",
      "166 번째 loss, accuracy:  1.0993438811129068 0.35833333333333334\n",
      "167 번째 loss, accuracy:  1.0991469633357023 0.35833333333333334\n",
      "168 번째 loss, accuracy:  1.0989541923410038 0.35833333333333334\n",
      "169 번째 loss, accuracy:  1.0987654805480214 0.35833333333333334\n",
      "170 번째 loss, accuracy:  1.0985807419585463 0.35833333333333334\n",
      "171 번째 loss, accuracy:  1.0983998921369345 0.35833333333333334\n",
      "172 번째 loss, accuracy:  1.098222848189996 0.35833333333333334\n",
      "173 번째 loss, accuracy:  1.0980495287468197 0.35833333333333334\n",
      "174 번째 loss, accuracy:  1.0978798539385626 0.35833333333333334\n",
      "175 번째 loss, accuracy:  1.0977137453781898 0.35833333333333334\n",
      "176 번째 loss, accuracy:  1.0975511261402047 0.35833333333333334\n",
      "177 번째 loss, accuracy:  1.097391920740381 0.35833333333333334\n",
      "178 번째 loss, accuracy:  1.0972360551154852 0.35833333333333334\n",
      "179 번째 loss, accuracy:  1.097083456603037 0.35833333333333334\n",
      "180 번째 loss, accuracy:  1.0969340539210972 0.35833333333333334\n",
      "181 번째 loss, accuracy:  1.0967877771481 0.35833333333333334\n",
      "182 번째 loss, accuracy:  1.0966445577027422 0.35833333333333334\n",
      "183 번째 loss, accuracy:  1.0965043283239502 0.35833333333333334\n",
      "184 번째 loss, accuracy:  1.0963670230509015 0.35833333333333334\n",
      "185 번째 loss, accuracy:  1.0962325772031571 0.35833333333333334\n",
      "186 번째 loss, accuracy:  1.0961009273608757 0.35833333333333334\n",
      "187 번째 loss, accuracy:  1.0959720113451383 0.35833333333333334\n",
      "188 번째 loss, accuracy:  1.0958457681983877 0.35833333333333334\n",
      "189 번째 loss, accuracy:  1.0957221381649844 0.35833333333333334\n",
      "190 번째 loss, accuracy:  1.0956010626718917 0.35833333333333334\n",
      "191 번째 loss, accuracy:  1.0954824843094995 0.35833333333333334\n",
      "192 번째 loss, accuracy:  1.0953663468125905 0.35833333333333334\n",
      "193 번째 loss, accuracy:  1.0952525950414553 0.35833333333333334\n",
      "194 번째 loss, accuracy:  1.0951411749631474 0.35833333333333334\n",
      "195 번째 loss, accuracy:  1.0950320336329271 0.35833333333333334\n",
      "196 번째 loss, accuracy:  1.094925119175836 0.35833333333333334\n",
      "197 번째 loss, accuracy:  1.094820380768469 0.35833333333333334\n",
      "198 번째 loss, accuracy:  1.0947177686209064 0.35833333333333334\n",
      "199 번째 loss, accuracy:  1.0946172339588212 0.35833333333333334\n",
      "200 번째 loss, accuracy:  1.0945187290057772 0.35833333333333334\n",
      "201 번째 loss, accuracy:  1.094422206965704 0.35833333333333334\n",
      "202 번째 loss, accuracy:  1.0943276220055709 0.35833333333333334\n",
      "203 번째 loss, accuracy:  1.094234929238238 0.35833333333333334\n",
      "204 번째 loss, accuracy:  1.0941440847055115 0.35833333333333334\n",
      "205 번째 loss, accuracy:  1.0940550453613915 0.35833333333333334\n",
      "206 번째 loss, accuracy:  1.0939677690555172 0.35833333333333334\n",
      "207 번째 loss, accuracy:  1.0938822145168177 0.35833333333333334\n",
      "208 번째 loss, accuracy:  1.0937983413373507 0.35833333333333334\n",
      "209 번째 loss, accuracy:  1.0937161099563648 0.35833333333333334\n",
      "210 번째 loss, accuracy:  1.0936354816445484 0.35833333333333334\n",
      "211 번째 loss, accuracy:  1.0935564184884927 0.35833333333333334\n",
      "212 번째 loss, accuracy:  1.0934788833753557 0.35833333333333334\n",
      "213 번째 loss, accuracy:  1.0934028399777431 0.35833333333333334\n",
      "214 번째 loss, accuracy:  1.0933282527387822 0.35833333333333334\n",
      "215 번째 loss, accuracy:  1.0932550868574153 0.35833333333333334\n",
      "216 번째 loss, accuracy:  1.0931833082738929 0.35833333333333334\n",
      "217 번째 loss, accuracy:  1.0931128836554804 0.35833333333333334\n",
      "218 번째 loss, accuracy:  1.093043780382365 0.35833333333333334\n",
      "219 번째 loss, accuracy:  1.092975966533777 0.35833333333333334\n",
      "220 번째 loss, accuracy:  1.0929094108743143 0.35833333333333334\n",
      "221 번째 loss, accuracy:  1.0928440828404706 0.35833333333333334\n",
      "222 번째 loss, accuracy:  1.0927799525273736 0.35833333333333334\n",
      "223 번째 loss, accuracy:  1.0927169906757233 0.35833333333333334\n",
      "224 번째 loss, accuracy:  1.092655168658936 0.35833333333333334\n",
      "225 번째 loss, accuracy:  1.0925944584704916 0.35833333333333334\n",
      "226 번째 loss, accuracy:  1.0925348327114754 0.35833333333333334\n",
      "227 번째 loss, accuracy:  1.0924762645783328 0.35833333333333334\n",
      "228 번째 loss, accuracy:  1.0924187278508102 0.35833333333333334\n",
      "229 번째 loss, accuracy:  1.0923621968800992 0.35833333333333334\n",
      "230 번째 loss, accuracy:  1.092306646577172 0.35833333333333334\n",
      "231 번째 loss, accuracy:  1.0922520524013215 0.35833333333333334\n",
      "232 번째 loss, accuracy:  1.0921983903488808 0.35833333333333334\n",
      "233 번째 loss, accuracy:  1.092145636942142 0.35833333333333334\n",
      "234 번째 loss, accuracy:  1.0920937692184627 0.35833333333333334\n",
      "235 번째 loss, accuracy:  1.092042764719558 0.35833333333333334\n",
      "236 번째 loss, accuracy:  1.091992601480981 0.35833333333333334\n",
      "237 번째 loss, accuracy:  1.091943258021783 0.35833333333333334\n",
      "238 번째 loss, accuracy:  1.0918947133343608 0.35833333333333334\n",
      "239 번째 loss, accuracy:  1.0918469468744783 0.35833333333333334\n",
      "240 번째 loss, accuracy:  1.0917999385514678 0.35833333333333334\n",
      "241 번째 loss, accuracy:  1.0917536687186133 0.35833333333333334\n",
      "242 번째 loss, accuracy:  1.0917081181636987 0.35833333333333334\n",
      "243 번째 loss, accuracy:  1.09166326809973 0.35833333333333334\n",
      "244 번째 loss, accuracy:  1.0916191001558304 0.35833333333333334\n",
      "245 번째 loss, accuracy:  1.0915755963682985 0.35833333333333334\n",
      "246 번째 loss, accuracy:  1.0915327391718397 0.35833333333333334\n",
      "247 번째 loss, accuracy:  1.0914905113909483 0.35833333333333334\n",
      "248 번째 loss, accuracy:  1.09144889623146 0.35833333333333334\n",
      "249 번째 loss, accuracy:  1.0914078772722633 0.35833333333333334\n",
      "250 번째 loss, accuracy:  1.0913674384571617 0.35833333333333334\n",
      "251 번째 loss, accuracy:  1.091327564086894 0.35833333333333334\n",
      "252 번째 loss, accuracy:  1.0912882388113085 0.35833333333333334\n",
      "253 번째 loss, accuracy:  1.0912494476216872 0.35833333333333334\n",
      "254 번째 loss, accuracy:  1.0912111758432086 0.35833333333333334\n",
      "255 번째 loss, accuracy:  1.0911734091275715 0.35833333333333334\n",
      "256 번째 loss, accuracy:  1.0911361334457501 0.35833333333333334\n",
      "257 번째 loss, accuracy:  1.0910993350808957 0.35833333333333334\n",
      "258 번째 loss, accuracy:  1.0910630006213777 0.35833333333333334\n",
      "259 번째 loss, accuracy:  1.0910271169539598 0.35833333333333334\n",
      "260 번째 loss, accuracy:  1.0909916712571095 0.35833333333333334\n",
      "261 번째 loss, accuracy:  1.0909566509944484 0.35833333333333334\n",
      "262 번째 loss, accuracy:  1.090922043908318 0.35833333333333334\n",
      "263 번째 loss, accuracy:  1.0908878380134939 0.35833333333333334\n",
      "264 번째 loss, accuracy:  1.090854021591007 0.35833333333333334\n",
      "265 번째 loss, accuracy:  1.0908205831820998 0.35833333333333334\n",
      "266 번째 loss, accuracy:  1.0907875115823022 0.35833333333333334\n",
      "267 번째 loss, accuracy:  1.0907547958356303 0.35833333333333334\n",
      "268 번째 loss, accuracy:  1.090722425228896 0.35833333333333334\n",
      "269 번째 loss, accuracy:  1.0906903892861386 0.35833333333333334\n",
      "270 번째 loss, accuracy:  1.0906586777631686 0.35833333333333334\n",
      "271 번째 loss, accuracy:  1.0906272806422233 0.35833333333333334\n",
      "272 번째 loss, accuracy:  1.090596188126734 0.35833333333333334\n",
      "273 번째 loss, accuracy:  1.0905653906361938 0.35833333333333334\n",
      "274 번째 loss, accuracy:  1.0905348788011484 0.35833333333333334\n",
      "275 번째 loss, accuracy:  1.0905046434582668 0.35833333333333334\n",
      "276 번째 loss, accuracy:  1.0904746756455344 0.35833333333333334\n",
      "277 번째 loss, accuracy:  1.0904449665975415 0.35833333333333334\n",
      "278 번째 loss, accuracy:  1.0904155077408642 0.35833333333333334\n",
      "279 번째 loss, accuracy:  1.0903862906895454 0.35833333333333334\n",
      "280 번째 loss, accuracy:  1.090357307240675 0.35833333333333334\n",
      "281 번째 loss, accuracy:  1.0903285493700556 0.35833333333333334\n",
      "282 번째 loss, accuracy:  1.0903000092279655 0.35833333333333334\n",
      "283 번째 loss, accuracy:  1.0902716791350102 0.35833333333333334\n",
      "284 번째 loss, accuracy:  1.0902435515780562 0.35833333333333334\n",
      "285 번째 loss, accuracy:  1.0902156192062615 0.35833333333333334\n",
      "286 번째 loss, accuracy:  1.0901878748271794 0.35833333333333334\n",
      "287 번째 loss, accuracy:  1.0901603114029537 0.35833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 번째 loss, accuracy:  1.0901329220465883 0.35833333333333334\n",
      "289 번째 loss, accuracy:  1.0901057000183034 0.35833333333333334\n",
      "290 번째 loss, accuracy:  1.0900786387219623 0.35833333333333334\n",
      "291 번째 loss, accuracy:  1.0900517317015839 0.35833333333333334\n",
      "292 번째 loss, accuracy:  1.0900249726379194 0.35833333333333334\n",
      "293 번째 loss, accuracy:  1.0899983553451107 0.35833333333333334\n",
      "294 번째 loss, accuracy:  1.0899718737674169 0.35833333333333334\n",
      "295 번째 loss, accuracy:  1.0899455219760115 0.35833333333333334\n",
      "296 번째 loss, accuracy:  1.0899192941658524 0.35833333333333334\n",
      "297 번째 loss, accuracy:  1.0898931846526132 0.35833333333333334\n",
      "298 번째 loss, accuracy:  1.0898671878696855 0.35833333333333334\n",
      "299 번째 loss, accuracy:  1.089841298365244 0.35833333333333334\n",
      "300 번째 loss, accuracy:  1.0898155107993783 0.35833333333333334\n",
      "301 번째 loss, accuracy:  1.0897898199412799 0.35833333333333334\n",
      "302 번째 loss, accuracy:  1.0897642206664995 0.35833333333333334\n",
      "303 번째 loss, accuracy:  1.0897387079542542 0.35833333333333334\n",
      "304 번째 loss, accuracy:  1.0897132768848032 0.35833333333333334\n",
      "305 번째 loss, accuracy:  1.089687922636868 0.35833333333333334\n",
      "306 번째 loss, accuracy:  1.0896626404851224 0.35833333333333334\n",
      "307 번째 loss, accuracy:  1.0896374257977295 0.35833333333333334\n",
      "308 번째 loss, accuracy:  1.0896122740339285 0.35833333333333334\n",
      "309 번째 loss, accuracy:  1.0895871807416837 0.35833333333333334\n",
      "310 번째 loss, accuracy:  1.0895621415553771 0.35833333333333334\n",
      "311 번째 loss, accuracy:  1.0895371521935548 0.35833333333333334\n",
      "312 번째 loss, accuracy:  1.08951220845672 0.35833333333333334\n",
      "313 번째 loss, accuracy:  1.0894873062251746 0.35833333333333334\n",
      "314 번째 loss, accuracy:  1.0894624414569118 0.35833333333333334\n",
      "315 번째 loss, accuracy:  1.0894376101855476 0.35833333333333334\n",
      "316 번째 loss, accuracy:  1.0894128085183001 0.35833333333333334\n",
      "317 번째 loss, accuracy:  1.0893880326340157 0.35833333333333334\n",
      "318 번째 loss, accuracy:  1.0893632787812342 0.35833333333333334\n",
      "319 번째 loss, accuracy:  1.0893385432762959 0.35833333333333334\n",
      "320 번째 loss, accuracy:  1.0893138225014922 0.35833333333333334\n",
      "321 번째 loss, accuracy:  1.089289112903254 0.35833333333333334\n",
      "322 번째 loss, accuracy:  1.0892644109903813 0.35833333333333334\n",
      "323 번째 loss, accuracy:  1.0892397133323086 0.35833333333333334\n",
      "324 번째 loss, accuracy:  1.0892150165574102 0.35833333333333334\n",
      "325 번째 loss, accuracy:  1.089190317351335 0.35833333333333334\n",
      "326 번째 loss, accuracy:  1.0891656124553917 0.35833333333333334\n",
      "327 번째 loss, accuracy:  1.089140898664951 0.35833333333333334\n",
      "328 번째 loss, accuracy:  1.089116172827895 0.35833333333333334\n",
      "329 번째 loss, accuracy:  1.0890914318430918 0.35833333333333334\n",
      "330 번째 loss, accuracy:  1.0890666726589127 0.35833333333333334\n",
      "331 번째 loss, accuracy:  1.0890418922717633 0.35833333333333334\n",
      "332 번째 loss, accuracy:  1.0890170877246674 0.35833333333333334\n",
      "333 번째 loss, accuracy:  1.0889922561058623 0.35833333333333334\n",
      "334 번째 loss, accuracy:  1.088967394547436 0.35833333333333334\n",
      "335 번째 loss, accuracy:  1.088942500223984 0.35833333333333334\n",
      "336 번째 loss, accuracy:  1.0889175703513032 0.35833333333333334\n",
      "337 번째 loss, accuracy:  1.0888926021851058 0.35833333333333334\n",
      "338 번째 loss, accuracy:  1.0888675930197635 0.35833333333333334\n",
      "339 번째 loss, accuracy:  1.0888425401870763 0.35833333333333334\n",
      "340 번째 loss, accuracy:  1.088817441055067 0.35833333333333334\n",
      "341 번째 loss, accuracy:  1.0887922930268015 0.35833333333333334\n",
      "342 번째 loss, accuracy:  1.0887670935392328 0.35833333333333334\n",
      "343 번째 loss, accuracy:  1.0887418400620683 0.35833333333333334\n",
      "344 번째 loss, accuracy:  1.0887165300966601 0.35833333333333334\n",
      "345 번째 loss, accuracy:  1.0886911611749142 0.35833333333333334\n",
      "346 번째 loss, accuracy:  1.0886657308582315 0.35833333333333334\n",
      "347 번째 loss, accuracy:  1.0886402367364583 0.35833333333333334\n",
      "348 번째 loss, accuracy:  1.0886146764268643 0.35833333333333334\n",
      "349 번째 loss, accuracy:  1.0885890475731381 0.35833333333333334\n",
      "350 번째 loss, accuracy:  1.0885633478444072 0.35833333333333334\n",
      "351 번째 loss, accuracy:  1.088537574934271 0.35833333333333334\n",
      "352 번째 loss, accuracy:  1.0885117265598507 0.35833333333333334\n",
      "353 번째 loss, accuracy:  1.0884858004608706 0.35833333333333334\n",
      "354 번째 loss, accuracy:  1.0884597943987386 0.35833333333333334\n",
      "355 번째 loss, accuracy:  1.0884337061556617 0.35833333333333334\n",
      "356 번째 loss, accuracy:  1.0884075335337613 0.35833333333333334\n",
      "357 번째 loss, accuracy:  1.0883812743542214 0.35833333333333334\n",
      "358 번째 loss, accuracy:  1.088354926456436 0.35833333333333334\n",
      "359 번째 loss, accuracy:  1.0883284876971888 0.35833333333333334\n",
      "360 번째 loss, accuracy:  1.0883019559498333 0.35833333333333334\n",
      "361 번째 loss, accuracy:  1.0882753291034954 0.35833333333333334\n",
      "362 번째 loss, accuracy:  1.0882486050622904 0.35833333333333334\n",
      "363 번째 loss, accuracy:  1.0882217817445463 0.35833333333333334\n",
      "364 번째 loss, accuracy:  1.0881948570820492 0.35833333333333334\n",
      "365 번째 loss, accuracy:  1.0881678290193 0.35833333333333334\n",
      "366 번째 loss, accuracy:  1.0881406955127744 0.35833333333333334\n",
      "367 번째 loss, accuracy:  1.0881134545302098 0.35833333333333334\n",
      "368 번째 loss, accuracy:  1.0880861040498933 0.35833333333333334\n",
      "369 번째 loss, accuracy:  1.0880586420599623 0.35833333333333334\n",
      "370 번째 loss, accuracy:  1.0880310665577209 0.35833333333333334\n",
      "371 번째 loss, accuracy:  1.0880033755489649 0.35833333333333334\n",
      "372 번째 loss, accuracy:  1.0879755670473135 0.35833333333333334\n",
      "373 번째 loss, accuracy:  1.0879476390735543 0.35833333333333334\n",
      "374 번째 loss, accuracy:  1.0879195896549996 0.35833333333333334\n",
      "375 번째 loss, accuracy:  1.087891416824849 0.35833333333333334\n",
      "376 번째 loss, accuracy:  1.087863118621564 0.35833333333333334\n",
      "377 번째 loss, accuracy:  1.0878346930882468 0.35833333333333334\n",
      "378 번째 loss, accuracy:  1.087806138272031 0.35833333333333334\n",
      "379 번째 loss, accuracy:  1.087777452223481 0.35833333333333334\n",
      "380 번째 loss, accuracy:  1.087748632995996 0.35833333333333334\n",
      "381 번째 loss, accuracy:  1.0877196786452288 0.35833333333333334\n",
      "382 번째 loss, accuracy:  1.0876905872284974 0.35833333333333334\n",
      "383 번째 loss, accuracy:  1.0876613568042193 0.35833333333333334\n",
      "384 번째 loss, accuracy:  1.0876319854313465 0.35833333333333334\n",
      "385 번째 loss, accuracy:  1.0876024711688008 0.35833333333333334\n",
      "386 번째 loss, accuracy:  1.0875728120749273 0.35833333333333334\n",
      "387 번째 loss, accuracy:  1.0875430062069422 0.35833333333333334\n",
      "388 번째 loss, accuracy:  1.087513051620394 0.35833333333333334\n",
      "389 번째 loss, accuracy:  1.0874829463686273 0.35833333333333334\n",
      "390 번째 loss, accuracy:  1.087452688502247 0.35833333333333334\n",
      "391 번째 loss, accuracy:  1.0874222760686016 0.35833333333333334\n",
      "392 번째 loss, accuracy:  1.0873917071112524 0.35833333333333334\n",
      "393 번째 loss, accuracy:  1.087360979669462 0.35833333333333334\n",
      "394 번째 loss, accuracy:  1.0873300917776765 0.35833333333333334\n",
      "395 번째 loss, accuracy:  1.0872990414650217 0.35833333333333334\n",
      "396 번째 loss, accuracy:  1.0872678267547935 0.35833333333333334\n",
      "397 번째 loss, accuracy:  1.0872364456639576 0.35833333333333334\n",
      "398 번째 loss, accuracy:  1.0872048962026488 0.35833333333333334\n",
      "399 번째 loss, accuracy:  1.0871731763736785 0.35833333333333334\n",
      "400 번째 loss, accuracy:  1.0871412841720385 0.35833333333333334\n",
      "401 번째 loss, accuracy:  1.0871092175844124 0.35833333333333334\n",
      "402 번째 loss, accuracy:  1.0870769745886828 0.35833333333333334\n",
      "403 번째 loss, accuracy:  1.0870445531534523 0.35833333333333334\n",
      "404 번째 loss, accuracy:  1.087011951237553 0.35833333333333334\n",
      "405 번째 loss, accuracy:  1.086979166789566 0.35833333333333334\n",
      "406 번째 loss, accuracy:  1.0869461977473427 0.35833333333333334\n",
      "407 번째 loss, accuracy:  1.0869130420375204 0.35833333333333334\n",
      "408 번째 loss, accuracy:  1.086879697575047 0.35833333333333334\n",
      "409 번째 loss, accuracy:  1.0868461622626995 0.35833333333333334\n",
      "410 번째 loss, accuracy:  1.0868124339906113 0.35833333333333334\n",
      "411 번째 loss, accuracy:  1.08677851063579 0.35833333333333334\n",
      "412 번째 loss, accuracy:  1.0867443900616485 0.35833333333333334\n",
      "413 번째 loss, accuracy:  1.0867100701175176 0.35833333333333334\n",
      "414 번째 loss, accuracy:  1.0866755486381776 0.35833333333333334\n",
      "415 번째 loss, accuracy:  1.0866408234433798 0.35833333333333334\n",
      "416 번째 loss, accuracy:  1.0866058923373656 0.35833333333333334\n",
      "417 번째 loss, accuracy:  1.0865707531083912 0.35833333333333334\n",
      "418 번째 loss, accuracy:  1.0865354035282488 0.35833333333333334\n",
      "419 번째 loss, accuracy:  1.0864998413517815 0.35833333333333334\n",
      "420 번째 loss, accuracy:  1.0864640643164105 0.35833333333333334\n",
      "421 번째 loss, accuracy:  1.0864280701416404 0.35833333333333334\n",
      "422 번째 loss, accuracy:  1.0863918565285857 0.35833333333333334\n",
      "423 번째 loss, accuracy:  1.0863554211594766 0.35833333333333334\n",
      "424 번째 loss, accuracy:  1.0863187616971717 0.35833333333333334\n",
      "425 번째 loss, accuracy:  1.0862818757846686 0.35833333333333334\n",
      "426 번째 loss, accuracy:  1.086244761044611 0.35833333333333334\n",
      "427 번째 loss, accuracy:  1.0862074150787906 0.35833333333333334\n",
      "428 번째 loss, accuracy:  1.0861698354676472 0.35833333333333334\n",
      "429 번째 loss, accuracy:  1.08613201976977 0.35833333333333334\n",
      "430 번째 loss, accuracy:  1.0860939655213884 0.35833333333333334\n",
      "431 번째 loss, accuracy:  1.0860556702358717 0.35833333333333334\n",
      "432 번째 loss, accuracy:  1.0860171314032057 0.35833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433 번째 loss, accuracy:  1.085978346489484 0.35833333333333334\n",
      "434 번째 loss, accuracy:  1.085939312936387 0.35833333333333334\n",
      "435 번째 loss, accuracy:  1.0859000281606572 0.35833333333333334\n",
      "436 번째 loss, accuracy:  1.0858604895535702 0.35833333333333334\n",
      "437 번째 loss, accuracy:  1.085820694480403 0.35833333333333334\n",
      "438 번째 loss, accuracy:  1.085780640279896 0.35833333333333334\n",
      "439 번째 loss, accuracy:  1.0857403242637096 0.35833333333333334\n",
      "440 번째 loss, accuracy:  1.0856997437158793 0.35833333333333334\n",
      "441 번째 loss, accuracy:  1.0856588958922586 0.35833333333333334\n",
      "442 번째 loss, accuracy:  1.085617778019962 0.35833333333333334\n",
      "443 번째 loss, accuracy:  1.0855763872967998 0.35833333333333334\n",
      "444 번째 loss, accuracy:  1.0855347208907113 0.35833333333333334\n",
      "445 번째 loss, accuracy:  1.0854927759391837 0.35833333333333334\n",
      "446 번째 loss, accuracy:  1.0854505495486735 0.35833333333333334\n",
      "447 번째 loss, accuracy:  1.0854080387940124 0.35833333333333334\n",
      "448 번째 loss, accuracy:  1.0853652407178156 0.35833333333333334\n",
      "449 번째 loss, accuracy:  1.0853221523298753 0.35833333333333334\n",
      "450 번째 loss, accuracy:  1.0852787706065505 0.35833333333333334\n",
      "451 번째 loss, accuracy:  1.0852350924901517 0.35833333333333334\n",
      "452 번째 loss, accuracy:  1.085191114888315 0.35833333333333334\n",
      "453 번째 loss, accuracy:  1.0851468346733653 0.35833333333333334\n",
      "454 번째 loss, accuracy:  1.0851022486816784 0.35833333333333334\n",
      "455 번째 loss, accuracy:  1.0850573537130297 0.35833333333333334\n",
      "456 번째 loss, accuracy:  1.0850121465299365 0.35833333333333334\n",
      "457 번째 loss, accuracy:  1.0849666238569897 0.35833333333333334\n",
      "458 번째 loss, accuracy:  1.0849207823801799 0.35833333333333334\n",
      "459 번째 loss, accuracy:  1.0848746187462097 0.35833333333333334\n",
      "460 번째 loss, accuracy:  1.0848281295618016 0.35833333333333334\n",
      "461 번째 loss, accuracy:  1.0847813113929894 0.35833333333333334\n",
      "462 번째 loss, accuracy:  1.0847341607644065 0.35833333333333334\n",
      "463 번째 loss, accuracy:  1.0846866741585601 0.35833333333333334\n",
      "464 번째 loss, accuracy:  1.0846388480150988 0.35833333333333334\n",
      "465 번째 loss, accuracy:  1.0845906787300583 0.35833333333333334\n",
      "466 번째 loss, accuracy:  1.0845421626551162 0.35833333333333334\n",
      "467 번째 loss, accuracy:  1.084493296096814 0.35833333333333334\n",
      "468 번째 loss, accuracy:  1.0844440753157798 0.35833333333333334\n",
      "469 번째 loss, accuracy:  1.0843944965259382 0.35833333333333334\n",
      "470 번째 loss, accuracy:  1.0843445558937062 0.35833333333333334\n",
      "471 번째 loss, accuracy:  1.0842942495371728 0.35833333333333334\n",
      "472 번째 loss, accuracy:  1.0842435735252767 0.35833333333333334\n",
      "473 번째 loss, accuracy:  1.0841925238769614 0.35833333333333334\n",
      "474 번째 loss, accuracy:  1.084141096560318 0.35833333333333334\n",
      "475 번째 loss, accuracy:  1.0840892874917187 0.35833333333333334\n",
      "476 번째 loss, accuracy:  1.084037092534935 0.35833333333333334\n",
      "477 번째 loss, accuracy:  1.0839845075002397 0.35833333333333334\n",
      "478 번째 loss, accuracy:  1.0839315281434945 0.35833333333333334\n",
      "479 번째 loss, accuracy:  1.083878150165226 0.35833333333333334\n",
      "480 번째 loss, accuracy:  1.0838243692096847 0.35833333333333334\n",
      "481 번째 loss, accuracy:  1.0837701808638893 0.35833333333333334\n",
      "482 번째 loss, accuracy:  1.0837155806566505 0.35833333333333334\n",
      "483 번째 loss, accuracy:  1.0836605640575852 0.35833333333333334\n",
      "484 번째 loss, accuracy:  1.0836051264761133 0.35833333333333334\n",
      "485 번째 loss, accuracy:  1.083549263260434 0.35833333333333334\n",
      "486 번째 loss, accuracy:  1.0834929696964815 0.35833333333333334\n",
      "487 번째 loss, accuracy:  1.0834362410068819 0.35833333333333334\n",
      "488 번째 loss, accuracy:  1.0833790723498657 0.35833333333333334\n",
      "489 번째 loss, accuracy:  1.0833214588181808 0.35833333333333334\n",
      "490 번째 loss, accuracy:  1.0832633954379844 0.35833333333333334\n",
      "491 번째 loss, accuracy:  1.0832048771677052 0.35833333333333334\n",
      "492 번째 loss, accuracy:  1.083145898896898 0.35833333333333334\n",
      "493 번째 loss, accuracy:  1.0830864554450745 0.35833333333333334\n",
      "494 번째 loss, accuracy:  1.083026541560512 0.35833333333333334\n",
      "495 번째 loss, accuracy:  1.0829661519190443 0.35833333333333334\n",
      "496 번째 loss, accuracy:  1.0829052811228268 0.35833333333333334\n",
      "497 번째 loss, accuracy:  1.0828439236990872 0.35833333333333334\n",
      "498 번째 loss, accuracy:  1.082782074098848 0.35833333333333334\n",
      "499 번째 loss, accuracy:  1.0827197266956305 0.35833333333333334\n",
      "500 번째 loss, accuracy:  1.0826568757841328 0.35833333333333334\n",
      "501 번째 loss, accuracy:  1.082593515578889 0.35833333333333334\n",
      "502 번째 loss, accuracy:  1.0825296402128985 0.35833333333333334\n",
      "503 번째 loss, accuracy:  1.082465243736235 0.35833333333333334\n",
      "504 번째 loss, accuracy:  1.082400320114633 0.35833333333333334\n",
      "505 번째 loss, accuracy:  1.0823348632280454 0.35833333333333334\n",
      "506 번째 loss, accuracy:  1.082268866869174 0.35833333333333334\n",
      "507 번째 loss, accuracy:  1.0822023247419774 0.35833333333333334\n",
      "508 번째 loss, accuracy:  1.0821352304601524 0.35833333333333334\n",
      "509 번째 loss, accuracy:  1.0820675775455866 0.35833333333333334\n",
      "510 번째 loss, accuracy:  1.0819993594267823 0.35833333333333334\n",
      "511 번째 loss, accuracy:  1.081930569437254 0.35833333333333334\n",
      "512 번째 loss, accuracy:  1.0818612008138966 0.35833333333333334\n",
      "513 번째 loss, accuracy:  1.0817912466953257 0.35833333333333334\n",
      "514 번째 loss, accuracy:  1.0817207001201807 0.35833333333333334\n",
      "515 번째 loss, accuracy:  1.0816495540254136 0.35833333333333334\n",
      "516 번째 loss, accuracy:  1.0815778012445254 0.35833333333333334\n",
      "517 번째 loss, accuracy:  1.0815054345057873 0.35833333333333334\n",
      "518 번째 loss, accuracy:  1.0814324464304272 0.35833333333333334\n",
      "519 번째 loss, accuracy:  1.0813588295307761 0.35833333333333334\n",
      "520 번째 loss, accuracy:  1.0812845762083858 0.35833333333333334\n",
      "521 번째 loss, accuracy:  1.0812096787521162 0.35833333333333334\n",
      "522 번째 loss, accuracy:  1.0811341293361787 0.35833333333333334\n",
      "523 번째 loss, accuracy:  1.081057920018158 0.35833333333333334\n",
      "524 번째 loss, accuracy:  1.080981042736981 0.35833333333333334\n",
      "525 번째 loss, accuracy:  1.0809034893108675 0.35833333333333334\n",
      "526 번째 loss, accuracy:  1.0808252514352248 0.35833333333333334\n",
      "527 번째 loss, accuracy:  1.080746320680525 0.35833333333333334\n",
      "528 번째 loss, accuracy:  1.0806666884901237 0.35833333333333334\n",
      "529 번째 loss, accuracy:  1.0805863461780594 0.35833333333333334\n",
      "530 번째 loss, accuracy:  1.0805052849267893 0.35833333333333334\n",
      "531 번째 loss, accuracy:  1.0804234957849128 0.35833333333333334\n",
      "532 번째 loss, accuracy:  1.0803409696648332 0.35833333333333334\n",
      "533 번째 loss, accuracy:  1.0802576973403804 0.35833333333333334\n",
      "534 번째 loss, accuracy:  1.080173669444403 0.35833333333333334\n",
      "535 번째 loss, accuracy:  1.080088876466303 0.35833333333333334\n",
      "536 번째 loss, accuracy:  1.0800033087495353 0.35833333333333334\n",
      "537 번째 loss, accuracy:  1.0799169564890614 0.35833333333333334\n",
      "538 번째 loss, accuracy:  1.0798298097287546 0.35833333333333334\n",
      "539 번째 loss, accuracy:  1.0797418583587621 0.35833333333333334\n",
      "540 번째 loss, accuracy:  1.0796530921128198 0.35833333333333334\n",
      "541 번째 loss, accuracy:  1.0795635005655233 0.35833333333333334\n",
      "542 번째 loss, accuracy:  1.0794730731295428 0.35833333333333334\n",
      "543 번째 loss, accuracy:  1.0793817990527999 0.35833333333333334\n",
      "544 번째 loss, accuracy:  1.0792896674155827 0.35833333333333334\n",
      "545 번째 loss, accuracy:  1.0791966671276285 0.35833333333333334\n",
      "546 번째 loss, accuracy:  1.0791027869251328 0.35833333333333334\n",
      "547 번째 loss, accuracy:  1.0790080153677246 0.35833333333333334\n",
      "548 번째 loss, accuracy:  1.0789123408353865 0.35833333333333334\n",
      "549 번째 loss, accuracy:  1.078815751525318 0.35833333333333334\n",
      "550 번째 loss, accuracy:  1.0787182354487495 0.35833333333333334\n",
      "551 번째 loss, accuracy:  1.0786197804276965 0.35833333333333334\n",
      "552 번째 loss, accuracy:  1.0785203740916667 0.35833333333333334\n",
      "553 번째 loss, accuracy:  1.0784200038743057 0.35833333333333334\n",
      "554 번째 loss, accuracy:  1.078318657009992 0.35833333333333334\n",
      "555 번째 loss, accuracy:  1.0782163205303756 0.35833333333333334\n",
      "556 번째 loss, accuracy:  1.0781129812608528 0.35833333333333334\n",
      "557 번째 loss, accuracy:  1.0780086258169896 0.35833333333333334\n",
      "558 번째 loss, accuracy:  1.0779032406008908 0.35833333333333334\n",
      "559 번째 loss, accuracy:  1.0777968117975003 0.35833333333333334\n",
      "560 번째 loss, accuracy:  1.07768932537085 0.35833333333333334\n",
      "561 번째 loss, accuracy:  1.0775807670602526 0.35833333333333334\n",
      "562 번째 loss, accuracy:  1.0774711223764253 0.35833333333333334\n",
      "563 번째 loss, accuracy:  1.0773603765975606 0.35833333333333334\n",
      "564 번째 loss, accuracy:  1.0772485147653366 0.35833333333333334\n",
      "565 번째 loss, accuracy:  1.077135521680865 0.35833333333333334\n",
      "566 번째 loss, accuracy:  1.077021381900577 0.35833333333333334\n",
      "567 번째 loss, accuracy:  1.0769060797320562 0.35833333333333334\n",
      "568 번째 loss, accuracy:  1.0767895992298002 0.35833333333333334\n",
      "569 번째 loss, accuracy:  1.07667192419093 0.35833333333333334\n",
      "570 번째 loss, accuracy:  1.0765530381508326 0.35833333333333334\n",
      "571 번째 loss, accuracy:  1.07643292437875 0.35833333333333334\n",
      "572 번째 loss, accuracy:  1.0763115658733018 0.35833333333333334\n",
      "573 번째 loss, accuracy:  1.0761889453579536 0.35833333333333334\n",
      "574 번째 loss, accuracy:  1.0760650452764144 0.35833333333333334\n",
      "575 번째 loss, accuracy:  1.0759398477879962 0.35833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576 번째 loss, accuracy:  1.0758133347628884 0.35833333333333334\n",
      "577 번째 loss, accuracy:  1.0756854877773945 0.35833333333333334\n",
      "578 번째 loss, accuracy:  1.0755562881091074 0.35833333333333334\n",
      "579 번째 loss, accuracy:  1.0754257167320236 0.35833333333333334\n",
      "580 번째 loss, accuracy:  1.0752937543116077 0.35833333333333334\n",
      "581 번째 loss, accuracy:  1.075160381199796 0.35833333333333334\n",
      "582 번째 loss, accuracy:  1.075025577429964 0.35833333333333334\n",
      "583 번째 loss, accuracy:  1.0748893227118204 0.35833333333333334\n",
      "584 번째 loss, accuracy:  1.074751596426273 0.35833333333333334\n",
      "585 번째 loss, accuracy:  1.0746123776202325 0.35833333333333334\n",
      "586 번째 loss, accuracy:  1.0744716450013785 0.35833333333333334\n",
      "587 번째 loss, accuracy:  1.0743293769328814 0.35833333333333334\n",
      "588 번째 loss, accuracy:  1.0741855514280862 0.35833333333333334\n",
      "589 번째 loss, accuracy:  1.074040146145154 0.35833333333333334\n",
      "590 번째 loss, accuracy:  1.0738931383816719 0.35833333333333334\n",
      "591 번째 loss, accuracy:  1.0737445050692287 0.35833333333333334\n",
      "592 번째 loss, accuracy:  1.073594222767966 0.35833333333333334\n",
      "593 번째 loss, accuracy:  1.0734422676611046 0.35833333333333334\n",
      "594 번째 loss, accuracy:  1.0732886155494508 0.35833333333333334\n",
      "595 번째 loss, accuracy:  1.073133241845878 0.35833333333333334\n",
      "596 번째 loss, accuracy:  1.0729761215698155 0.35833333333333334\n",
      "597 번째 loss, accuracy:  1.0728172293417158 0.35833333333333334\n",
      "598 번째 loss, accuracy:  1.0726565393775236 0.35833333333333334\n",
      "599 번째 loss, accuracy:  1.0724940254831596 0.35833333333333334\n",
      "600 번째 loss, accuracy:  1.0723296610490012 0.35833333333333334\n",
      "601 번째 loss, accuracy:  1.0721634190443874 0.35833333333333334\n",
      "602 번째 loss, accuracy:  1.0719952720121586 0.35833333333333334\n",
      "603 번째 loss, accuracy:  1.0718251920632118 0.35833333333333334\n",
      "604 번째 loss, accuracy:  1.071653150871106 0.35833333333333334\n",
      "605 번째 loss, accuracy:  1.0714791196667206 0.35833333333333334\n",
      "606 번째 loss, accuracy:  1.0713030692329524 0.35833333333333334\n",
      "607 번째 loss, accuracy:  1.071124969899514 0.35833333333333334\n",
      "608 번째 loss, accuracy:  1.0709447915377752 0.35833333333333334\n",
      "609 번째 loss, accuracy:  1.0707625035557102 0.35833333333333334\n",
      "610 번째 loss, accuracy:  1.0705780748929523 0.35833333333333334\n",
      "611 번째 loss, accuracy:  1.070391474015944 0.35833333333333334\n",
      "612 번째 loss, accuracy:  1.0702026689132247 0.35833333333333334\n",
      "613 번째 loss, accuracy:  1.0700116270908566 0.35833333333333334\n",
      "614 번째 loss, accuracy:  1.069818315567992 0.35833333333333334\n",
      "615 번째 loss, accuracy:  1.0696227008726216 0.35833333333333334\n",
      "616 번째 loss, accuracy:  1.0694247490375128 0.35833333333333334\n",
      "617 번째 loss, accuracy:  1.069224425596325 0.35833333333333334\n",
      "618 번째 loss, accuracy:  1.0690216955799745 0.35833333333333334\n",
      "619 번째 loss, accuracy:  1.0688165235132145 0.35833333333333334\n",
      "620 번째 loss, accuracy:  1.0686088734114763 0.35833333333333334\n",
      "621 번째 loss, accuracy:  1.0683987087779983 0.35833333333333334\n",
      "622 번째 loss, accuracy:  1.0681859926012445 0.35833333333333334\n",
      "623 번째 loss, accuracy:  1.0679706873526595 0.35833333333333334\n",
      "624 번째 loss, accuracy:  1.0677527549847396 0.35833333333333334\n",
      "625 번째 loss, accuracy:  1.0675321569295144 0.35833333333333334\n",
      "626 번째 loss, accuracy:  1.0673088540973887 0.35833333333333334\n",
      "627 번째 loss, accuracy:  1.0670828068764462 0.35833333333333334\n",
      "628 번째 loss, accuracy:  1.0668539751321604 0.35833333333333334\n",
      "629 번째 loss, accuracy:  1.0666223182076309 0.35833333333333334\n",
      "630 번째 loss, accuracy:  1.0663877949243035 0.35833333333333334\n",
      "631 번째 loss, accuracy:  1.0661503635832532 0.35833333333333334\n",
      "632 번째 loss, accuracy:  1.0659099819670368 0.35833333333333334\n",
      "633 번째 loss, accuracy:  1.0656666073421635 0.35833333333333334\n",
      "634 번째 loss, accuracy:  1.0654201964622094 0.35833333333333334\n",
      "635 번째 loss, accuracy:  1.065170705571627 0.35833333333333334\n",
      "636 번째 loss, accuracy:  1.0649180904102864 0.35833333333333334\n",
      "637 번째 loss, accuracy:  1.0646623062187552 0.35833333333333334\n",
      "638 번째 loss, accuracy:  1.0644033077444321 0.35833333333333334\n",
      "639 번째 loss, accuracy:  1.0641410492484893 0.35833333333333334\n",
      "640 번째 loss, accuracy:  1.0638754845137306 0.35833333333333334\n",
      "641 번째 loss, accuracy:  1.0636065668533905 0.35833333333333334\n",
      "642 번째 loss, accuracy:  1.0633342491209241 0.35833333333333334\n",
      "643 번째 loss, accuracy:  1.063058483720826 0.36666666666666664\n",
      "644 번째 loss, accuracy:  1.0627792226205306 0.36666666666666664\n",
      "645 번째 loss, accuracy:  1.0624964173634692 0.36666666666666664\n",
      "646 번째 loss, accuracy:  1.0622100190832955 0.36666666666666664\n",
      "647 번째 loss, accuracy:  1.0619199785193605 0.375\n",
      "648 번째 loss, accuracy:  1.0616262460334829 0.375\n",
      "649 번째 loss, accuracy:  1.0613287716280586 0.375\n",
      "650 번째 loss, accuracy:  1.061027504965592 0.375\n",
      "651 번째 loss, accuracy:  1.060722395389667 0.375\n",
      "652 번째 loss, accuracy:  1.0604133919474414 0.375\n",
      "653 번째 loss, accuracy:  1.0601004434137158 0.375\n",
      "654 번째 loss, accuracy:  1.0597834983166263 0.375\n",
      "655 번째 loss, accuracy:  1.0594625049650097 0.375\n",
      "656 번째 loss, accuracy:  1.0591374114775318 0.375\n",
      "657 번째 loss, accuracy:  1.058808165813582 0.375\n",
      "658 번째 loss, accuracy:  1.0584747158060286 0.375\n",
      "659 번째 loss, accuracy:  1.058137009195871 0.375\n",
      "660 번째 loss, accuracy:  1.0577949936688396 0.375\n",
      "661 번째 loss, accuracy:  1.0574486168939854 0.375\n",
      "662 번째 loss, accuracy:  1.057097826564341 0.38333333333333336\n",
      "663 번째 loss, accuracy:  1.0567425704396234 0.38333333333333336\n",
      "664 번째 loss, accuracy:  1.0563827963911068 0.38333333333333336\n",
      "665 번째 loss, accuracy:  1.0560184524486416 0.39166666666666666\n",
      "666 번째 loss, accuracy:  1.0556494868498616 0.39166666666666666\n",
      "667 번째 loss, accuracy:  1.0552758480916242 0.39166666666666666\n",
      "668 번째 loss, accuracy:  1.0548974849836865 0.4\n",
      "669 번째 loss, accuracy:  1.054514346704661 0.4\n",
      "670 번째 loss, accuracy:  1.0541263828602074 0.4\n",
      "671 번째 loss, accuracy:  1.0537335435435142 0.4\n",
      "672 번째 loss, accuracy:  1.053335779398052 0.4083333333333333\n",
      "673 번째 loss, accuracy:  1.0529330416825682 0.4083333333333333\n",
      "674 번째 loss, accuracy:  1.0525252823383002 0.4083333333333333\n",
      "675 번째 loss, accuracy:  1.0521124540583897 0.4166666666666667\n",
      "676 번째 loss, accuracy:  1.0516945103594444 0.4166666666666667\n",
      "677 번째 loss, accuracy:  1.0512714056551882 0.425\n",
      "678 번째 loss, accuracy:  1.0508430953321288 0.425\n",
      "679 번째 loss, accuracy:  1.0504095358271774 0.425\n",
      "680 번째 loss, accuracy:  1.0499706847070978 0.425\n",
      "681 번째 loss, accuracy:  1.0495265007496977 0.43333333333333335\n",
      "682 번째 loss, accuracy:  1.0490769440266408 0.43333333333333335\n",
      "683 번째 loss, accuracy:  1.0486219759877151 0.43333333333333335\n",
      "684 번째 loss, accuracy:  1.0481615595464133 0.43333333333333335\n",
      "685 번째 loss, accuracy:  1.0476956591666573 0.43333333333333335\n",
      "686 번째 loss, accuracy:  1.0472242409504766 0.43333333333333335\n",
      "687 번째 loss, accuracy:  1.046747272726418 0.43333333333333335\n",
      "688 번째 loss, accuracy:  1.0462647241384613 0.43333333333333335\n",
      "689 번째 loss, accuracy:  1.0457765667352164 0.44166666666666665\n",
      "690 번째 loss, accuracy:  1.0452827740591215 0.44166666666666665\n",
      "691 번째 loss, accuracy:  1.0447833217353557 0.45\n",
      "692 번째 loss, accuracy:  1.0442781875601856 0.45\n",
      "693 번째 loss, accuracy:  1.0437673515884265 0.45\n",
      "694 번째 loss, accuracy:  1.0432507962196524 0.45\n",
      "695 번째 loss, accuracy:  1.042728506282855 0.45\n",
      "696 번째 loss, accuracy:  1.0422004691191558 0.45\n",
      "697 번째 loss, accuracy:  1.0416666746622028 0.45\n",
      "698 번째 loss, accuracy:  1.0411271155158355 0.45\n",
      "699 번째 loss, accuracy:  1.040581787028677 0.45\n",
      "700 번째 loss, accuracy:  1.0400306873651608 0.45\n",
      "701 번째 loss, accuracy:  1.0394738175726466 0.45\n",
      "702 번째 loss, accuracy:  1.0389111816441488 0.45\n",
      "703 번째 loss, accuracy:  1.0383427865762724 0.45\n",
      "704 번째 loss, accuracy:  1.0377686424219446 0.45\n",
      "705 번째 loss, accuracy:  1.0371887623375051 0.45\n",
      "706 번째 loss, accuracy:  1.0366031626237302 0.45\n",
      "707 번째 loss, accuracy:  1.0360118627604045 0.45\n",
      "708 번째 loss, accuracy:  1.0354148854340586 0.4583333333333333\n",
      "709 번째 loss, accuracy:  1.0348122565584557 0.4666666666666667\n",
      "710 번째 loss, accuracy:  1.0342040052875456 0.475\n",
      "711 번째 loss, accuracy:  1.0335901640204794 0.48333333333333334\n",
      "712 번째 loss, accuracy:  1.032970768398472 0.48333333333333334\n",
      "713 번째 loss, accuracy:  1.032345857293168 0.48333333333333334\n",
      "714 번째 loss, accuracy:  1.031715472786367 0.48333333333333334\n",
      "715 번째 loss, accuracy:  1.0310796601408434 0.48333333333333334\n",
      "716 번째 loss, accuracy:  1.030438467762191 0.48333333333333334\n",
      "717 번째 loss, accuracy:  1.0297919471515629 0.48333333333333334\n",
      "718 번째 loss, accuracy:  1.0291401528492752 0.48333333333333334\n",
      "719 번째 loss, accuracy:  1.0284831423692873 0.48333333333333334\n",
      "720 번째 loss, accuracy:  1.0278209761246546 0.48333333333333334\n",
      "721 번째 loss, accuracy:  1.02715371734403 0.48333333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722 번째 loss, accuracy:  1.0264814319794762 0.49166666666666664\n",
      "723 번째 loss, accuracy:  1.0258041886057718 0.49166666666666664\n",
      "724 번째 loss, accuracy:  1.0251220583115785 0.49166666666666664\n",
      "725 번째 loss, accuracy:  1.024435114582824 0.5083333333333333\n",
      "726 번째 loss, accuracy:  1.0237434331787187 0.5166666666666667\n",
      "727 번째 loss, accuracy:  1.023047092000931 0.5166666666666667\n",
      "728 번째 loss, accuracy:  1.022346170956458 0.5166666666666667\n",
      "729 번째 loss, accuracy:  1.0216407518147776 0.5166666666666667\n",
      "730 번째 loss, accuracy:  1.0209309180599864 0.5166666666666667\n",
      "731 번째 loss, accuracy:  1.0202167547385543 0.5166666666666667\n",
      "732 번째 loss, accuracy:  1.019498348303488 0.525\n",
      "733 번째 loss, accuracy:  1.018775786455641 0.525\n",
      "734 번째 loss, accuracy:  1.018049157983012 0.525\n",
      "735 번째 loss, accuracy:  1.0173185525987967 0.525\n",
      "736 번째 loss, accuracy:  1.0165840607790761 0.525\n",
      "737 번째 loss, accuracy:  1.0158457736009647 0.525\n",
      "738 번째 loss, accuracy:  1.0151037825820746 0.525\n",
      "739 번째 loss, accuracy:  1.0143581795220917 0.5333333333333333\n",
      "740 번째 loss, accuracy:  1.0136090563473394 0.5333333333333333\n",
      "741 번째 loss, accuracy:  1.0128565049590543 0.5416666666666666\n",
      "742 번째 loss, accuracy:  1.0121006170862 0.5416666666666666\n",
      "743 번째 loss, accuracy:  1.0113414841434993 0.55\n",
      "744 번째 loss, accuracy:  1.0105791970953748 0.55\n",
      "745 번째 loss, accuracy:  1.0098138463264157 0.55\n",
      "746 번째 loss, accuracy:  1.009045521518947 0.55\n",
      "747 번째 loss, accuracy:  1.0082743115381985 0.5583333333333333\n",
      "748 번째 loss, accuracy:  1.0075003043255275 0.5583333333333333\n",
      "749 번째 loss, accuracy:  1.0067235868000233 0.5583333333333333\n",
      "750 번째 loss, accuracy:  1.005944244768855 0.5666666666666667\n",
      "751 번째 loss, accuracy:  1.0051623628465156 0.5666666666666667\n",
      "752 번째 loss, accuracy:  1.0043780243831586 0.5666666666666667\n",
      "753 번째 loss, accuracy:  1.0035913114020643 0.5666666666666667\n",
      "754 번째 loss, accuracy:  1.002802304546271 0.5666666666666667\n",
      "755 번째 loss, accuracy:  1.0020110830342859 0.5666666666666667\n",
      "756 번째 loss, accuracy:  1.0012177246246996 0.5666666666666667\n",
      "757 번째 loss, accuracy:  1.0004223055895678 0.575\n",
      "758 번째 loss, accuracy:  0.9996249006962228 0.5833333333333334\n",
      "759 번째 loss, accuracy:  0.998825583197246 0.5833333333333334\n",
      "760 번째 loss, accuracy:  0.9980244248281936 0.5833333333333334\n",
      "761 번째 loss, accuracy:  0.9972214958126623 0.5833333333333334\n",
      "762 번째 loss, accuracy:  0.9964168648742557 0.5833333333333334\n",
      "763 번째 loss, accuracy:  0.995610599254935 0.5833333333333334\n",
      "764 번째 loss, accuracy:  0.9948027647392389 0.5833333333333334\n",
      "765 번째 loss, accuracy:  0.9939934256838345 0.5833333333333334\n",
      "766 번째 loss, accuracy:  0.9931826450518666 0.5916666666666667\n",
      "767 번째 loss, accuracy:  0.9923704844515032 0.5916666666666667\n",
      "768 번째 loss, accuracy:  0.9915570041781543 0.5916666666666667\n",
      "769 번째 loss, accuracy:  0.9907422632597666 0.5916666666666667\n",
      "770 번째 loss, accuracy:  0.9899263195046518 0.5916666666666667\n",
      "771 번째 loss, accuracy:  0.9891092295513513 0.5916666666666667\n",
      "772 번째 loss, accuracy:  0.9882910489199369 0.5916666666666667\n",
      "773 번째 loss, accuracy:  0.9874718320643355 0.5916666666666667\n",
      "774 번째 loss, accuracy:  0.9866516324251258 0.6\n",
      "775 번째 loss, accuracy:  0.9858305024824243 0.6\n",
      "776 번째 loss, accuracy:  0.9850084938084189 0.6\n",
      "777 번째 loss, accuracy:  0.9841856571191525 0.6\n",
      "778 번째 loss, accuracy:  0.9833620423252459 0.6\n",
      "779 번째 loss, accuracy:  0.9825376985812111 0.6\n",
      "780 번째 loss, accuracy:  0.9817126743331045 0.6083333333333333\n",
      "781 번째 loss, accuracy:  0.9808870173642651 0.6083333333333333\n",
      "782 번째 loss, accuracy:  0.9800607748389124 0.6083333333333333\n",
      "783 번째 loss, accuracy:  0.9792339933434727 0.6083333333333333\n",
      "784 번째 loss, accuracy:  0.9784067189254437 0.625\n",
      "785 번째 loss, accuracy:  0.9775789971297253 0.625\n",
      "786 번째 loss, accuracy:  0.9767508730323122 0.625\n",
      "787 번째 loss, accuracy:  0.9759223912713263 0.625\n",
      "788 번째 loss, accuracy:  0.975093596075309 0.6333333333333333\n",
      "789 번째 loss, accuracy:  0.9742645312888587 0.6333333333333333\n",
      "790 번째 loss, accuracy:  0.9734352403955364 0.6333333333333333\n",
      "791 번째 loss, accuracy:  0.9726057665381602 0.6333333333333333\n",
      "792 번째 loss, accuracy:  0.9717761525365148 0.65\n",
      "793 번째 loss, accuracy:  0.9709464409025479 0.65\n",
      "794 번째 loss, accuracy:  0.9701166738531575 0.65\n",
      "795 번째 loss, accuracy:  0.9692868933206593 0.65\n",
      "796 번째 loss, accuracy:  0.9684571409610544 0.65\n",
      "797 번째 loss, accuracy:  0.9676274581602027 0.6583333333333333\n",
      "798 번째 loss, accuracy:  0.9667978860380465 0.6583333333333333\n",
      "799 번째 loss, accuracy:  0.9659684654510011 0.6666666666666666\n",
      "800 번째 loss, accuracy:  0.9651392369926267 0.6666666666666666\n",
      "801 번째 loss, accuracy:  0.9643102409927532 0.6666666666666666\n",
      "802 번째 loss, accuracy:  0.9634815175151734 0.6666666666666666\n",
      "803 번째 loss, accuracy:  0.9626531063540195 0.6666666666666666\n",
      "804 번째 loss, accuracy:  0.9618250470289775 0.6666666666666666\n",
      "805 번째 loss, accuracy:  0.9609973787794581 0.6666666666666666\n",
      "806 번째 loss, accuracy:  0.9601701405578633 0.6666666666666666\n",
      "807 번째 loss, accuracy:  0.9593433710220332 0.675\n",
      "808 번째 loss, accuracy:  0.9585171085270187 0.675\n",
      "809 번째 loss, accuracy:  0.9576913911163007 0.675\n",
      "810 번째 loss, accuracy:  0.9568662565125094 0.675\n",
      "811 번째 loss, accuracy:  0.9560417421077957 0.675\n",
      "812 번째 loss, accuracy:  0.9552178849539323 0.675\n",
      "813 번째 loss, accuracy:  0.9543947217522049 0.675\n",
      "814 번째 loss, accuracy:  0.9535722888432083 0.675\n",
      "815 번째 loss, accuracy:  0.9527506221966086 0.675\n",
      "816 번째 loss, accuracy:  0.9519297574009407 0.675\n",
      "817 번째 loss, accuracy:  0.9511097296534854 0.675\n",
      "818 번째 loss, accuracy:  0.9502905737503309 0.675\n",
      "819 번째 loss, accuracy:  0.9494723240766104 0.675\n",
      "820 번째 loss, accuracy:  0.9486550145970064 0.675\n",
      "821 번째 loss, accuracy:  0.9478386788465322 0.675\n",
      "822 번째 loss, accuracy:  0.9470233499216639 0.675\n",
      "823 번째 loss, accuracy:  0.9462090604718095 0.675\n",
      "824 번째 loss, accuracy:  0.9453958426911571 0.675\n",
      "825 번째 loss, accuracy:  0.9445837283109562 0.675\n",
      "826 번째 loss, accuracy:  0.9437727485922037 0.675\n",
      "827 번째 loss, accuracy:  0.9429629343187752 0.675\n",
      "828 번째 loss, accuracy:  0.9421543157910166 0.675\n",
      "829 번째 loss, accuracy:  0.9413469228197769 0.675\n",
      "830 번째 loss, accuracy:  0.9405407847209434 0.675\n",
      "831 번째 loss, accuracy:  0.9397359303103968 0.675\n",
      "832 번째 loss, accuracy:  0.9389323878994904 0.6833333333333333\n",
      "833 번째 loss, accuracy:  0.938130185290967 0.6833333333333333\n",
      "834 번째 loss, accuracy:  0.9373293497753428 0.6833333333333333\n",
      "835 번째 loss, accuracy:  0.9365299081277741 0.6833333333333333\n",
      "836 번째 loss, accuracy:  0.935731886605356 0.6833333333333333\n",
      "837 번째 loss, accuracy:  0.9349353109448917 0.6833333333333333\n",
      "838 번째 loss, accuracy:  0.9341402063610805 0.6833333333333333\n",
      "839 번째 loss, accuracy:  0.9333465975451586 0.6916666666666667\n",
      "840 번째 loss, accuracy:  0.932554508663928 0.6916666666666667\n",
      "841 번째 loss, accuracy:  0.9317639633592187 0.6916666666666667\n",
      "842 번째 loss, accuracy:  0.930974984747751 0.6916666666666667\n",
      "843 번째 loss, accuracy:  0.9301875954213508 0.6916666666666667\n",
      "844 번째 loss, accuracy:  0.9294018174475615 0.6916666666666667\n",
      "845 번째 loss, accuracy:  0.9286176723706068 0.6916666666666667\n",
      "846 번째 loss, accuracy:  0.9278351812126829 0.6916666666666667\n",
      "847 번째 loss, accuracy:  0.9270543644756111 0.6916666666666667\n",
      "848 번째 loss, accuracy:  0.9262752421427665 0.6916666666666667\n",
      "849 번째 loss, accuracy:  0.925497833681351 0.6916666666666667\n",
      "850 번째 loss, accuracy:  0.9247221580449151 0.6916666666666667\n",
      "851 번째 loss, accuracy:  0.9239482336761919 0.6916666666666667\n",
      "852 번째 loss, accuracy:  0.9231760785101669 0.6916666666666667\n",
      "853 번째 loss, accuracy:  0.9224057099774085 0.6916666666666667\n",
      "854 번째 loss, accuracy:  0.9216371450076287 0.6916666666666667\n",
      "855 번째 loss, accuracy:  0.9208704000334713 0.6916666666666667\n",
      "856 번째 loss, accuracy:  0.9201054909945117 0.6916666666666667\n",
      "857 번째 loss, accuracy:  0.9193424333414486 0.6916666666666667\n",
      "858 번째 loss, accuracy:  0.9185812420404832 0.6916666666666667\n",
      "859 번째 loss, accuracy:  0.9178219315778781 0.6916666666666667\n",
      "860 번째 loss, accuracy:  0.9170645159646706 0.6916666666666667\n",
      "861 번째 loss, accuracy:  0.9163090087415525 0.6916666666666667\n",
      "862 번째 loss, accuracy:  0.9155554229838814 0.6916666666666667\n",
      "863 번째 loss, accuracy:  0.9148037713068088 0.6916666666666667\n",
      "864 번째 loss, accuracy:  0.9140540658705644 0.6916666666666667\n",
      "865 번째 loss, accuracy:  0.913306318385821 0.6916666666666667\n",
      "866 번째 loss, accuracy:  0.9125605401191735 0.6916666666666667\n",
      "867 번째 loss, accuracy:  0.9118167418987173 0.6916666666666667\n",
      "868 번째 loss, accuracy:  0.9110749341196935 0.6916666666666667\n",
      "869 번째 loss, accuracy:  0.9103351267502288 0.6916666666666667\n",
      "870 번째 loss, accuracy:  0.909597329337128 0.6916666666666667\n",
      "871 번째 loss, accuracy:  0.9088615510117407 0.6916666666666667\n",
      "872 번째 loss, accuracy:  0.9081278004958778 0.6916666666666667\n",
      "873 번째 loss, accuracy:  0.9073960861077659 0.6916666666666667\n",
      "874 번째 loss, accuracy:  0.906666415768037 0.6916666666666667\n",
      "875 번째 loss, accuracy:  0.9059387970057848 0.6916666666666667\n",
      "876 번째 loss, accuracy:  0.9052132369646001 0.6916666666666667\n",
      "877 번째 loss, accuracy:  0.9044897424086618 0.6916666666666667\n",
      "878 번째 loss, accuracy:  0.9037683197288289 0.6916666666666667\n",
      "879 번째 loss, accuracy:  0.903048974948749 0.6916666666666667\n",
      "880 번째 loss, accuracy:  0.9023317137309589 0.6916666666666667\n",
      "881 번째 loss, accuracy:  0.9016165413829994 0.6916666666666667\n",
      "882 번째 loss, accuracy:  0.9009034628635272 0.6916666666666667\n",
      "883 번째 loss, accuracy:  0.9001924827883919 0.6916666666666667\n",
      "884 번째 loss, accuracy:  0.8994836054367352 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "885 번째 loss, accuracy:  0.8987768347570471 0.6916666666666667\n",
      "886 번째 loss, accuracy:  0.8980721743732204 0.6916666666666667\n",
      "887 번째 loss, accuracy:  0.8973696275905527 0.6916666666666667\n",
      "888 번째 loss, accuracy:  0.8966691974017511 0.6916666666666667\n",
      "889 번째 loss, accuracy:  0.8959708864928954 0.6916666666666667\n",
      "890 번째 loss, accuracy:  0.895274697249354 0.6916666666666667\n",
      "891 번째 loss, accuracy:  0.8945806317616878 0.6916666666666667\n",
      "892 번째 loss, accuracy:  0.8938886918314919 0.6916666666666667\n",
      "893 번째 loss, accuracy:  0.893198878977209 0.6916666666666667\n",
      "894 번째 loss, accuracy:  0.892511194439893 0.6916666666666667\n",
      "895 번째 loss, accuracy:  0.8918256391889368 0.6916666666666667\n",
      "896 번째 loss, accuracy:  0.8911422139277311 0.6916666666666667\n",
      "897 번째 loss, accuracy:  0.8904609190992938 0.6916666666666667\n",
      "898 번째 loss, accuracy:  0.8897817548918382 0.6916666666666667\n",
      "899 번째 loss, accuracy:  0.8891047212442907 0.6916666666666667\n",
      "900 번째 loss, accuracy:  0.8884298178517466 0.6916666666666667\n",
      "901 번째 loss, accuracy:  0.8877570441708806 0.6916666666666667\n",
      "902 번째 loss, accuracy:  0.887086399425291 0.6916666666666667\n",
      "903 번째 loss, accuracy:  0.8864178826107941 0.6916666666666667\n",
      "904 번째 loss, accuracy:  0.8857514925006477 0.6916666666666667\n",
      "905 번째 loss, accuracy:  0.8850872276507177 0.6916666666666667\n",
      "906 번째 loss, accuracy:  0.8844250864045949 0.6916666666666667\n",
      "907 번째 loss, accuracy:  0.8837650668986277 0.6916666666666667\n",
      "908 번째 loss, accuracy:  0.8831071670669135 0.6916666666666667\n",
      "909 번째 loss, accuracy:  0.8824513846462063 0.6916666666666667\n",
      "910 번째 loss, accuracy:  0.8817977171807821 0.6916666666666667\n",
      "911 번째 loss, accuracy:  0.8811461620272153 0.6916666666666667\n",
      "912 번째 loss, accuracy:  0.8804967163591132 0.6916666666666667\n",
      "913 번째 loss, accuracy:  0.8798493771717705 0.6916666666666667\n",
      "914 번째 loss, accuracy:  0.8792041412867557 0.6916666666666667\n",
      "915 번째 loss, accuracy:  0.8785610053564538 0.6916666666666667\n",
      "916 번째 loss, accuracy:  0.8779199658685092 0.6916666666666667\n",
      "917 번째 loss, accuracy:  0.8772810191502326 0.6916666666666667\n",
      "918 번째 loss, accuracy:  0.8766441613729306 0.6916666666666667\n",
      "919 번째 loss, accuracy:  0.8760093885561607 0.6916666666666667\n",
      "920 번째 loss, accuracy:  0.8753766965719305 0.6916666666666667\n",
      "921 번째 loss, accuracy:  0.8747460811488328 0.6916666666666667\n",
      "922 번째 loss, accuracy:  0.874117537876107 0.6916666666666667\n",
      "923 번째 loss, accuracy:  0.8734910622076435 0.6916666666666667\n",
      "924 번째 loss, accuracy:  0.872866649465913 0.6916666666666667\n",
      "925 번째 loss, accuracy:  0.8722442948458355 0.6916666666666667\n",
      "926 번째 loss, accuracy:  0.8716239934185879 0.6916666666666667\n",
      "927 번째 loss, accuracy:  0.8710057401353428 0.6916666666666667\n",
      "928 번째 loss, accuracy:  0.8703895298309445 0.6916666666666667\n",
      "929 번째 loss, accuracy:  0.8697753572275239 0.6916666666666667\n",
      "930 번째 loss, accuracy:  0.8691632169380419 0.6916666666666667\n",
      "931 번째 loss, accuracy:  0.8685531034697818 0.6916666666666667\n",
      "932 번째 loss, accuracy:  0.8679450112277661 0.6916666666666667\n",
      "933 번째 loss, accuracy:  0.8673389345181285 0.6916666666666667\n",
      "934 번째 loss, accuracy:  0.8667348675514045 0.6916666666666667\n",
      "935 번째 loss, accuracy:  0.8661328044457749 0.6916666666666667\n",
      "936 번째 loss, accuracy:  0.8655327392302513 0.6916666666666667\n",
      "937 번째 loss, accuracy:  0.8649346658477852 0.6916666666666667\n",
      "938 번째 loss, accuracy:  0.8643385781583333 0.6916666666666667\n",
      "939 번째 loss, accuracy:  0.8637444699418636 0.6916666666666667\n",
      "940 번째 loss, accuracy:  0.8631523349012921 0.6916666666666667\n",
      "941 번째 loss, accuracy:  0.8625621666653764 0.6916666666666667\n",
      "942 번째 loss, accuracy:  0.861973958791538 0.6916666666666667\n",
      "943 번째 loss, accuracy:  0.8613877047686419 0.6916666666666667\n",
      "944 번째 loss, accuracy:  0.8608033980197133 0.6916666666666667\n",
      "945 번째 loss, accuracy:  0.8602210319046034 0.6916666666666667\n",
      "946 번째 loss, accuracy:  0.8596405997225927 0.6916666666666667\n",
      "947 번째 loss, accuracy:  0.8590620947149489 0.6916666666666667\n",
      "948 번째 loss, accuracy:  0.8584855100674367 0.6916666666666667\n",
      "949 번째 loss, accuracy:  0.8579108389127575 0.6916666666666667\n",
      "950 번째 loss, accuracy:  0.8573380743329603 0.6916666666666667\n",
      "951 번째 loss, accuracy:  0.8567672093617754 0.6916666666666667\n",
      "952 번째 loss, accuracy:  0.8561982369869278 0.6916666666666667\n",
      "953 번째 loss, accuracy:  0.8556311501523801 0.6916666666666667\n",
      "954 번째 loss, accuracy:  0.8550659417605299 0.6916666666666667\n",
      "955 번째 loss, accuracy:  0.8545026046743599 0.6916666666666667\n",
      "956 번째 loss, accuracy:  0.8539411317195567 0.6916666666666667\n",
      "957 번째 loss, accuracy:  0.8533815156865499 0.6916666666666667\n",
      "958 번째 loss, accuracy:  0.8528237493325397 0.6916666666666667\n",
      "959 번째 loss, accuracy:  0.8522678253834525 0.6916666666666667\n",
      "960 번째 loss, accuracy:  0.8517137365358669 0.6916666666666667\n",
      "961 번째 loss, accuracy:  0.851161475458896 0.6916666666666667\n",
      "962 번째 loss, accuracy:  0.8506110347960155 0.6916666666666667\n",
      "963 번째 loss, accuracy:  0.8500624071668561 0.6916666666666667\n",
      "964 번째 loss, accuracy:  0.8495155851689613 0.6916666666666667\n",
      "965 번째 loss, accuracy:  0.8489705613794897 0.6916666666666667\n",
      "966 번째 loss, accuracy:  0.8484273283568792 0.6916666666666667\n",
      "967 번째 loss, accuracy:  0.8478858786424877 0.6916666666666667\n",
      "968 번째 loss, accuracy:  0.8473462047621749 0.6916666666666667\n",
      "969 번째 loss, accuracy:  0.8468082992278513 0.6916666666666667\n",
      "970 번째 loss, accuracy:  0.8462721545389914 0.6916666666666667\n",
      "971 번째 loss, accuracy:  0.8457377631841148 0.6916666666666667\n",
      "972 번째 loss, accuracy:  0.8452051176422157 0.6916666666666667\n",
      "973 번째 loss, accuracy:  0.8446742103841721 0.6916666666666667\n",
      "974 번째 loss, accuracy:  0.8441450338741139 0.6916666666666667\n",
      "975 번째 loss, accuracy:  0.8436175805707415 0.6916666666666667\n",
      "976 번째 loss, accuracy:  0.8430918429286431 0.6916666666666667\n",
      "977 번째 loss, accuracy:  0.8425678133995477 0.6916666666666667\n",
      "978 번째 loss, accuracy:  0.8420454844335611 0.6916666666666667\n",
      "979 번째 loss, accuracy:  0.8415248484803589 0.6916666666666667\n",
      "980 번째 loss, accuracy:  0.8410058979903579 0.6916666666666667\n",
      "981 번째 loss, accuracy:  0.8404886254158561 0.6916666666666667\n",
      "982 번째 loss, accuracy:  0.8399730232121283 0.6916666666666667\n",
      "983 번째 loss, accuracy:  0.839459083838513 0.6916666666666667\n",
      "984 번째 loss, accuracy:  0.8389467997594482 0.6916666666666667\n",
      "985 번째 loss, accuracy:  0.8384361634454962 0.6916666666666667\n",
      "986 번째 loss, accuracy:  0.837927167374324 0.6916666666666667\n",
      "987 번째 loss, accuracy:  0.8374198040316727 0.6916666666666667\n",
      "988 번째 loss, accuracy:  0.8369140659122907 0.6916666666666667\n",
      "989 번째 loss, accuracy:  0.8364099455208261 0.6916666666666667\n",
      "990 번째 loss, accuracy:  0.8359074353727268 0.6916666666666667\n",
      "991 번째 loss, accuracy:  0.835406527995085 0.6916666666666667\n",
      "992 번째 loss, accuracy:  0.8349072159274673 0.6916666666666667\n",
      "993 번째 loss, accuracy:  0.8344094917227172 0.6916666666666667\n",
      "994 번째 loss, accuracy:  0.8339133479477454 0.6916666666666667\n",
      "995 번째 loss, accuracy:  0.8334187771842778 0.6916666666666667\n",
      "996 번째 loss, accuracy:  0.832925772029593 0.6916666666666667\n",
      "997 번째 loss, accuracy:  0.8324343250972265 0.6916666666666667\n",
      "998 번째 loss, accuracy:  0.8319444290176757 0.6916666666666667\n",
      "999 번째 loss, accuracy:  0.8314560764390433 0.6916666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 loss, accuracy:  1.1062680255401636 0.16666666666666666\n",
      "1 번째 loss, accuracy:  1.1059961426261262 0.125\n",
      "2 번째 loss, accuracy:  1.1057277320999588 0.11666666666666667\n",
      "3 번째 loss, accuracy:  1.1054624068126024 0.10833333333333334\n",
      "4 번째 loss, accuracy:  1.105200018340363 0.10833333333333334\n",
      "5 번째 loss, accuracy:  1.104941577268517 0.1\n",
      "6 번째 loss, accuracy:  1.1046856650374353 0.1\n",
      "7 번째 loss, accuracy:  1.1044330047044209 0.1\n",
      "8 번째 loss, accuracy:  1.104183668963842 0.09166666666666666\n",
      "9 번째 loss, accuracy:  1.1039366777681372 0.09166666666666666\n",
      "10 번째 loss, accuracy:  1.1036925244259437 0.075\n",
      "11 번째 loss, accuracy:  1.103451477369667 0.075\n",
      "12 번째 loss, accuracy:  1.1032132437909397 0.075\n",
      "13 번째 loss, accuracy:  1.1029775846722847 0.075\n",
      "14 번째 loss, accuracy:  1.102743877768521 0.075\n",
      "15 번째 loss, accuracy:  1.1025132082432654 0.075\n",
      "16 번째 loss, accuracy:  1.1022844280806696 0.058333333333333334\n",
      "17 번째 loss, accuracy:  1.1020588172655428 0.058333333333333334\n",
      "18 번째 loss, accuracy:  1.1018358050721153 0.05\n",
      "19 번째 loss, accuracy:  1.101614466988302 0.05\n",
      "20 번째 loss, accuracy:  1.1013961727731878 0.05\n",
      "21 번째 loss, accuracy:  1.101179589298034 0.05\n",
      "22 번째 loss, accuracy:  1.1009651476149565 0.03333333333333333\n",
      "23 번째 loss, accuracy:  1.1007527629037586 0.025\n",
      "24 번째 loss, accuracy:  1.1005419933385228 0.025\n",
      "25 번째 loss, accuracy:  1.100334158673804 0.03333333333333333\n",
      "26 번째 loss, accuracy:  1.1001280747700095 0.03333333333333333\n",
      "27 번째 loss, accuracy:  1.0999240441098719 0.025\n",
      "28 번째 loss, accuracy:  1.099722194309953 0.03333333333333333\n",
      "29 번째 loss, accuracy:  1.0995217122610943 0.03333333333333333\n",
      "30 번째 loss, accuracy:  1.0993230287137181 0.03333333333333333\n",
      "31 번째 loss, accuracy:  1.099126307548444 0.03333333333333333\n",
      "32 번째 loss, accuracy:  1.0989311337583483 0.03333333333333333\n",
      "33 번째 loss, accuracy:  1.0987377023335432 0.03333333333333333\n",
      "34 번째 loss, accuracy:  1.0985457445220372 0.03333333333333333\n",
      "35 번째 loss, accuracy:  1.0983552813878854 0.041666666666666664\n",
      "36 번째 loss, accuracy:  1.098166538964564 0.041666666666666664\n",
      "37 번째 loss, accuracy:  1.09797949998244 0.041666666666666664\n",
      "38 번째 loss, accuracy:  1.097794125627164 0.058333333333333334\n",
      "39 번째 loss, accuracy:  1.0976098745029808 0.058333333333333334\n",
      "40 번째 loss, accuracy:  1.0974266733085922 0.058333333333333334\n",
      "41 번째 loss, accuracy:  1.0972451510250465 0.058333333333333334\n",
      "42 번째 loss, accuracy:  1.097064728948266 0.058333333333333334\n",
      "43 번째 loss, accuracy:  1.0968858156571017 0.058333333333333334\n",
      "44 번째 loss, accuracy:  1.096707929020994 0.058333333333333334\n",
      "45 번째 loss, accuracy:  1.0965315538743874 0.058333333333333334\n",
      "46 번째 loss, accuracy:  1.0963557686203846 0.058333333333333334\n",
      "47 번째 loss, accuracy:  1.096181407102684 0.058333333333333334\n",
      "48 번째 loss, accuracy:  1.0960077896790386 0.058333333333333334\n",
      "49 번째 loss, accuracy:  1.0958357668940357 0.058333333333333334\n",
      "50 번째 loss, accuracy:  1.0956640925122585 0.058333333333333334\n",
      "51 번째 loss, accuracy:  1.095493862110056 0.058333333333333334\n",
      "52 번째 loss, accuracy:  1.0953238768362312 0.075\n",
      "53 번째 loss, accuracy:  1.0951551001867614 0.075\n",
      "54 번째 loss, accuracy:  1.0949870089323972 0.075\n",
      "55 번째 loss, accuracy:  1.0948198505823543 0.08333333333333333\n",
      "56 번째 loss, accuracy:  1.0946532067448556 0.08333333333333333\n",
      "57 번째 loss, accuracy:  1.094487599168248 0.08333333333333333\n",
      "58 번째 loss, accuracy:  1.094322588859293 0.08333333333333333\n",
      "59 번째 loss, accuracy:  1.094157705022705 0.08333333333333333\n",
      "60 번째 loss, accuracy:  1.093994223285565 0.08333333333333333\n",
      "61 번째 loss, accuracy:  1.093830767253324 0.08333333333333333\n",
      "62 번째 loss, accuracy:  1.0936681407161999 0.08333333333333333\n",
      "63 번째 loss, accuracy:  1.0935057424665124 0.08333333333333333\n",
      "64 번째 loss, accuracy:  1.093343577440858 0.08333333333333333\n",
      "65 번째 loss, accuracy:  1.0931824788330315 0.1\n",
      "66 번째 loss, accuracy:  1.0930214128812152 0.1\n",
      "67 번째 loss, accuracy:  1.0928609604118853 0.1\n",
      "68 번째 loss, accuracy:  1.0927006064611224 0.1\n",
      "69 번째 loss, accuracy:  1.0925403455371991 0.1\n",
      "70 번째 loss, accuracy:  1.0923805878187935 0.1\n",
      "71 번째 loss, accuracy:  1.0922209935456324 0.1\n",
      "72 번째 loss, accuracy:  1.0920616368015912 0.1\n",
      "73 번째 loss, accuracy:  1.0919020902141305 0.10833333333333334\n",
      "74 번째 loss, accuracy:  1.0917427611224515 0.10833333333333334\n",
      "75 번째 loss, accuracy:  1.0915835482920169 0.10833333333333334\n",
      "76 번째 loss, accuracy:  1.091424541079672 0.10833333333333334\n",
      "77 번째 loss, accuracy:  1.0912655872882009 0.10833333333333334\n",
      "78 번째 loss, accuracy:  1.0911061250591818 0.10833333333333334\n",
      "79 번째 loss, accuracy:  1.090946771076642 0.10833333333333334\n",
      "80 번째 loss, accuracy:  1.0907872966518373 0.10833333333333334\n",
      "81 번째 loss, accuracy:  1.0906277200150152 0.10833333333333334\n",
      "82 번째 loss, accuracy:  1.0904679327494347 0.11666666666666667\n",
      "83 번째 loss, accuracy:  1.090307741638161 0.11666666666666667\n",
      "84 번째 loss, accuracy:  1.0901475028669076 0.11666666666666667\n",
      "85 번째 loss, accuracy:  1.0899871363800469 0.11666666666666667\n",
      "86 번째 loss, accuracy:  1.0898265781016467 0.125\n",
      "87 번째 loss, accuracy:  1.0896655335107197 0.125\n",
      "88 번째 loss, accuracy:  1.0895042271814868 0.13333333333333333\n",
      "89 번째 loss, accuracy:  1.089342302920983 0.13333333333333333\n",
      "90 번째 loss, accuracy:  1.0891798395557184 0.14166666666666666\n",
      "91 번째 loss, accuracy:  1.089016598286503 0.14166666666666666\n",
      "92 번째 loss, accuracy:  1.0888530968764423 0.14166666666666666\n",
      "93 번째 loss, accuracy:  1.0886892870339382 0.15\n",
      "94 번째 loss, accuracy:  1.0885244081227483 0.15\n",
      "95 번째 loss, accuracy:  1.0883591633320024 0.15\n",
      "96 번째 loss, accuracy:  1.0881929483281059 0.15\n",
      "97 번째 loss, accuracy:  1.0880260701227242 0.15\n",
      "98 번째 loss, accuracy:  1.087858539461512 0.15833333333333333\n",
      "99 번째 loss, accuracy:  1.0876902055815085 0.15833333333333333\n",
      "100 번째 loss, accuracy:  1.0875209013304004 0.15833333333333333\n",
      "101 번째 loss, accuracy:  1.0873507697783478 0.15833333333333333\n",
      "102 번째 loss, accuracy:  1.0871796982532096 0.15833333333333333\n",
      "103 번째 loss, accuracy:  1.0870074883797434 0.16666666666666666\n",
      "104 번째 loss, accuracy:  1.0868342322072762 0.175\n",
      "105 번째 loss, accuracy:  1.0866603790228935 0.175\n",
      "106 번째 loss, accuracy:  1.0864853017051046 0.175\n",
      "107 번째 loss, accuracy:  1.0863088298433534 0.175\n",
      "108 번째 loss, accuracy:  1.0861312512564363 0.18333333333333332\n",
      "109 번째 loss, accuracy:  1.0859524562106724 0.18333333333333332\n",
      "110 번째 loss, accuracy:  1.0857721823172137 0.18333333333333332\n",
      "111 번째 loss, accuracy:  1.0855908985116898 0.18333333333333332\n",
      "112 번째 loss, accuracy:  1.085408270096751 0.18333333333333332\n",
      "113 번째 loss, accuracy:  1.085224069832614 0.18333333333333332\n",
      "114 번째 loss, accuracy:  1.085038293134878 0.19166666666666668\n",
      "115 번째 loss, accuracy:  1.0848510109324836 0.19166666666666668\n",
      "116 번째 loss, accuracy:  1.0846622696060302 0.2\n",
      "117 번째 loss, accuracy:  1.0844717897866787 0.2\n",
      "118 번째 loss, accuracy:  1.084279744494584 0.2\n",
      "119 번째 loss, accuracy:  1.084085925276174 0.2\n",
      "120 번째 loss, accuracy:  1.0838904561648506 0.2\n",
      "121 번째 loss, accuracy:  1.0836931692964675 0.2\n",
      "122 번째 loss, accuracy:  1.083493939700566 0.2\n",
      "123 번째 loss, accuracy:  1.0832928874195111 0.2\n",
      "124 번째 loss, accuracy:  1.083089906537507 0.2\n",
      "125 번째 loss, accuracy:  1.0828850704544044 0.2\n",
      "126 번째 loss, accuracy:  1.0826777982199074 0.2\n",
      "127 번째 loss, accuracy:  1.0824683810844535 0.2\n",
      "128 번째 loss, accuracy:  1.0822565642434028 0.19166666666666668\n",
      "129 번째 loss, accuracy:  1.0820427281625935 0.19166666666666668\n",
      "130 번째 loss, accuracy:  1.0818266242038874 0.19166666666666668\n",
      "131 번째 loss, accuracy:  1.0816079033294175 0.19166666666666668\n",
      "132 번째 loss, accuracy:  1.0813865376970928 0.19166666666666668\n",
      "133 번째 loss, accuracy:  1.0811628825512065 0.19166666666666668\n",
      "134 번째 loss, accuracy:  1.0809367727650125 0.19166666666666668\n",
      "135 번째 loss, accuracy:  1.0807078571963504 0.19166666666666668\n",
      "136 번째 loss, accuracy:  1.0804762517462279 0.19166666666666668\n",
      "137 번째 loss, accuracy:  1.0802421952835626 0.19166666666666668\n",
      "138 번째 loss, accuracy:  1.0800048229951742 0.19166666666666668\n",
      "139 번째 loss, accuracy:  1.079764723613753 0.19166666666666668\n",
      "140 번째 loss, accuracy:  1.0795215296201268 0.19166666666666668\n",
      "141 번째 loss, accuracy:  1.0792753162649693 0.19166666666666668\n",
      "142 번째 loss, accuracy:  1.0790262371506594 0.19166666666666668\n",
      "143 번째 loss, accuracy:  1.078773679819343 0.2\n",
      "144 번째 loss, accuracy:  1.0785179376502674 0.2\n",
      "145 번째 loss, accuracy:  1.0782585030996092 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 번째 loss, accuracy:  1.0779959468683462 0.2\n",
      "147 번째 loss, accuracy:  1.077729666348362 0.19166666666666668\n",
      "148 번째 loss, accuracy:  1.0774598478577389 0.2\n",
      "149 번째 loss, accuracy:  1.077186123898221 0.2\n",
      "150 번째 loss, accuracy:  1.0769089018811018 0.2\n",
      "151 번째 loss, accuracy:  1.0766278303005818 0.2\n",
      "152 번째 loss, accuracy:  1.0763428196597462 0.2\n",
      "153 번째 loss, accuracy:  1.0760538705088005 0.2\n",
      "154 번째 loss, accuracy:  1.0757606555895585 0.19166666666666668\n",
      "155 번째 loss, accuracy:  1.075463436838672 0.2\n",
      "156 번째 loss, accuracy:  1.0751619442870644 0.23333333333333334\n",
      "157 번째 loss, accuracy:  1.074856233622458 0.23333333333333334\n",
      "158 번째 loss, accuracy:  1.0745460930042265 0.23333333333333334\n",
      "159 번째 loss, accuracy:  1.0742314678490041 0.23333333333333334\n",
      "160 번째 loss, accuracy:  1.0739121434840224 0.24166666666666667\n",
      "161 번째 loss, accuracy:  1.0735880501291353 0.24166666666666667\n",
      "162 번째 loss, accuracy:  1.073259360674883 0.24166666666666667\n",
      "163 번째 loss, accuracy:  1.0729262795533525 0.24166666666666667\n",
      "164 번째 loss, accuracy:  1.072588347126897 0.225\n",
      "165 번째 loss, accuracy:  1.072245789124442 0.24166666666666667\n",
      "166 번째 loss, accuracy:  1.071897392162519 0.25\n",
      "167 번째 loss, accuracy:  1.0715437588979044 0.25833333333333336\n",
      "168 번째 loss, accuracy:  1.0711850989286755 0.25833333333333336\n",
      "169 번째 loss, accuracy:  1.070821439658495 0.26666666666666666\n",
      "170 번째 loss, accuracy:  1.070451914788463 0.275\n",
      "171 번째 loss, accuracy:  1.0700775347883456 0.26666666666666666\n",
      "172 번째 loss, accuracy:  1.069697352177841 0.2833333333333333\n",
      "173 번째 loss, accuracy:  1.0693116547778858 0.2833333333333333\n",
      "174 번째 loss, accuracy:  1.068921206839895 0.2833333333333333\n",
      "175 번째 loss, accuracy:  1.0685242176222578 0.2833333333333333\n",
      "176 번째 loss, accuracy:  1.0681215661311554 0.2833333333333333\n",
      "177 번째 loss, accuracy:  1.067713435818963 0.3\n",
      "178 번째 loss, accuracy:  1.0672994294865485 0.3\n",
      "179 번째 loss, accuracy:  1.0668794045904393 0.30833333333333335\n",
      "180 번째 loss, accuracy:  1.0664549459553607 0.30833333333333335\n",
      "181 번째 loss, accuracy:  1.0660239773694717 0.30833333333333335\n",
      "182 번째 loss, accuracy:  1.0655863876884388 0.30833333333333335\n",
      "183 번째 loss, accuracy:  1.0651428982007736 0.30833333333333335\n",
      "184 번째 loss, accuracy:  1.064693921748109 0.30833333333333335\n",
      "185 번째 loss, accuracy:  1.0642385732443411 0.30833333333333335\n",
      "186 번째 loss, accuracy:  1.0637778490693697 0.30833333333333335\n",
      "187 번째 loss, accuracy:  1.0633107287521242 0.30833333333333335\n",
      "188 번째 loss, accuracy:  1.0628386954567406 0.31666666666666665\n",
      "189 번째 loss, accuracy:  1.0623595729443005 0.325\n",
      "190 번째 loss, accuracy:  1.0618744713470085 0.3333333333333333\n",
      "191 번째 loss, accuracy:  1.0613843729827548 0.325\n",
      "192 번째 loss, accuracy:  1.0608878219567903 0.3333333333333333\n",
      "193 번째 loss, accuracy:  1.060386491580304 0.3333333333333333\n",
      "194 번째 loss, accuracy:  1.0598791920825898 0.3333333333333333\n",
      "195 번째 loss, accuracy:  1.0593672896524147 0.3416666666666667\n",
      "196 번째 loss, accuracy:  1.0588490073134535 0.3416666666666667\n",
      "197 번째 loss, accuracy:  1.058325653621934 0.35\n",
      "198 번째 loss, accuracy:  1.0577968277360783 0.35\n",
      "199 번째 loss, accuracy:  1.0572628642893074 0.36666666666666664\n",
      "200 번째 loss, accuracy:  1.0567240554389137 0.36666666666666664\n",
      "201 번째 loss, accuracy:  1.0561798336485233 0.36666666666666664\n",
      "202 번째 loss, accuracy:  1.0556316527357976 0.375\n",
      "203 번째 loss, accuracy:  1.0550790943687347 0.39166666666666666\n",
      "204 번째 loss, accuracy:  1.054523370603776 0.39166666666666666\n",
      "205 번째 loss, accuracy:  1.0539631768468625 0.39166666666666666\n",
      "206 번째 loss, accuracy:  1.0533969215661918 0.39166666666666666\n",
      "207 번째 loss, accuracy:  1.0528261570169641 0.39166666666666666\n",
      "208 번째 loss, accuracy:  1.0522522593841417 0.39166666666666666\n",
      "209 번째 loss, accuracy:  1.0516747466671352 0.39166666666666666\n",
      "210 번째 loss, accuracy:  1.0510952026045324 0.39166666666666666\n",
      "211 번째 loss, accuracy:  1.0505150379883013 0.39166666666666666\n",
      "212 번째 loss, accuracy:  1.0499273230812998 0.4\n",
      "213 번째 loss, accuracy:  1.049336548654879 0.4\n",
      "214 번째 loss, accuracy:  1.0487437646023807 0.4\n",
      "215 번째 loss, accuracy:  1.0481484481775338 0.4\n",
      "216 번째 loss, accuracy:  1.0475509176768119 0.4083333333333333\n",
      "217 번째 loss, accuracy:  1.0469500350935241 0.4083333333333333\n",
      "218 번째 loss, accuracy:  1.046346898969142 0.4083333333333333\n",
      "219 번째 loss, accuracy:  1.0457446557158638 0.4083333333333333\n",
      "220 번째 loss, accuracy:  1.0451400844018517 0.4083333333333333\n",
      "221 번째 loss, accuracy:  1.0445320466874075 0.4083333333333333\n",
      "222 번째 loss, accuracy:  1.0439241182121664 0.4083333333333333\n",
      "223 번째 loss, accuracy:  1.043314639941357 0.4083333333333333\n",
      "224 번째 loss, accuracy:  1.042702149666932 0.4083333333333333\n",
      "225 번째 loss, accuracy:  1.0420892924913658 0.4083333333333333\n",
      "226 번째 loss, accuracy:  1.0414759689561475 0.4083333333333333\n",
      "227 번째 loss, accuracy:  1.0408610168394057 0.4083333333333333\n",
      "228 번째 loss, accuracy:  1.040247614852121 0.4083333333333333\n",
      "229 번째 loss, accuracy:  1.0396303507319111 0.4083333333333333\n",
      "230 번째 loss, accuracy:  1.0390179223795084 0.4083333333333333\n",
      "231 번째 loss, accuracy:  1.0383997873869906 0.4083333333333333\n",
      "232 번째 loss, accuracy:  1.0377818626339346 0.4083333333333333\n",
      "233 번째 loss, accuracy:  1.0371627874384284 0.4083333333333333\n",
      "234 번째 loss, accuracy:  1.0365461239110922 0.4083333333333333\n",
      "235 번째 loss, accuracy:  1.0359268474732921 0.4083333333333333\n",
      "236 번째 loss, accuracy:  1.035308293885896 0.4083333333333333\n",
      "237 번째 loss, accuracy:  1.0346896248243578 0.4083333333333333\n",
      "238 번째 loss, accuracy:  1.0340686159085668 0.4083333333333333\n",
      "239 번째 loss, accuracy:  1.0334495787792997 0.4083333333333333\n",
      "240 번째 loss, accuracy:  1.0328292816092772 0.4083333333333333\n",
      "241 번째 loss, accuracy:  1.0322084772959152 0.4083333333333333\n",
      "242 번째 loss, accuracy:  1.031587549994352 0.4083333333333333\n",
      "243 번째 loss, accuracy:  1.0309672862254946 0.4083333333333333\n",
      "244 번째 loss, accuracy:  1.0303461392278264 0.4083333333333333\n",
      "245 번째 loss, accuracy:  1.02972667997161 0.4083333333333333\n",
      "246 번째 loss, accuracy:  1.0291081871938725 0.4083333333333333\n",
      "247 번째 loss, accuracy:  1.0284879178292874 0.4083333333333333\n",
      "248 번째 loss, accuracy:  1.0278672334566568 0.4083333333333333\n",
      "249 번째 loss, accuracy:  1.0272459194903376 0.4166666666666667\n",
      "250 번째 loss, accuracy:  1.0266251280662766 0.425\n",
      "251 번째 loss, accuracy:  1.0260044328705493 0.425\n",
      "252 번째 loss, accuracy:  1.0253841530376577 0.425\n",
      "253 번째 loss, accuracy:  1.024763665454265 0.425\n",
      "254 번째 loss, accuracy:  1.02414304089204 0.425\n",
      "255 번째 loss, accuracy:  1.0235213710360556 0.425\n",
      "256 번째 loss, accuracy:  1.0229014211980592 0.425\n",
      "257 번째 loss, accuracy:  1.0222811040721451 0.425\n",
      "258 번째 loss, accuracy:  1.0216598391530258 0.425\n",
      "259 번째 loss, accuracy:  1.0210392881168204 0.43333333333333335\n",
      "260 번째 loss, accuracy:  1.0204178747596595 0.43333333333333335\n",
      "261 번째 loss, accuracy:  1.0197970611890377 0.43333333333333335\n",
      "262 번째 loss, accuracy:  1.0191757601241445 0.45\n",
      "263 번째 loss, accuracy:  1.0185552462769258 0.45\n",
      "264 번째 loss, accuracy:  1.017932827690435 0.45\n",
      "265 번째 loss, accuracy:  1.0173102515658343 0.45\n",
      "266 번째 loss, accuracy:  1.0166875439525618 0.45\n",
      "267 번째 loss, accuracy:  1.0160661965426259 0.4583333333333333\n",
      "268 번째 loss, accuracy:  1.015443274636815 0.4583333333333333\n",
      "269 번째 loss, accuracy:  1.0148212294054642 0.4583333333333333\n",
      "270 번째 loss, accuracy:  1.0141984798207633 0.4583333333333333\n",
      "271 번째 loss, accuracy:  1.0135759535646425 0.4583333333333333\n",
      "272 번째 loss, accuracy:  1.0129534544132548 0.4583333333333333\n",
      "273 번째 loss, accuracy:  1.0123299278425644 0.4583333333333333\n",
      "274 번째 loss, accuracy:  1.0117064986768016 0.4583333333333333\n",
      "275 번째 loss, accuracy:  1.011083237627925 0.475\n",
      "276 번째 loss, accuracy:  1.010459704791651 0.48333333333333334\n",
      "277 번째 loss, accuracy:  1.0098358090295154 0.48333333333333334\n",
      "278 번째 loss, accuracy:  1.009211748750027 0.48333333333333334\n",
      "279 번째 loss, accuracy:  1.0085874047255432 0.49166666666666664\n",
      "280 번째 loss, accuracy:  1.0079629189946444 0.49166666666666664\n",
      "281 번째 loss, accuracy:  1.0073387009685382 0.5\n",
      "282 번째 loss, accuracy:  1.006714333767512 0.5\n",
      "283 번째 loss, accuracy:  1.006089650077659 0.5083333333333333\n",
      "284 번째 loss, accuracy:  1.005464925411249 0.5166666666666667\n",
      "285 번째 loss, accuracy:  1.0048406477925311 0.5166666666666667\n",
      "286 번째 loss, accuracy:  1.0042157281791317 0.5166666666666667\n",
      "287 번째 loss, accuracy:  1.0035906887950783 0.5166666666666667\n",
      "288 번째 loss, accuracy:  1.0029654476600929 0.5166666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289 번째 loss, accuracy:  1.0023406457414694 0.525\n",
      "290 번째 loss, accuracy:  1.001715392122755 0.525\n",
      "291 번째 loss, accuracy:  1.0010900398865654 0.5333333333333333\n",
      "292 번째 loss, accuracy:  1.00046483101623 0.5333333333333333\n",
      "293 번째 loss, accuracy:  0.9998397452257055 0.5333333333333333\n",
      "294 번째 loss, accuracy:  0.999214100301821 0.5333333333333333\n",
      "295 번째 loss, accuracy:  0.998588660968371 0.5416666666666666\n",
      "296 번째 loss, accuracy:  0.9979632410267494 0.5416666666666666\n",
      "297 번째 loss, accuracy:  0.9973374764757457 0.5416666666666666\n",
      "298 번째 loss, accuracy:  0.9967114431712983 0.55\n",
      "299 번째 loss, accuracy:  0.9960855708827019 0.5583333333333333\n",
      "300 번째 loss, accuracy:  0.9954594983833255 0.5583333333333333\n",
      "301 번째 loss, accuracy:  0.9948334349095928 0.5583333333333333\n",
      "302 번째 loss, accuracy:  0.9942074562416946 0.5583333333333333\n",
      "303 번째 loss, accuracy:  0.9935812952479978 0.5583333333333333\n",
      "304 번째 loss, accuracy:  0.9929548985123707 0.5583333333333333\n",
      "305 번째 loss, accuracy:  0.9923282999272774 0.5666666666666667\n",
      "306 번째 loss, accuracy:  0.9917015455626431 0.5666666666666667\n",
      "307 번째 loss, accuracy:  0.9910747471707264 0.5666666666666667\n",
      "308 번째 loss, accuracy:  0.9904481450604032 0.5666666666666667\n",
      "309 번째 loss, accuracy:  0.9898214607649649 0.5666666666666667\n",
      "310 번째 loss, accuracy:  0.9891943781112401 0.5666666666666667\n",
      "311 번째 loss, accuracy:  0.9885672969080272 0.5666666666666667\n",
      "312 번째 loss, accuracy:  0.9879401981186177 0.5666666666666667\n",
      "313 번째 loss, accuracy:  0.9873126693105373 0.5666666666666667\n",
      "314 번째 loss, accuracy:  0.9866851852307524 0.5666666666666667\n",
      "315 번째 loss, accuracy:  0.9860575857120414 0.5666666666666667\n",
      "316 번째 loss, accuracy:  0.9854297876825175 0.5583333333333333\n",
      "317 번째 loss, accuracy:  0.9848017795788764 0.5583333333333333\n",
      "318 번째 loss, accuracy:  0.9841744658465676 0.5583333333333333\n",
      "319 번째 loss, accuracy:  0.9835462093484567 0.5583333333333333\n",
      "320 번째 loss, accuracy:  0.9829172955784055 0.5583333333333333\n",
      "321 번째 loss, accuracy:  0.9822883098952631 0.5583333333333333\n",
      "322 번째 loss, accuracy:  0.9816593927205768 0.5666666666666667\n",
      "323 번째 loss, accuracy:  0.9810299136601164 0.5666666666666667\n",
      "324 번째 loss, accuracy:  0.9804005462808857 0.575\n",
      "325 번째 loss, accuracy:  0.9797712363833623 0.5833333333333334\n",
      "326 번째 loss, accuracy:  0.9791418098543716 0.5833333333333334\n",
      "327 번째 loss, accuracy:  0.9785116581737381 0.5833333333333334\n",
      "328 번째 loss, accuracy:  0.9778811584625637 0.5916666666666667\n",
      "329 번째 loss, accuracy:  0.9772508141540472 0.5916666666666667\n",
      "330 번째 loss, accuracy:  0.9766200373433986 0.5916666666666667\n",
      "331 번째 loss, accuracy:  0.975989321118846 0.5916666666666667\n",
      "332 번째 loss, accuracy:  0.9753581381410982 0.5916666666666667\n",
      "333 번째 loss, accuracy:  0.9747262540171893 0.6083333333333333\n",
      "334 번째 loss, accuracy:  0.9740943874804324 0.6083333333333333\n",
      "335 번째 loss, accuracy:  0.9734620111937897 0.6083333333333333\n",
      "336 번째 loss, accuracy:  0.9728293532217561 0.6083333333333333\n",
      "337 번째 loss, accuracy:  0.9721965747089768 0.6166666666666667\n",
      "338 번째 loss, accuracy:  0.9715636492990891 0.625\n",
      "339 번째 loss, accuracy:  0.9709302847774237 0.625\n",
      "340 번째 loss, accuracy:  0.9702966534956832 0.625\n",
      "341 번째 loss, accuracy:  0.9696628374953096 0.625\n",
      "342 번째 loss, accuracy:  0.9690286202702036 0.625\n",
      "343 번째 loss, accuracy:  0.9683940172377253 0.6333333333333333\n",
      "344 번째 loss, accuracy:  0.9677589839269944 0.6333333333333333\n",
      "345 번째 loss, accuracy:  0.9671241470113682 0.6416666666666667\n",
      "346 번째 loss, accuracy:  0.9664890231982334 0.6416666666666667\n",
      "347 번째 loss, accuracy:  0.9658534035833649 0.6416666666666667\n",
      "348 번째 loss, accuracy:  0.9652172844909849 0.6416666666666667\n",
      "349 번째 loss, accuracy:  0.9645807643253625 0.6416666666666667\n",
      "350 번째 loss, accuracy:  0.9639439386080364 0.6416666666666667\n",
      "351 번째 loss, accuracy:  0.9633070151669162 0.6416666666666667\n",
      "352 번째 loss, accuracy:  0.9626692291769084 0.6416666666666667\n",
      "353 번째 loss, accuracy:  0.9620313842279162 0.6416666666666667\n",
      "354 번째 loss, accuracy:  0.9613932171670714 0.6416666666666667\n",
      "355 번째 loss, accuracy:  0.9607547372667755 0.6416666666666667\n",
      "356 번째 loss, accuracy:  0.9601159989936892 0.6416666666666667\n",
      "357 번째 loss, accuracy:  0.9594769635254193 0.6416666666666667\n",
      "358 번째 loss, accuracy:  0.9588376388903115 0.6416666666666667\n",
      "359 번째 loss, accuracy:  0.9581980518086203 0.6416666666666667\n",
      "360 번째 loss, accuracy:  0.957557477807012 0.6416666666666667\n",
      "361 번째 loss, accuracy:  0.9569172765588073 0.6416666666666667\n",
      "362 번째 loss, accuracy:  0.9562765032278636 0.6416666666666667\n",
      "363 번째 loss, accuracy:  0.9556349448413008 0.6416666666666667\n",
      "364 번째 loss, accuracy:  0.9549933846152306 0.6416666666666667\n",
      "365 번째 loss, accuracy:  0.9543513707682226 0.6416666666666667\n",
      "366 번째 loss, accuracy:  0.9537090991545415 0.6416666666666667\n",
      "367 번째 loss, accuracy:  0.953066886064525 0.6416666666666667\n",
      "368 번째 loss, accuracy:  0.952424593647584 0.6416666666666667\n",
      "369 번째 loss, accuracy:  0.9517820423229506 0.6416666666666667\n",
      "370 번째 loss, accuracy:  0.9511385635377323 0.6416666666666667\n",
      "371 번째 loss, accuracy:  0.9504951658505528 0.6416666666666667\n",
      "372 번째 loss, accuracy:  0.9498509920861101 0.6416666666666667\n",
      "373 번째 loss, accuracy:  0.9492071941938884 0.6416666666666667\n",
      "374 번째 loss, accuracy:  0.9485630753646244 0.6416666666666667\n",
      "375 번째 loss, accuracy:  0.9479187957356164 0.6416666666666667\n",
      "376 번째 loss, accuracy:  0.9472739081884675 0.6416666666666667\n",
      "377 번째 loss, accuracy:  0.9466284592128537 0.6416666666666667\n",
      "378 번째 loss, accuracy:  0.9459834509904343 0.6416666666666667\n",
      "379 번째 loss, accuracy:  0.9453383593574515 0.6416666666666667\n",
      "380 번째 loss, accuracy:  0.9446923823949772 0.6416666666666667\n",
      "381 번째 loss, accuracy:  0.9440470946338488 0.6416666666666667\n",
      "382 번째 loss, accuracy:  0.9434018629209985 0.6416666666666667\n",
      "383 번째 loss, accuracy:  0.9427557523845901 0.6416666666666667\n",
      "384 번째 loss, accuracy:  0.9421093674507512 0.6416666666666667\n",
      "385 번째 loss, accuracy:  0.9414629598522498 0.6416666666666667\n",
      "386 번째 loss, accuracy:  0.9408167304888476 0.6416666666666667\n",
      "387 번째 loss, accuracy:  0.9401703952356454 0.6416666666666667\n",
      "388 번째 loss, accuracy:  0.9395238811269496 0.6416666666666667\n",
      "389 번째 loss, accuracy:  0.9388770507154205 0.6416666666666667\n",
      "390 번째 loss, accuracy:  0.9382299129225967 0.6416666666666667\n",
      "391 번째 loss, accuracy:  0.9375828496888995 0.6416666666666667\n",
      "392 번째 loss, accuracy:  0.9369359311266687 0.6416666666666667\n",
      "393 번째 loss, accuracy:  0.9362889193696087 0.6416666666666667\n",
      "394 번째 loss, accuracy:  0.9356417263453127 0.65\n",
      "395 번째 loss, accuracy:  0.93499502653944 0.65\n",
      "396 번째 loss, accuracy:  0.9343481558092503 0.65\n",
      "397 번째 loss, accuracy:  0.933701580934279 0.65\n",
      "398 번째 loss, accuracy:  0.9330550810383845 0.65\n",
      "399 번째 loss, accuracy:  0.932408184850927 0.65\n",
      "400 번째 loss, accuracy:  0.9317613480775268 0.65\n",
      "401 번째 loss, accuracy:  0.9311148066623534 0.65\n",
      "402 번째 loss, accuracy:  0.9304682852105585 0.65\n",
      "403 번째 loss, accuracy:  0.9298215422857271 0.6416666666666667\n",
      "404 번째 loss, accuracy:  0.929175224094985 0.6416666666666667\n",
      "405 번째 loss, accuracy:  0.9285291896188499 0.6416666666666667\n",
      "406 번째 loss, accuracy:  0.927883247984018 0.6416666666666667\n",
      "407 번째 loss, accuracy:  0.927236945182832 0.6416666666666667\n",
      "408 번째 loss, accuracy:  0.9265911008983776 0.6416666666666667\n",
      "409 번째 loss, accuracy:  0.9259458163974652 0.6416666666666667\n",
      "410 번째 loss, accuracy:  0.9253005463421362 0.6416666666666667\n",
      "411 번째 loss, accuracy:  0.9246558111884788 0.6416666666666667\n",
      "412 번째 loss, accuracy:  0.9240104513577557 0.6416666666666667\n",
      "413 번째 loss, accuracy:  0.9233662254219775 0.6416666666666667\n",
      "414 번째 loss, accuracy:  0.9227223332678272 0.6416666666666667\n",
      "415 번째 loss, accuracy:  0.922077949484178 0.6416666666666667\n",
      "416 번째 loss, accuracy:  0.9214341991615494 0.6416666666666667\n",
      "417 번째 loss, accuracy:  0.9207907305540631 0.6416666666666667\n",
      "418 번째 loss, accuracy:  0.9201469073745305 0.6416666666666667\n",
      "419 번째 loss, accuracy:  0.919504129321773 0.6416666666666667\n",
      "420 번째 loss, accuracy:  0.9188618448596064 0.6416666666666667\n",
      "421 번째 loss, accuracy:  0.918220098665102 0.6416666666666667\n",
      "422 번째 loss, accuracy:  0.9175781882547525 0.6416666666666667\n",
      "423 번째 loss, accuracy:  0.9169366761802638 0.6416666666666667\n",
      "424 번째 loss, accuracy:  0.9162956849898714 0.6416666666666667\n",
      "425 번째 loss, accuracy:  0.9156549227701909 0.6416666666666667\n",
      "426 번째 loss, accuracy:  0.9150150552446003 0.6416666666666667\n",
      "427 번째 loss, accuracy:  0.9143754589288023 0.6416666666666667\n",
      "428 번째 loss, accuracy:  0.9137363429277844 0.6416666666666667\n",
      "429 번째 loss, accuracy:  0.9130975911214757 0.6416666666666667\n",
      "430 번째 loss, accuracy:  0.9124592127332609 0.6416666666666667\n",
      "431 번째 loss, accuracy:  0.9118207461356441 0.6416666666666667\n",
      "432 번째 loss, accuracy:  0.9111831672234753 0.6416666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433 번째 loss, accuracy:  0.9105460188514836 0.6416666666666667\n",
      "434 번째 loss, accuracy:  0.9099096197797018 0.6333333333333333\n",
      "435 번째 loss, accuracy:  0.9092733935205939 0.6333333333333333\n",
      "436 번째 loss, accuracy:  0.9086375822264432 0.6333333333333333\n",
      "437 번째 loss, accuracy:  0.9080023423117941 0.6333333333333333\n",
      "438 번째 loss, accuracy:  0.9073674510149073 0.6333333333333333\n",
      "439 번째 loss, accuracy:  0.9067334281378675 0.6333333333333333\n",
      "440 번째 loss, accuracy:  0.906100160801765 0.6333333333333333\n",
      "441 번째 loss, accuracy:  0.905467258453917 0.6333333333333333\n",
      "442 번째 loss, accuracy:  0.9048346847636303 0.6333333333333333\n",
      "443 번째 loss, accuracy:  0.9042032085093804 0.6333333333333333\n",
      "444 번째 loss, accuracy:  0.9035719057531266 0.6333333333333333\n",
      "445 번째 loss, accuracy:  0.9029409571135977 0.6333333333333333\n",
      "446 번째 loss, accuracy:  0.9023106874208859 0.6333333333333333\n",
      "447 번째 loss, accuracy:  0.9016815030676104 0.6333333333333333\n",
      "448 번째 loss, accuracy:  0.9010526284340775 0.6333333333333333\n",
      "449 번째 loss, accuracy:  0.9004242622529107 0.6333333333333333\n",
      "450 번째 loss, accuracy:  0.8997965721827473 0.6333333333333333\n",
      "451 번째 loss, accuracy:  0.8991697421025254 0.6333333333333333\n",
      "452 번째 loss, accuracy:  0.8985436605131037 0.6333333333333333\n",
      "453 번째 loss, accuracy:  0.8979176124339293 0.6333333333333333\n",
      "454 번째 loss, accuracy:  0.89729236055886 0.6333333333333333\n",
      "455 번째 loss, accuracy:  0.8966680483051876 0.6333333333333333\n",
      "456 번째 loss, accuracy:  0.8960443721603698 0.6333333333333333\n",
      "457 번째 loss, accuracy:  0.895421079820349 0.6333333333333333\n",
      "458 번째 loss, accuracy:  0.8947984324484847 0.6333333333333333\n",
      "459 번째 loss, accuracy:  0.8941763126156045 0.6333333333333333\n",
      "460 번째 loss, accuracy:  0.8935549049858279 0.6333333333333333\n",
      "461 번째 loss, accuracy:  0.892934163804281 0.6333333333333333\n",
      "462 번째 loss, accuracy:  0.892314213008722 0.6333333333333333\n",
      "463 번째 loss, accuracy:  0.8916947116348996 0.6333333333333333\n",
      "464 번째 loss, accuracy:  0.8910762267744469 0.6333333333333333\n",
      "465 번째 loss, accuracy:  0.890458227693671 0.6333333333333333\n",
      "466 번째 loss, accuracy:  0.8898409560359396 0.6333333333333333\n",
      "467 번째 loss, accuracy:  0.8892246307503145 0.6333333333333333\n",
      "468 번째 loss, accuracy:  0.8886087015888756 0.6333333333333333\n",
      "469 번째 loss, accuracy:  0.8879934833346118 0.6333333333333333\n",
      "470 번째 loss, accuracy:  0.8873790008397672 0.6333333333333333\n",
      "471 번째 loss, accuracy:  0.8867655863921076 0.6333333333333333\n",
      "472 번째 loss, accuracy:  0.8861524953820779 0.6333333333333333\n",
      "473 번째 loss, accuracy:  0.8855397988081412 0.6333333333333333\n",
      "474 번째 loss, accuracy:  0.8849278752856029 0.6333333333333333\n",
      "475 번째 loss, accuracy:  0.8843167052809016 0.6333333333333333\n",
      "476 번째 loss, accuracy:  0.8837064988365639 0.6333333333333333\n",
      "477 번째 loss, accuracy:  0.8830969164681133 0.6333333333333333\n",
      "478 번째 loss, accuracy:  0.882487766795295 0.6333333333333333\n",
      "479 번째 loss, accuracy:  0.8818796696726287 0.6333333333333333\n",
      "480 번째 loss, accuracy:  0.8812721362737806 0.6333333333333333\n",
      "481 번째 loss, accuracy:  0.8806650683362013 0.6333333333333333\n",
      "482 번째 loss, accuracy:  0.8800588715000198 0.6333333333333333\n",
      "483 번째 loss, accuracy:  0.879452947632363 0.625\n",
      "484 번째 loss, accuracy:  0.878848149998525 0.625\n",
      "485 번째 loss, accuracy:  0.8782442324069134 0.625\n",
      "486 번째 loss, accuracy:  0.8776407308596175 0.625\n",
      "487 번째 loss, accuracy:  0.8770380854075995 0.625\n",
      "488 번째 loss, accuracy:  0.8764358945484733 0.625\n",
      "489 번째 loss, accuracy:  0.8758345731370399 0.625\n",
      "490 번째 loss, accuracy:  0.8752337646015397 0.625\n",
      "491 번째 loss, accuracy:  0.8746335409274778 0.625\n",
      "492 번째 loss, accuracy:  0.8740343369323262 0.625\n",
      "493 번째 loss, accuracy:  0.8734356797560866 0.625\n",
      "494 번째 loss, accuracy:  0.8728376538150938 0.625\n",
      "495 번째 loss, accuracy:  0.872240352806113 0.625\n",
      "496 번째 loss, accuracy:  0.8716438339661396 0.625\n",
      "497 번째 loss, accuracy:  0.8710479619424761 0.625\n",
      "498 번째 loss, accuracy:  0.8704524939555492 0.625\n",
      "499 번째 loss, accuracy:  0.8698578306415613 0.625\n",
      "500 번째 loss, accuracy:  0.8692637489553804 0.625\n",
      "501 번째 loss, accuracy:  0.8686704129697389 0.625\n",
      "502 번째 loss, accuracy:  0.8680779584655905 0.625\n",
      "503 번째 loss, accuracy:  0.8674861830580319 0.625\n",
      "504 번째 loss, accuracy:  0.8668951033537464 0.625\n",
      "505 번째 loss, accuracy:  0.8663047004852246 0.625\n",
      "506 번째 loss, accuracy:  0.8657148010752387 0.625\n",
      "507 번째 loss, accuracy:  0.865125586443911 0.625\n",
      "508 번째 loss, accuracy:  0.8645371844241482 0.625\n",
      "509 번째 loss, accuracy:  0.8639492327445782 0.625\n",
      "510 번째 loss, accuracy:  0.8633621846343736 0.625\n",
      "511 번째 loss, accuracy:  0.8627753558795774 0.625\n",
      "512 번째 loss, accuracy:  0.8621893373022494 0.625\n",
      "513 번째 loss, accuracy:  0.8616042514167687 0.625\n",
      "514 번째 loss, accuracy:  0.8610196496680298 0.625\n",
      "515 번째 loss, accuracy:  0.8604357190499944 0.625\n",
      "516 번째 loss, accuracy:  0.8598526401881575 0.625\n",
      "517 번째 loss, accuracy:  0.8592701091460331 0.625\n",
      "518 번째 loss, accuracy:  0.8586881793019002 0.625\n",
      "519 번째 loss, accuracy:  0.8581067864919592 0.6166666666666667\n",
      "520 번째 loss, accuracy:  0.8575259231835181 0.6166666666666667\n",
      "521 번째 loss, accuracy:  0.8569459620482403 0.6166666666666667\n",
      "522 번째 loss, accuracy:  0.8563667377213222 0.6166666666666667\n",
      "523 번째 loss, accuracy:  0.8557880227516653 0.6166666666666667\n",
      "524 번째 loss, accuracy:  0.855209962956187 0.6166666666666667\n",
      "525 번째 loss, accuracy:  0.854632296650166 0.6166666666666667\n",
      "526 번째 loss, accuracy:  0.8540555050042936 0.6166666666666667\n",
      "527 번째 loss, accuracy:  0.853479308601568 0.6166666666666667\n",
      "528 번째 loss, accuracy:  0.8529036901895087 0.6166666666666667\n",
      "529 번째 loss, accuracy:  0.8523285789947473 0.6166666666666667\n",
      "530 번째 loss, accuracy:  0.8517539346656956 0.6166666666666667\n",
      "531 번째 loss, accuracy:  0.8511802629045725 0.6166666666666667\n",
      "532 번째 loss, accuracy:  0.8506073530020141 0.6166666666666667\n",
      "533 번째 loss, accuracy:  0.8500350653129407 0.6166666666666667\n",
      "534 번째 loss, accuracy:  0.8494634688381334 0.6166666666666667\n",
      "535 번째 loss, accuracy:  0.8488923424770408 0.6166666666666667\n",
      "536 번째 loss, accuracy:  0.8483219409555807 0.6166666666666667\n",
      "537 번째 loss, accuracy:  0.8477521189316798 0.6166666666666667\n",
      "538 번째 loss, accuracy:  0.8471829681081535 0.6166666666666667\n",
      "539 번째 loss, accuracy:  0.846614190041356 0.6166666666666667\n",
      "540 번째 loss, accuracy:  0.8460463860001087 0.6166666666666667\n",
      "541 번째 loss, accuracy:  0.8454791409247433 0.6166666666666667\n",
      "542 번째 loss, accuracy:  0.8449124364166811 0.6166666666666667\n",
      "543 번째 loss, accuracy:  0.8443464385402777 0.6166666666666667\n",
      "544 번째 loss, accuracy:  0.8437805097093547 0.6166666666666667\n",
      "545 번째 loss, accuracy:  0.8432156116266112 0.6166666666666667\n",
      "546 번째 loss, accuracy:  0.8426514735793039 0.6166666666666667\n",
      "547 번째 loss, accuracy:  0.8420877261081571 0.6166666666666667\n",
      "548 번째 loss, accuracy:  0.8415246924531182 0.6166666666666667\n",
      "549 번째 loss, accuracy:  0.8409621740467987 0.6166666666666667\n",
      "550 번째 loss, accuracy:  0.8404004385826397 0.6166666666666667\n",
      "551 번째 loss, accuracy:  0.8398393165331683 0.6166666666666667\n",
      "552 번째 loss, accuracy:  0.8392786132678933 0.6166666666666667\n",
      "553 번째 loss, accuracy:  0.8387186992834315 0.6166666666666667\n",
      "554 번째 loss, accuracy:  0.8381594162128129 0.6166666666666667\n",
      "555 번째 loss, accuracy:  0.8376007310936089 0.6166666666666667\n",
      "556 번째 loss, accuracy:  0.8370426319178986 0.6166666666666667\n",
      "557 번째 loss, accuracy:  0.8364851765893672 0.6166666666666667\n",
      "558 번째 loss, accuracy:  0.8359283014053536 0.6166666666666667\n",
      "559 번째 loss, accuracy:  0.8353721219683332 0.6166666666666667\n",
      "560 번째 loss, accuracy:  0.8348164134511521 0.6166666666666667\n",
      "561 번째 loss, accuracy:  0.8342612233870104 0.6166666666666667\n",
      "562 번째 loss, accuracy:  0.8337067895889217 0.6166666666666667\n",
      "563 번째 loss, accuracy:  0.8331530066327981 0.6166666666666667\n",
      "564 번째 loss, accuracy:  0.8325998442386485 0.6166666666666667\n",
      "565 번째 loss, accuracy:  0.8320472608799935 0.6166666666666667\n",
      "566 번째 loss, accuracy:  0.8314949052221153 0.6166666666666667\n",
      "567 번째 loss, accuracy:  0.830943175781494 0.6166666666666667\n",
      "568 번째 loss, accuracy:  0.8303923917708876 0.6166666666666667\n",
      "569 번째 loss, accuracy:  0.8298421175920683 0.6083333333333333\n",
      "570 번째 loss, accuracy:  0.8292923191222059 0.6083333333333333\n",
      "571 번째 loss, accuracy:  0.8287433853502756 0.6083333333333333\n",
      "572 번째 loss, accuracy:  0.8281949613590276 0.6083333333333333\n",
      "573 번째 loss, accuracy:  0.8276471951051952 0.6083333333333333\n",
      "574 번째 loss, accuracy:  0.8271000083226615 0.6083333333333333\n",
      "575 번째 loss, accuracy:  0.8265534249570123 0.6083333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576 번째 loss, accuracy:  0.8260074523978819 0.6083333333333333\n",
      "577 번째 loss, accuracy:  0.8254621099012808 0.6083333333333333\n",
      "578 번째 loss, accuracy:  0.82491716745027 0.6083333333333333\n",
      "579 번째 loss, accuracy:  0.8243726991813447 0.6083333333333333\n",
      "580 번째 loss, accuracy:  0.823829119212312 0.6083333333333333\n",
      "581 번째 loss, accuracy:  0.8232861948069174 0.6083333333333333\n",
      "582 번째 loss, accuracy:  0.8227436692604461 0.6083333333333333\n",
      "583 번째 loss, accuracy:  0.8222019300838767 0.6083333333333333\n",
      "584 번째 loss, accuracy:  0.8216606882390468 0.6083333333333333\n",
      "585 번째 loss, accuracy:  0.821120142742527 0.6083333333333333\n",
      "586 번째 loss, accuracy:  0.8205799167092054 0.6083333333333333\n",
      "587 번째 loss, accuracy:  0.8200406406644108 0.6083333333333333\n",
      "588 번째 loss, accuracy:  0.8195017747153107 0.6083333333333333\n",
      "589 번째 loss, accuracy:  0.8189636260210107 0.6083333333333333\n",
      "590 번째 loss, accuracy:  0.8184260631631245 0.6083333333333333\n",
      "591 번째 loss, accuracy:  0.8178891059716864 0.6083333333333333\n",
      "592 번째 loss, accuracy:  0.8173526627829737 0.6083333333333333\n",
      "593 번째 loss, accuracy:  0.816816767982275 0.6083333333333333\n",
      "594 번째 loss, accuracy:  0.8162815710510429 0.6083333333333333\n",
      "595 번째 loss, accuracy:  0.8157470980890126 0.6083333333333333\n",
      "596 번째 loss, accuracy:  0.815213187609267 0.6083333333333333\n",
      "597 번째 loss, accuracy:  0.8146798867103857 0.6083333333333333\n",
      "598 번째 loss, accuracy:  0.8141471270940047 0.6083333333333333\n",
      "599 번째 loss, accuracy:  0.8136149945379556 0.6083333333333333\n",
      "600 번째 loss, accuracy:  0.8130835342448517 0.6083333333333333\n",
      "601 번째 loss, accuracy:  0.8125525340759341 0.6083333333333333\n",
      "602 번째 loss, accuracy:  0.8120221801832368 0.6083333333333333\n",
      "603 번째 loss, accuracy:  0.8114925801054305 0.6083333333333333\n",
      "604 번째 loss, accuracy:  0.8109635551027414 0.6083333333333333\n",
      "605 번째 loss, accuracy:  0.8104346179644278 0.6083333333333333\n",
      "606 번째 loss, accuracy:  0.8099066085411467 0.6083333333333333\n",
      "607 번째 loss, accuracy:  0.8093792139178502 0.6083333333333333\n",
      "608 번째 loss, accuracy:  0.8088524328010076 0.6083333333333333\n",
      "609 번째 loss, accuracy:  0.8083263297264599 0.6083333333333333\n",
      "610 번째 loss, accuracy:  0.8078009053494702 0.6083333333333333\n",
      "611 번째 loss, accuracy:  0.8072760286669385 0.6083333333333333\n",
      "612 번째 loss, accuracy:  0.8067518930717663 0.6083333333333333\n",
      "613 번째 loss, accuracy:  0.806228327558866 0.6083333333333333\n",
      "614 번째 loss, accuracy:  0.8057052595773416 0.6083333333333333\n",
      "615 번째 loss, accuracy:  0.805182938178442 0.6083333333333333\n",
      "616 번째 loss, accuracy:  0.8046609035708463 0.6083333333333333\n",
      "617 번째 loss, accuracy:  0.8041398836722917 0.6083333333333333\n",
      "618 번째 loss, accuracy:  0.803619313011476 0.6083333333333333\n",
      "619 번째 loss, accuracy:  0.8030994680233434 0.6083333333333333\n",
      "620 번째 loss, accuracy:  0.8025802022384961 0.6083333333333333\n",
      "621 번째 loss, accuracy:  0.8020614615942782 0.6083333333333333\n",
      "622 번째 loss, accuracy:  0.8015435137881325 0.6083333333333333\n",
      "623 번째 loss, accuracy:  0.8010260908480785 0.6083333333333333\n",
      "624 번째 loss, accuracy:  0.8005092346675108 0.6083333333333333\n",
      "625 번째 loss, accuracy:  0.7999930784231388 0.6083333333333333\n",
      "626 번째 loss, accuracy:  0.799477494470187 0.6083333333333333\n",
      "627 번째 loss, accuracy:  0.7989625167156246 0.6083333333333333\n",
      "628 번째 loss, accuracy:  0.798448257323361 0.6083333333333333\n",
      "629 번째 loss, accuracy:  0.7979345422812509 0.6083333333333333\n",
      "630 번째 loss, accuracy:  0.7974215596583385 0.6083333333333333\n",
      "631 번째 loss, accuracy:  0.7969091551053494 0.6083333333333333\n",
      "632 번째 loss, accuracy:  0.7963973065028517 0.6083333333333333\n",
      "633 번째 loss, accuracy:  0.7958858658525269 0.6083333333333333\n",
      "634 번째 loss, accuracy:  0.7953753674892088 0.6083333333333333\n",
      "635 번째 loss, accuracy:  0.7948655007966315 0.6083333333333333\n",
      "636 번째 loss, accuracy:  0.7943562138955254 0.6083333333333333\n",
      "637 번째 loss, accuracy:  0.7938476232740033 0.6083333333333333\n",
      "638 번째 loss, accuracy:  0.7933394123437304 0.6083333333333333\n",
      "639 번째 loss, accuracy:  0.7928318183192107 0.6083333333333333\n",
      "640 번째 loss, accuracy:  0.7923250777997553 0.6083333333333333\n",
      "641 번째 loss, accuracy:  0.7918190119206453 0.6083333333333333\n",
      "642 번째 loss, accuracy:  0.7913135283614835 0.6083333333333333\n",
      "643 번째 loss, accuracy:  0.7908087076593591 0.6083333333333333\n",
      "644 번째 loss, accuracy:  0.7903045276557948 0.6083333333333333\n",
      "645 번째 loss, accuracy:  0.7898007465695625 0.6083333333333333\n",
      "646 번째 loss, accuracy:  0.7892977895372363 0.6083333333333333\n",
      "647 번째 loss, accuracy:  0.7887953797924983 0.6083333333333333\n",
      "648 번째 loss, accuracy:  0.7882936288844585 0.6083333333333333\n",
      "649 번째 loss, accuracy:  0.787792566527732 0.6083333333333333\n",
      "650 번째 loss, accuracy:  0.7872921147933032 0.6083333333333333\n",
      "651 번째 loss, accuracy:  0.7867921819833031 0.6083333333333333\n",
      "652 번째 loss, accuracy:  0.7862930496928555 0.6083333333333333\n",
      "653 번째 loss, accuracy:  0.7857945007332072 0.6083333333333333\n",
      "654 번째 loss, accuracy:  0.7852966205733549 0.6083333333333333\n",
      "655 번째 loss, accuracy:  0.7847990729003576 0.6083333333333333\n",
      "656 번째 loss, accuracy:  0.784302434842657 0.6083333333333333\n",
      "657 번째 loss, accuracy:  0.7838064785642244 0.6083333333333333\n",
      "658 번째 loss, accuracy:  0.7833111018534574 0.6083333333333333\n",
      "659 번째 loss, accuracy:  0.782816266510587 0.6083333333333333\n",
      "660 번째 loss, accuracy:  0.7823222452297418 0.6083333333333333\n",
      "661 번째 loss, accuracy:  0.781828885351691 0.6083333333333333\n",
      "662 번째 loss, accuracy:  0.7813361290764306 0.6083333333333333\n",
      "663 번째 loss, accuracy:  0.7808440485969524 0.6083333333333333\n",
      "664 번째 loss, accuracy:  0.7803525745740214 0.6083333333333333\n",
      "665 번째 loss, accuracy:  0.779861770092891 0.6083333333333333\n",
      "666 번째 loss, accuracy:  0.7793716106725461 0.6083333333333333\n",
      "667 번째 loss, accuracy:  0.7788819564742283 0.6083333333333333\n",
      "668 번째 loss, accuracy:  0.7783930233475933 0.6083333333333333\n",
      "669 번째 loss, accuracy:  0.7779047641661022 0.6083333333333333\n",
      "670 번째 loss, accuracy:  0.7774171493452852 0.6083333333333333\n",
      "671 번째 loss, accuracy:  0.776930199135968 0.6083333333333333\n",
      "672 번째 loss, accuracy:  0.7764439393769046 0.6083333333333333\n",
      "673 번째 loss, accuracy:  0.7759583110469735 0.6083333333333333\n",
      "674 번째 loss, accuracy:  0.77547331327056 0.6083333333333333\n",
      "675 번째 loss, accuracy:  0.7749888198244516 0.6083333333333333\n",
      "676 번째 loss, accuracy:  0.7745051842030046 0.6083333333333333\n",
      "677 번째 loss, accuracy:  0.7740221743843843 0.6083333333333333\n",
      "678 번째 loss, accuracy:  0.7735397045655283 0.6083333333333333\n",
      "679 번째 loss, accuracy:  0.7730578869551342 0.6083333333333333\n",
      "680 번째 loss, accuracy:  0.7725768360812352 0.6083333333333333\n",
      "681 번째 loss, accuracy:  0.7720964251177718 0.6083333333333333\n",
      "682 번째 loss, accuracy:  0.7716166397604304 0.6083333333333333\n",
      "683 번째 loss, accuracy:  0.7711375082520899 0.6083333333333333\n",
      "684 번째 loss, accuracy:  0.7706590632404438 0.6083333333333333\n",
      "685 번째 loss, accuracy:  0.7701811822263226 0.6083333333333333\n",
      "686 번째 loss, accuracy:  0.769703959731856 0.6083333333333333\n",
      "687 번째 loss, accuracy:  0.7692273195619461 0.6083333333333333\n",
      "688 번째 loss, accuracy:  0.7687514748399098 0.6083333333333333\n",
      "689 번째 loss, accuracy:  0.7682762507807811 0.6083333333333333\n",
      "690 번째 loss, accuracy:  0.7678017049566035 0.6083333333333333\n",
      "691 번째 loss, accuracy:  0.7673278024781642 0.6083333333333333\n",
      "692 번째 loss, accuracy:  0.766854493706527 0.6083333333333333\n",
      "693 번째 loss, accuracy:  0.7663819451904411 0.6083333333333333\n",
      "694 번째 loss, accuracy:  0.7659100845564363 0.6083333333333333\n",
      "695 번째 loss, accuracy:  0.7654388785438588 0.6083333333333333\n",
      "696 번째 loss, accuracy:  0.7649682146064734 0.6083333333333333\n",
      "697 번째 loss, accuracy:  0.7644982971068683 0.6083333333333333\n",
      "698 번째 loss, accuracy:  0.7640289687853873 0.6083333333333333\n",
      "699 번째 loss, accuracy:  0.7635602247601211 0.6083333333333333\n",
      "700 번째 loss, accuracy:  0.7630923131774544 0.6083333333333333\n",
      "701 번째 loss, accuracy:  0.7626250895227652 0.6083333333333333\n",
      "702 번째 loss, accuracy:  0.7621585160957735 0.6083333333333333\n",
      "703 번째 loss, accuracy:  0.7616926136819367 0.6083333333333333\n",
      "704 번째 loss, accuracy:  0.7612273294408084 0.6083333333333333\n",
      "705 번째 loss, accuracy:  0.76076270253306 0.6083333333333333\n",
      "706 번째 loss, accuracy:  0.7602987287121086 0.6083333333333333\n",
      "707 번째 loss, accuracy:  0.759835278672958 0.6083333333333333\n",
      "708 번째 loss, accuracy:  0.7593725650870592 0.6083333333333333\n",
      "709 번째 loss, accuracy:  0.7589102771387156 0.6083333333333333\n",
      "710 번째 loss, accuracy:  0.7584489992907345 0.6083333333333333\n",
      "711 번째 loss, accuracy:  0.7579883517630852 0.6083333333333333\n",
      "712 번째 loss, accuracy:  0.7575283805120295 0.6083333333333333\n",
      "713 번째 loss, accuracy:  0.7570690833576996 0.6083333333333333\n",
      "714 번째 loss, accuracy:  0.7566104628165969 0.6083333333333333\n",
      "715 번째 loss, accuracy:  0.7561524995192194 0.6083333333333333\n",
      "716 번째 loss, accuracy:  0.7556951656642303 0.6083333333333333\n",
      "717 번째 loss, accuracy:  0.7552385840506183 0.6083333333333333\n",
      "718 번째 loss, accuracy:  0.7547826469022547 0.6083333333333333\n",
      "719 번째 loss, accuracy:  0.7543273313198547 0.6083333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 번째 loss, accuracy:  0.7538726421831353 0.6083333333333333\n",
      "721 번째 loss, accuracy:  0.7534186713359503 0.6083333333333333\n",
      "722 번째 loss, accuracy:  0.7529653252677647 0.6083333333333333\n",
      "723 번째 loss, accuracy:  0.752512713242628 0.6083333333333333\n",
      "724 번째 loss, accuracy:  0.7520607776618297 0.6083333333333333\n",
      "725 번째 loss, accuracy:  0.7516094723505764 0.6083333333333333\n",
      "726 번째 loss, accuracy:  0.7511588475264148 0.6083333333333333\n",
      "727 번째 loss, accuracy:  0.7507088582731143 0.6083333333333333\n",
      "728 번째 loss, accuracy:  0.7502595648291165 0.6083333333333333\n",
      "729 번째 loss, accuracy:  0.7498107884137838 0.6083333333333333\n",
      "730 번째 loss, accuracy:  0.7493628508968274 0.6083333333333333\n",
      "731 번째 loss, accuracy:  0.7489155384930882 0.6083333333333333\n",
      "732 번째 loss, accuracy:  0.7484688120137297 0.6083333333333333\n",
      "733 번째 loss, accuracy:  0.7480228384875335 0.6083333333333333\n",
      "734 번째 loss, accuracy:  0.7475775532306467 0.6083333333333333\n",
      "735 번째 loss, accuracy:  0.7471329587960513 0.6083333333333333\n",
      "736 번째 loss, accuracy:  0.7466889784669639 0.6083333333333333\n",
      "737 번째 loss, accuracy:  0.7462457655843389 0.6083333333333333\n",
      "738 번째 loss, accuracy:  0.745803181597531 0.6083333333333333\n",
      "739 번째 loss, accuracy:  0.7453612215118152 0.6083333333333333\n",
      "740 번째 loss, accuracy:  0.7449199663265179 0.6083333333333333\n",
      "741 번째 loss, accuracy:  0.7444794350750393 0.6083333333333333\n",
      "742 번째 loss, accuracy:  0.7440395179294532 0.6083333333333333\n",
      "743 번째 loss, accuracy:  0.743600314391088 0.6083333333333333\n",
      "744 번째 loss, accuracy:  0.743161745994396 0.6083333333333333\n",
      "745 번째 loss, accuracy:  0.7427238388097702 0.6083333333333333\n",
      "746 번째 loss, accuracy:  0.742286536062128 0.6083333333333333\n",
      "747 번째 loss, accuracy:  0.7418498300185933 0.6083333333333333\n",
      "748 번째 loss, accuracy:  0.7414139348530729 0.6083333333333333\n",
      "749 번째 loss, accuracy:  0.7409786872639493 0.6083333333333333\n",
      "750 번째 loss, accuracy:  0.7405441019125479 0.6083333333333333\n",
      "751 번째 loss, accuracy:  0.7401101266404138 0.6\n",
      "752 번째 loss, accuracy:  0.7396768653380688 0.6\n",
      "753 번째 loss, accuracy:  0.7392443057428192 0.6\n",
      "754 번째 loss, accuracy:  0.7388124498987587 0.6\n",
      "755 번째 loss, accuracy:  0.7383813006602147 0.6\n",
      "756 번째 loss, accuracy:  0.7379507519534485 0.6\n",
      "757 번째 loss, accuracy:  0.7375208884101946 0.6\n",
      "758 번째 loss, accuracy:  0.7370917033731381 0.6\n",
      "759 번째 loss, accuracy:  0.7366632001367545 0.6\n",
      "760 번째 loss, accuracy:  0.7362353238730319 0.5916666666666667\n",
      "761 번째 loss, accuracy:  0.7358080918466459 0.5916666666666667\n",
      "762 번째 loss, accuracy:  0.7353815977788336 0.5916666666666667\n",
      "763 번째 loss, accuracy:  0.7349557873437386 0.5916666666666667\n",
      "764 번째 loss, accuracy:  0.7345306196390761 0.5916666666666667\n",
      "765 번째 loss, accuracy:  0.7341061188463153 0.5916666666666667\n",
      "766 번째 loss, accuracy:  0.73368223672413 0.5916666666666667\n",
      "767 번째 loss, accuracy:  0.7332590795352207 0.5916666666666667\n",
      "768 번째 loss, accuracy:  0.7328365735024458 0.5916666666666667\n",
      "769 번째 loss, accuracy:  0.7324147399908215 0.5916666666666667\n",
      "770 번째 loss, accuracy:  0.7319935343142989 0.5916666666666667\n",
      "771 번째 loss, accuracy:  0.7315730517341251 0.5916666666666667\n",
      "772 번째 loss, accuracy:  0.7311532846160067 0.5916666666666667\n",
      "773 번째 loss, accuracy:  0.7307340087769705 0.5916666666666667\n",
      "774 번째 loss, accuracy:  0.7303155493741371 0.5916666666666667\n",
      "775 번째 loss, accuracy:  0.7298976545730507 0.5916666666666667\n",
      "776 번째 loss, accuracy:  0.7294804602185037 0.5916666666666667\n",
      "777 번째 loss, accuracy:  0.7290639806405598 0.5916666666666667\n",
      "778 번째 loss, accuracy:  0.728648152375531 0.5916666666666667\n",
      "779 번째 loss, accuracy:  0.7282330417748706 0.5916666666666667\n",
      "780 번째 loss, accuracy:  0.7278185691166014 0.5916666666666667\n",
      "781 번째 loss, accuracy:  0.727404759606466 0.5916666666666667\n",
      "782 번째 loss, accuracy:  0.7269915272762059 0.5916666666666667\n",
      "783 번째 loss, accuracy:  0.7265790466839661 0.5916666666666667\n",
      "784 번째 loss, accuracy:  0.7261672212387521 0.5916666666666667\n",
      "785 번째 loss, accuracy:  0.7257560964312244 0.5916666666666667\n",
      "786 번째 loss, accuracy:  0.7253455948377553 0.5916666666666667\n",
      "787 번째 loss, accuracy:  0.7249357348724049 0.5916666666666667\n",
      "788 번째 loss, accuracy:  0.7245265042844271 0.5916666666666667\n",
      "789 번째 loss, accuracy:  0.7241179840630015 0.5916666666666667\n",
      "790 번째 loss, accuracy:  0.7237102094973277 0.5916666666666667\n",
      "791 번째 loss, accuracy:  0.7233030588435241 0.5916666666666667\n",
      "792 번째 loss, accuracy:  0.7228962726274879 0.5916666666666667\n",
      "793 번째 loss, accuracy:  0.7224904293899199 0.5916666666666667\n",
      "794 번째 loss, accuracy:  0.7220852493838372 0.5916666666666667\n",
      "795 번째 loss, accuracy:  0.7216806674649293 0.5833333333333334\n",
      "796 번째 loss, accuracy:  0.7212768174157527 0.5833333333333334\n",
      "797 번째 loss, accuracy:  0.7208736580077895 0.5833333333333334\n",
      "798 번째 loss, accuracy:  0.7204711199998545 0.5833333333333334\n",
      "799 번째 loss, accuracy:  0.7200692589019034 0.5833333333333334\n",
      "800 번째 loss, accuracy:  0.7196680206162858 0.5833333333333334\n",
      "801 번째 loss, accuracy:  0.7192674796801214 0.5833333333333334\n",
      "802 번째 loss, accuracy:  0.7188676410119802 0.5833333333333334\n",
      "803 번째 loss, accuracy:  0.7184683565291705 0.575\n",
      "804 번째 loss, accuracy:  0.7180698106700134 0.575\n",
      "805 번째 loss, accuracy:  0.7176719289921475 0.575\n",
      "806 번째 loss, accuracy:  0.7172746496181887 0.575\n",
      "807 번째 loss, accuracy:  0.7168781031104402 0.575\n",
      "808 번째 loss, accuracy:  0.7164822208019065 0.575\n",
      "809 번째 loss, accuracy:  0.7160869204712398 0.575\n",
      "810 번째 loss, accuracy:  0.7156921904229245 0.575\n",
      "811 번째 loss, accuracy:  0.7152982029856287 0.575\n",
      "812 번째 loss, accuracy:  0.7149049502003357 0.575\n",
      "813 번째 loss, accuracy:  0.7145123109118764 0.575\n",
      "814 번째 loss, accuracy:  0.7141202708783588 0.575\n",
      "815 번째 loss, accuracy:  0.7137289361711721 0.575\n",
      "816 번째 loss, accuracy:  0.7133381547554727 0.575\n",
      "817 번째 loss, accuracy:  0.7129481270183803 0.5666666666666667\n",
      "818 번째 loss, accuracy:  0.7125587553112495 0.5666666666666667\n",
      "819 번째 loss, accuracy:  0.7121700957118837 0.5666666666666667\n",
      "820 번째 loss, accuracy:  0.7117820673646797 0.5666666666666667\n",
      "821 번째 loss, accuracy:  0.7113947046173573 0.5666666666666667\n",
      "822 번째 loss, accuracy:  0.711007948301041 0.5666666666666667\n",
      "823 번째 loss, accuracy:  0.7106218648965767 0.5666666666666667\n",
      "824 번째 loss, accuracy:  0.7102363354237506 0.5666666666666667\n",
      "825 번째 loss, accuracy:  0.709851564996306 0.5666666666666667\n",
      "826 번째 loss, accuracy:  0.7094674515021827 0.5666666666666667\n",
      "827 번째 loss, accuracy:  0.7090839987928597 0.5666666666666667\n",
      "828 번째 loss, accuracy:  0.7087011590919251 0.5666666666666667\n",
      "829 번째 loss, accuracy:  0.7083189743524729 0.5666666666666667\n",
      "830 번째 loss, accuracy:  0.7079374365376537 0.5666666666666667\n",
      "831 번째 loss, accuracy:  0.7075565090189847 0.5666666666666667\n",
      "832 번째 loss, accuracy:  0.7071763124353203 0.5666666666666667\n",
      "833 번째 loss, accuracy:  0.7067967535851842 0.5666666666666667\n",
      "834 번째 loss, accuracy:  0.7064178425177458 0.5666666666666667\n",
      "835 번째 loss, accuracy:  0.7060395587848469 0.5666666666666667\n",
      "836 번째 loss, accuracy:  0.7056618224751604 0.5666666666666667\n",
      "837 번째 loss, accuracy:  0.7052848364886981 0.5666666666666667\n",
      "838 번째 loss, accuracy:  0.7049084775723243 0.5666666666666667\n",
      "839 번째 loss, accuracy:  0.7045327566067752 0.5666666666666667\n",
      "840 번째 loss, accuracy:  0.7041576870414287 0.5666666666666667\n",
      "841 번째 loss, accuracy:  0.7037831489425198 0.5666666666666667\n",
      "842 번째 loss, accuracy:  0.7034093234013784 0.5666666666666667\n",
      "843 번째 loss, accuracy:  0.7030361740150821 0.5666666666666667\n",
      "844 번째 loss, accuracy:  0.7026636812440666 0.5666666666666667\n",
      "845 번째 loss, accuracy:  0.7022918468767378 0.5666666666666667\n",
      "846 번째 loss, accuracy:  0.7019206285836052 0.5666666666666667\n",
      "847 번째 loss, accuracy:  0.7015499951124569 0.5666666666666667\n",
      "848 번째 loss, accuracy:  0.7011800876414028 0.5666666666666667\n",
      "849 번째 loss, accuracy:  0.7008107846806976 0.5666666666666667\n",
      "850 번째 loss, accuracy:  0.7004421285360356 0.5666666666666667\n",
      "851 번째 loss, accuracy:  0.7000739969710466 0.5666666666666667\n",
      "852 번째 loss, accuracy:  0.69970662981923 0.5666666666666667\n",
      "853 번째 loss, accuracy:  0.6993398776393269 0.5666666666666667\n",
      "854 번째 loss, accuracy:  0.6989737815849802 0.5666666666666667\n",
      "855 번째 loss, accuracy:  0.698608293304918 0.5666666666666667\n",
      "856 번째 loss, accuracy:  0.698243363735807 0.5666666666666667\n",
      "857 번째 loss, accuracy:  0.697879147678083 0.5666666666666667\n",
      "858 번째 loss, accuracy:  0.6975155896055936 0.5666666666666667\n",
      "859 번째 loss, accuracy:  0.6971526620686445 0.5666666666666667\n",
      "860 번째 loss, accuracy:  0.6967903548452247 0.5666666666666667\n",
      "861 번째 loss, accuracy:  0.6964286868023106 0.5666666666666667\n",
      "862 번째 loss, accuracy:  0.6960675968549651 0.5666666666666667\n",
      "863 번째 loss, accuracy:  0.695707174562497 0.5666666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864 번째 loss, accuracy:  0.6953473505055293 0.5666666666666667\n",
      "865 번째 loss, accuracy:  0.6949881820104601 0.5666666666666667\n",
      "866 번째 loss, accuracy:  0.6946296234192115 0.5666666666666667\n",
      "867 번째 loss, accuracy:  0.6942717175979278 0.5583333333333333\n",
      "868 번째 loss, accuracy:  0.6939144066115541 0.5583333333333333\n",
      "869 번째 loss, accuracy:  0.6935577007060995 0.5583333333333333\n",
      "870 번째 loss, accuracy:  0.6932015708593682 0.5583333333333333\n",
      "871 번째 loss, accuracy:  0.6928461128552189 0.5583333333333333\n",
      "872 번째 loss, accuracy:  0.6924912884422514 0.5583333333333333\n",
      "873 번째 loss, accuracy:  0.6921370908389293 0.5583333333333333\n",
      "874 번째 loss, accuracy:  0.6917835275666225 0.5583333333333333\n",
      "875 번째 loss, accuracy:  0.691430619644484 0.5583333333333333\n",
      "876 번째 loss, accuracy:  0.691078328361337 0.5583333333333333\n",
      "877 번째 loss, accuracy:  0.6907266328646032 0.5583333333333333\n",
      "878 번째 loss, accuracy:  0.6903755934117253 0.5583333333333333\n",
      "879 번째 loss, accuracy:  0.6900251761539653 0.5583333333333333\n",
      "880 번째 loss, accuracy:  0.6896753135673742 0.5583333333333333\n",
      "881 번째 loss, accuracy:  0.6893261150519163 0.5583333333333333\n",
      "882 번째 loss, accuracy:  0.6889774679953343 0.5583333333333333\n",
      "883 번째 loss, accuracy:  0.6886294903760154 0.55\n",
      "884 번째 loss, accuracy:  0.6882820321178631 0.55\n",
      "885 번째 loss, accuracy:  0.6879352520484079 0.55\n",
      "886 번째 loss, accuracy:  0.6875890635001959 0.55\n",
      "887 번째 loss, accuracy:  0.6872435461331957 0.55\n",
      "888 번째 loss, accuracy:  0.6868986240068821 0.55\n",
      "889 번째 loss, accuracy:  0.6865543128000324 0.55\n",
      "890 번째 loss, accuracy:  0.6862105631094032 0.55\n",
      "891 번째 loss, accuracy:  0.685867512801256 0.55\n",
      "892 번째 loss, accuracy:  0.6855250422764846 0.55\n",
      "893 번째 loss, accuracy:  0.6851831599557617 0.55\n",
      "894 번째 loss, accuracy:  0.6848419445957181 0.55\n",
      "895 번째 loss, accuracy:  0.6845013105946485 0.55\n",
      "896 번째 loss, accuracy:  0.6841613024310299 0.55\n",
      "897 번째 loss, accuracy:  0.6838218681459516 0.55\n",
      "898 번째 loss, accuracy:  0.6834830272197662 0.55\n",
      "899 번째 loss, accuracy:  0.6831448315686283 0.55\n",
      "900 번째 loss, accuracy:  0.682807239069657 0.55\n",
      "901 번째 loss, accuracy:  0.6824702517152322 0.55\n",
      "902 번째 loss, accuracy:  0.6821337882253725 0.55\n",
      "903 번째 loss, accuracy:  0.6817979540735734 0.55\n",
      "904 번째 loss, accuracy:  0.681462758866919 0.55\n",
      "905 번째 loss, accuracy:  0.6811281443040479 0.55\n",
      "906 번째 loss, accuracy:  0.6807941578977263 0.55\n",
      "907 번째 loss, accuracy:  0.6804607300928935 0.55\n",
      "908 번째 loss, accuracy:  0.6801279359532933 0.55\n",
      "909 번째 loss, accuracy:  0.6797957151859019 0.5416666666666666\n",
      "910 번째 loss, accuracy:  0.679464051605471 0.5416666666666666\n",
      "911 번째 loss, accuracy:  0.6791330374569922 0.5416666666666666\n",
      "912 번째 loss, accuracy:  0.6788025455994563 0.5416666666666666\n",
      "913 번째 loss, accuracy:  0.6784726977171663 0.5416666666666666\n",
      "914 번째 loss, accuracy:  0.6781434831986883 0.5416666666666666\n",
      "915 번째 loss, accuracy:  0.6778148180330565 0.5416666666666666\n",
      "916 번째 loss, accuracy:  0.6774867702239166 0.5416666666666666\n",
      "917 번째 loss, accuracy:  0.6771592940604485 0.5416666666666666\n",
      "918 번째 loss, accuracy:  0.6768324073927949 0.5333333333333333\n",
      "919 번째 loss, accuracy:  0.6765061101493297 0.5333333333333333\n",
      "920 번째 loss, accuracy:  0.6761803073572384 0.5333333333333333\n",
      "921 번째 loss, accuracy:  0.6758551865466965 0.5333333333333333\n",
      "922 번째 loss, accuracy:  0.675530576807687 0.5333333333333333\n",
      "923 번째 loss, accuracy:  0.6752064971535872 0.5333333333333333\n",
      "924 번째 loss, accuracy:  0.6748831146675388 0.5333333333333333\n",
      "925 번째 loss, accuracy:  0.6745602914819148 0.5333333333333333\n",
      "926 번째 loss, accuracy:  0.6742381134665144 0.5333333333333333\n",
      "927 번째 loss, accuracy:  0.6739163746915502 0.5333333333333333\n",
      "928 번째 loss, accuracy:  0.6735952715950847 0.5333333333333333\n",
      "929 번째 loss, accuracy:  0.6732748221968521 0.5333333333333333\n",
      "930 번째 loss, accuracy:  0.6729549573206047 0.5333333333333333\n",
      "931 번째 loss, accuracy:  0.6726356768867883 0.5333333333333333\n",
      "932 번째 loss, accuracy:  0.6723169188258142 0.5333333333333333\n",
      "933 번째 loss, accuracy:  0.6719988003949322 0.5333333333333333\n",
      "934 번째 loss, accuracy:  0.6716812530759827 0.5333333333333333\n",
      "935 번째 loss, accuracy:  0.6713642957658728 0.5333333333333333\n",
      "936 번째 loss, accuracy:  0.6710478507384291 0.5333333333333333\n",
      "937 번째 loss, accuracy:  0.6707320162230012 0.5333333333333333\n",
      "938 번째 loss, accuracy:  0.6704167318655553 0.5333333333333333\n",
      "939 번째 loss, accuracy:  0.6701020119666449 0.5333333333333333\n",
      "940 번째 loss, accuracy:  0.6697878369580723 0.5333333333333333\n",
      "941 번째 loss, accuracy:  0.6694742853150065 0.5333333333333333\n",
      "942 번째 loss, accuracy:  0.6691613220649814 0.5333333333333333\n",
      "943 번째 loss, accuracy:  0.6688489117857176 0.5333333333333333\n",
      "944 번째 loss, accuracy:  0.6685370681833838 0.5333333333333333\n",
      "945 번째 loss, accuracy:  0.6682258210325778 0.5333333333333333\n",
      "946 번째 loss, accuracy:  0.6679151382865655 0.5416666666666666\n",
      "947 번째 loss, accuracy:  0.6676050394750509 0.5416666666666666\n",
      "948 번째 loss, accuracy:  0.6672954763352529 0.5416666666666666\n",
      "949 번째 loss, accuracy:  0.6669864084196876 0.5416666666666666\n",
      "950 번째 loss, accuracy:  0.6666780062812906 0.5333333333333333\n",
      "951 번째 loss, accuracy:  0.6663700787723433 0.5333333333333333\n",
      "952 번째 loss, accuracy:  0.6660627843318867 0.525\n",
      "953 번째 loss, accuracy:  0.6657560692134837 0.525\n",
      "954 번째 loss, accuracy:  0.6654498965950857 0.525\n",
      "955 번째 loss, accuracy:  0.665144265296825 0.525\n",
      "956 번째 loss, accuracy:  0.6648391944491292 0.525\n",
      "957 번째 loss, accuracy:  0.6645347012010916 0.525\n",
      "958 번째 loss, accuracy:  0.6642307930022012 0.525\n",
      "959 번째 loss, accuracy:  0.6639274107266505 0.525\n",
      "960 번째 loss, accuracy:  0.6636245258450337 0.525\n",
      "961 번째 loss, accuracy:  0.6633221314452752 0.525\n",
      "962 번째 loss, accuracy:  0.6630204363812628 0.525\n",
      "963 번째 loss, accuracy:  0.6627191936236225 0.525\n",
      "964 번째 loss, accuracy:  0.6624185641373539 0.525\n",
      "965 번째 loss, accuracy:  0.6621184485096901 0.5166666666666667\n",
      "966 번째 loss, accuracy:  0.6618188969356017 0.5166666666666667\n",
      "967 번째 loss, accuracy:  0.6615199233072742 0.5166666666666667\n",
      "968 번째 loss, accuracy:  0.6612214947231504 0.5166666666666667\n",
      "969 번째 loss, accuracy:  0.6609236509470177 0.5166666666666667\n",
      "970 번째 loss, accuracy:  0.6606262983926304 0.5166666666666667\n",
      "971 번째 loss, accuracy:  0.6603295082685802 0.5166666666666667\n",
      "972 번째 loss, accuracy:  0.6600332768852802 0.5166666666666667\n",
      "973 번째 loss, accuracy:  0.6597376270972461 0.5166666666666667\n",
      "974 번째 loss, accuracy:  0.6594424841349313 0.5166666666666667\n",
      "975 번째 loss, accuracy:  0.6591479014345638 0.5166666666666667\n",
      "976 번째 loss, accuracy:  0.6588538306442866 0.5166666666666667\n",
      "977 번째 loss, accuracy:  0.6585603292832793 0.5166666666666667\n",
      "978 번째 loss, accuracy:  0.6582673633600357 0.5166666666666667\n",
      "979 번째 loss, accuracy:  0.6579748794417045 0.5166666666666667\n",
      "980 번째 loss, accuracy:  0.6576829536618194 0.5166666666666667\n",
      "981 번째 loss, accuracy:  0.6573915679622195 0.5166666666666667\n",
      "982 번째 loss, accuracy:  0.6571007111934509 0.5166666666666667\n",
      "983 번째 loss, accuracy:  0.6568103762309764 0.5166666666666667\n",
      "984 번째 loss, accuracy:  0.6565206147278528 0.5166666666666667\n",
      "985 번째 loss, accuracy:  0.6562313947965639 0.5166666666666667\n",
      "986 번째 loss, accuracy:  0.655942729753616 0.5166666666666667\n",
      "987 번째 loss, accuracy:  0.6556545175625323 0.5166666666666667\n",
      "988 번째 loss, accuracy:  0.6553668384775726 0.5166666666666667\n",
      "989 번째 loss, accuracy:  0.65507973868622 0.5166666666666667\n",
      "990 번째 loss, accuracy:  0.654793189619821 0.5166666666666667\n",
      "991 번째 loss, accuracy:  0.6545070475535044 0.5166666666666667\n",
      "992 번째 loss, accuracy:  0.6542215363693997 0.5166666666666667\n",
      "993 번째 loss, accuracy:  0.6539365684271838 0.5166666666666667\n",
      "994 번째 loss, accuracy:  0.6536521103386367 0.5166666666666667\n",
      "995 번째 loss, accuracy:  0.6533681454390813 0.5166666666666667\n",
      "996 번째 loss, accuracy:  0.6530846749259404 0.5166666666666667\n",
      "997 번째 loss, accuracy:  0.6528017823918381 0.5166666666666667\n",
      "998 번째 loss, accuracy:  0.6525194452839329 0.5166666666666667\n",
      "999 번째 loss, accuracy:  0.652237416343192 0.5166666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 loss, accuracy:  1.8200048608723145 0.3333333333333333\n",
      "1 번째 loss, accuracy:  1.8108763430642623 0.3333333333333333\n",
      "2 번째 loss, accuracy:  1.801722498686081 0.3333333333333333\n",
      "3 번째 loss, accuracy:  1.7925405891914845 0.3333333333333333\n",
      "4 번째 loss, accuracy:  1.7833319069873368 0.3333333333333333\n",
      "5 번째 loss, accuracy:  1.7740886148262045 0.3333333333333333\n",
      "6 번째 loss, accuracy:  1.76481993654858 0.3333333333333333\n",
      "7 번째 loss, accuracy:  1.7555212365306996 0.3333333333333333\n",
      "8 번째 loss, accuracy:  1.7461909610999231 0.3333333333333333\n",
      "9 번째 loss, accuracy:  1.7368311201183448 0.3333333333333333\n",
      "10 번째 loss, accuracy:  1.7274382173629244 0.3333333333333333\n",
      "11 번째 loss, accuracy:  1.7180127733798747 0.3333333333333333\n",
      "12 번째 loss, accuracy:  1.7085535744604665 0.3333333333333333\n",
      "13 번째 loss, accuracy:  1.6990546972604046 0.3333333333333333\n",
      "14 번째 loss, accuracy:  1.6895220108172142 0.3333333333333333\n",
      "15 번째 loss, accuracy:  1.679950218656495 0.3333333333333333\n",
      "16 번째 loss, accuracy:  1.6703406542254744 0.3333333333333333\n",
      "17 번째 loss, accuracy:  1.6606831493534788 0.3333333333333333\n",
      "18 번째 loss, accuracy:  1.650976269121227 0.3333333333333333\n",
      "19 번째 loss, accuracy:  1.64121936495972 0.3333333333333333\n",
      "20 번째 loss, accuracy:  1.6314032959271514 0.3333333333333333\n",
      "21 번째 loss, accuracy:  1.621528951881057 0.3333333333333333\n",
      "22 번째 loss, accuracy:  1.6115876510360876 0.325\n",
      "23 번째 loss, accuracy:  1.601575426727816 0.325\n",
      "24 번째 loss, accuracy:  1.5914875036740352 0.31666666666666665\n",
      "25 번째 loss, accuracy:  1.581318139743136 0.30833333333333335\n",
      "26 번째 loss, accuracy:  1.5710509739556409 0.3\n",
      "27 번째 loss, accuracy:  1.5606909021760431 0.2916666666666667\n",
      "28 번째 loss, accuracy:  1.5502252030096169 0.275\n",
      "29 번째 loss, accuracy:  1.539644436665722 0.26666666666666666\n",
      "30 번째 loss, accuracy:  1.5289419895129879 0.26666666666666666\n",
      "31 번째 loss, accuracy:  1.5181148101500501 0.26666666666666666\n",
      "32 번째 loss, accuracy:  1.5071526629776069 0.25\n",
      "33 번째 loss, accuracy:  1.4960454921721105 0.25\n",
      "34 번째 loss, accuracy:  1.4847914906757038 0.21666666666666667\n",
      "35 번째 loss, accuracy:  1.4733952459539692 0.15833333333333333\n",
      "36 번째 loss, accuracy:  1.4618519326180557 0.14166666666666666\n",
      "37 번째 loss, accuracy:  1.4501642877202137 0.11666666666666667\n",
      "38 번째 loss, accuracy:  1.4383369900183152 0.1\n",
      "39 번째 loss, accuracy:  1.426375749966119 0.05\n",
      "40 번째 loss, accuracy:  1.4142763678384083 0.03333333333333333\n",
      "41 번째 loss, accuracy:  1.4021071773018543 0.008333333333333333\n",
      "42 번째 loss, accuracy:  1.389818075341706 0.0\n",
      "43 번째 loss, accuracy:  1.3774541544158196 0.0\n",
      "44 번째 loss, accuracy:  1.3650623033449505 0.0\n",
      "45 번째 loss, accuracy:  1.3526900076614192 0.0\n",
      "46 번째 loss, accuracy:  1.340355663389939 0.0\n",
      "47 번째 loss, accuracy:  1.3280797707080396 0.0\n",
      "48 번째 loss, accuracy:  1.3159083183465834 0.0\n",
      "49 번째 loss, accuracy:  1.3038991333295378 0.0\n",
      "50 번째 loss, accuracy:  1.2920623283420491 0.0\n",
      "51 번째 loss, accuracy:  1.2804469376814018 0.0\n",
      "52 번째 loss, accuracy:  1.2690732363055826 0.0\n",
      "53 번째 loss, accuracy:  1.257994944522964 0.0\n",
      "54 번째 loss, accuracy:  1.247241831803373 0.0\n",
      "55 번째 loss, accuracy:  1.2368036380591543 0.0\n",
      "56 번째 loss, accuracy:  1.226708548313906 0.0\n",
      "57 번째 loss, accuracy:  1.2169611256353348 0.0\n",
      "58 번째 loss, accuracy:  1.2075811874410531 0.0\n",
      "59 번째 loss, accuracy:  1.1985734656351494 0.0\n",
      "60 번째 loss, accuracy:  1.1899147616024675 0.0\n",
      "61 번째 loss, accuracy:  1.181621762368999 0.0\n",
      "62 번째 loss, accuracy:  1.1736885263642884 0.0\n",
      "63 번째 loss, accuracy:  1.1660897716068874 0.0\n",
      "64 번째 loss, accuracy:  1.1588296414056354 0.0\n",
      "65 번째 loss, accuracy:  1.1518854929736961 0.0\n",
      "66 번째 loss, accuracy:  1.1452545433212753 0.0\n",
      "67 번째 loss, accuracy:  1.1389109400623483 0.0\n",
      "68 번째 loss, accuracy:  1.1328630238308075 0.0\n",
      "69 번째 loss, accuracy:  1.1270850726020807 0.0\n",
      "70 번째 loss, accuracy:  1.1215620223750828 0.0\n",
      "71 번째 loss, accuracy:  1.1162818146539053 0.0\n",
      "72 번째 loss, accuracy:  1.1112378852517344 0.0\n",
      "73 번째 loss, accuracy:  1.1064038852251097 0.0\n",
      "74 번째 loss, accuracy:  1.101781694972464 0.0\n",
      "75 번째 loss, accuracy:  1.0973574731444893 0.0\n",
      "76 번째 loss, accuracy:  1.09311562122941 0.0\n",
      "77 번째 loss, accuracy:  1.0890545810883887 0.008333333333333333\n",
      "78 번째 loss, accuracy:  1.0851554149760199 0.016666666666666666\n",
      "79 번째 loss, accuracy:  1.0814123560855273 0.016666666666666666\n",
      "80 번째 loss, accuracy:  1.077817743374131 0.016666666666666666\n",
      "81 번째 loss, accuracy:  1.074360244565156 0.016666666666666666\n",
      "82 번째 loss, accuracy:  1.0710312780627023 0.016666666666666666\n",
      "83 번째 loss, accuracy:  1.0678313519486433 0.025\n",
      "84 번째 loss, accuracy:  1.0647489185999162 0.03333333333333333\n",
      "85 번째 loss, accuracy:  1.0617780762406528 0.041666666666666664\n",
      "86 번째 loss, accuracy:  1.0589135490314305 0.041666666666666664\n",
      "87 번째 loss, accuracy:  1.0561479507221503 0.05\n",
      "88 번째 loss, accuracy:  1.0534773759938048 0.05\n",
      "89 번째 loss, accuracy:  1.050897965772657 0.05\n",
      "90 번째 loss, accuracy:  1.048403649116752 0.058333333333333334\n",
      "91 번째 loss, accuracy:  1.04599180432211 0.058333333333333334\n",
      "92 번째 loss, accuracy:  1.0436576151315078 0.075\n",
      "93 번째 loss, accuracy:  1.041396959000117 0.09166666666666666\n",
      "94 번째 loss, accuracy:  1.0392054615018098 0.1\n",
      "95 번째 loss, accuracy:  1.0370807179102337 0.10833333333333334\n",
      "96 번째 loss, accuracy:  1.0350199241167037 0.10833333333333334\n",
      "97 번째 loss, accuracy:  1.0330203610649706 0.11666666666666667\n",
      "98 번째 loss, accuracy:  1.03107849074664 0.125\n",
      "99 번째 loss, accuracy:  1.029191160788045 0.14166666666666666\n",
      "100 번째 loss, accuracy:  1.0273572504273099 0.15\n",
      "101 번째 loss, accuracy:  1.0255733424675462 0.175\n",
      "102 번째 loss, accuracy:  1.023838030611386 0.19166666666666668\n",
      "103 번째 loss, accuracy:  1.0221494797919175 0.2\n",
      "104 번째 loss, accuracy:  1.0205049156299133 0.21666666666666667\n",
      "105 번째 loss, accuracy:  1.0189016495657592 0.225\n",
      "106 번째 loss, accuracy:  1.017339347882791 0.24166666666666667\n",
      "107 번째 loss, accuracy:  1.015815880291066 0.26666666666666666\n",
      "108 번째 loss, accuracy:  1.0143296886438227 0.2916666666666667\n",
      "109 번째 loss, accuracy:  1.0128793472722257 0.325\n",
      "110 번째 loss, accuracy:  1.0114634776506517 0.375\n",
      "111 번째 loss, accuracy:  1.0100799708010384 0.39166666666666666\n",
      "112 번째 loss, accuracy:  1.0087289232857481 0.39166666666666666\n",
      "113 번째 loss, accuracy:  1.0074082974939136 0.4\n",
      "114 번째 loss, accuracy:  1.0061170041472858 0.4166666666666667\n",
      "115 번째 loss, accuracy:  1.004853687965855 0.44166666666666665\n",
      "116 번째 loss, accuracy:  1.0036174839390435 0.45\n",
      "117 번째 loss, accuracy:  1.002407056355353 0.4583333333333333\n",
      "118 번째 loss, accuracy:  1.0012210364366634 0.48333333333333334\n",
      "119 번째 loss, accuracy:  1.000060217405261 0.49166666666666664\n",
      "120 번째 loss, accuracy:  0.9989213563256379 0.5083333333333333\n",
      "121 번째 loss, accuracy:  0.9978057599293464 0.5083333333333333\n",
      "122 번째 loss, accuracy:  0.996712118562365 0.525\n",
      "123 번째 loss, accuracy:  0.9956398788832944 0.5416666666666666\n",
      "124 번째 loss, accuracy:  0.9945877596431241 0.5416666666666666\n",
      "125 번째 loss, accuracy:  0.9935540038591547 0.55\n",
      "126 번째 loss, accuracy:  0.992539643996988 0.55\n",
      "127 번째 loss, accuracy:  0.9915439953062927 0.55\n",
      "128 번째 loss, accuracy:  0.990565996658706 0.5666666666666667\n",
      "129 번째 loss, accuracy:  0.9896045962259026 0.5666666666666667\n",
      "130 번째 loss, accuracy:  0.9886591177307034 0.5666666666666667\n",
      "131 번째 loss, accuracy:  0.9877303707336277 0.575\n",
      "132 번째 loss, accuracy:  0.9868168431818218 0.575\n",
      "133 번째 loss, accuracy:  0.9859177709719418 0.5916666666666667\n",
      "134 번째 loss, accuracy:  0.9850336392192743 0.6\n",
      "135 번째 loss, accuracy:  0.9841626334959941 0.6\n",
      "136 번째 loss, accuracy:  0.9833057812624857 0.6\n",
      "137 번째 loss, accuracy:  0.9824615463220242 0.6\n",
      "138 번째 loss, accuracy:  0.9816298308833192 0.6\n",
      "139 번째 loss, accuracy:  0.9808105818805992 0.6\n",
      "140 번째 loss, accuracy:  0.9800035220395645 0.6083333333333333\n",
      "141 번째 loss, accuracy:  0.9792078437044766 0.6083333333333333\n",
      "142 번째 loss, accuracy:  0.978423559396597 0.6083333333333333\n",
      "143 번째 loss, accuracy:  0.9776498732644114 0.6166666666666667\n",
      "144 번째 loss, accuracy:  0.9768869209628289 0.6166666666666667\n",
      "145 번째 loss, accuracy:  0.9761340070970789 0.6166666666666667\n",
      "146 번째 loss, accuracy:  0.9753906028496157 0.6166666666666667\n",
      "147 번째 loss, accuracy:  0.9746567202563905 0.6166666666666667\n",
      "148 번째 loss, accuracy:  0.9739324052493846 0.6333333333333333\n",
      "149 번째 loss, accuracy:  0.9732174277569842 0.6333333333333333\n",
      "150 번째 loss, accuracy:  0.9725106898117691 0.6333333333333333\n",
      "151 번째 loss, accuracy:  0.9718128210416329 0.6416666666666667\n",
      "152 번째 loss, accuracy:  0.971122604161854 0.6416666666666667\n",
      "153 번째 loss, accuracy:  0.9704407841384363 0.6416666666666667\n",
      "154 번째 loss, accuracy:  0.9697668191177559 0.6416666666666667\n",
      "155 번째 loss, accuracy:  0.969100728080454 0.6416666666666667\n",
      "156 번째 loss, accuracy:  0.9684414337360754 0.6416666666666667\n",
      "157 번째 loss, accuracy:  0.96778900717711 0.6416666666666667\n",
      "158 번째 loss, accuracy:  0.9671440463083913 0.6416666666666667\n",
      "159 번째 loss, accuracy:  0.9665057544621144 0.6416666666666667\n",
      "160 번째 loss, accuracy:  0.9658741911563188 0.6416666666666667\n",
      "161 번째 loss, accuracy:  0.9652487556469389 0.65\n",
      "162 번째 loss, accuracy:  0.9646297365814471 0.65\n",
      "163 번째 loss, accuracy:  0.9640168195975352 0.65\n",
      "164 번째 loss, accuracy:  0.9634095372962475 0.65\n",
      "165 번째 loss, accuracy:  0.9628084083250376 0.65\n",
      "166 번째 loss, accuracy:  0.962212827701457 0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 번째 loss, accuracy:  0.9616225825403731 0.65\n",
      "168 번째 loss, accuracy:  0.9610374464092822 0.65\n",
      "169 번째 loss, accuracy:  0.9604575066550344 0.65\n",
      "170 번째 loss, accuracy:  0.9598829527908695 0.65\n",
      "171 번째 loss, accuracy:  0.9593130630635837 0.65\n",
      "172 번째 loss, accuracy:  0.9587480698294111 0.65\n",
      "173 번째 loss, accuracy:  0.9581878935237029 0.65\n",
      "174 번째 loss, accuracy:  0.957632081834679 0.65\n",
      "175 번째 loss, accuracy:  0.9570806429370912 0.65\n",
      "176 번째 loss, accuracy:  0.956533263507256 0.65\n",
      "177 번째 loss, accuracy:  0.9559901152201294 0.65\n",
      "178 번째 loss, accuracy:  0.9554513023166895 0.65\n",
      "179 번째 loss, accuracy:  0.954916211537173 0.65\n",
      "180 번째 loss, accuracy:  0.9543853282657613 0.65\n",
      "181 번째 loss, accuracy:  0.9538580473679907 0.65\n",
      "182 번째 loss, accuracy:  0.9533345048122875 0.65\n",
      "183 번째 loss, accuracy:  0.9528144417709307 0.65\n",
      "184 번째 loss, accuracy:  0.9522975285867706 0.65\n",
      "185 번째 loss, accuracy:  0.9517844946821817 0.65\n",
      "186 번째 loss, accuracy:  0.9512746418079073 0.65\n",
      "187 번째 loss, accuracy:  0.9507680613579663 0.65\n",
      "188 번째 loss, accuracy:  0.950264582076154 0.65\n",
      "189 번째 loss, accuracy:  0.9497643604698739 0.65\n",
      "190 번째 loss, accuracy:  0.9492671187237818 0.65\n",
      "191 번째 loss, accuracy:  0.9487729606139127 0.65\n",
      "192 번째 loss, accuracy:  0.9482815467525593 0.65\n",
      "193 번째 loss, accuracy:  0.9477931493886381 0.65\n",
      "194 번째 loss, accuracy:  0.947307188699289 0.65\n",
      "195 번째 loss, accuracy:  0.9468240987824996 0.65\n",
      "196 번째 loss, accuracy:  0.9463434577980034 0.65\n",
      "197 번째 loss, accuracy:  0.9458655174102799 0.65\n",
      "198 번째 loss, accuracy:  0.9453902719762927 0.65\n",
      "199 번째 loss, accuracy:  0.9449172037206583 0.65\n",
      "200 번째 loss, accuracy:  0.9444465440056024 0.65\n",
      "201 번째 loss, accuracy:  0.9439781305777657 0.65\n",
      "202 번째 loss, accuracy:  0.943512118860787 0.65\n",
      "203 번째 loss, accuracy:  0.9430485785057023 0.65\n",
      "204 번째 loss, accuracy:  0.9425870794976218 0.65\n",
      "205 번째 loss, accuracy:  0.942127595515254 0.65\n",
      "206 번째 loss, accuracy:  0.9416702778202314 0.65\n",
      "207 번째 loss, accuracy:  0.9412151581118507 0.65\n",
      "208 번째 loss, accuracy:  0.9407618593367942 0.65\n",
      "209 번째 loss, accuracy:  0.9403104058108637 0.65\n",
      "210 번째 loss, accuracy:  0.9398611014115604 0.65\n",
      "211 번째 loss, accuracy:  0.9394137670505377 0.65\n",
      "212 번째 loss, accuracy:  0.9389680124247867 0.65\n",
      "213 번째 loss, accuracy:  0.938524189868494 0.65\n",
      "214 번째 loss, accuracy:  0.9380823318458825 0.65\n",
      "215 번째 loss, accuracy:  0.9376419783576397 0.65\n",
      "216 번째 loss, accuracy:  0.9372034794701922 0.6583333333333333\n",
      "217 번째 loss, accuracy:  0.9367666157716741 0.6583333333333333\n",
      "218 번째 loss, accuracy:  0.9363312868004631 0.6583333333333333\n",
      "219 번째 loss, accuracy:  0.9358974928802043 0.6583333333333333\n",
      "220 번째 loss, accuracy:  0.9354655038912244 0.6583333333333333\n",
      "221 번째 loss, accuracy:  0.9350349265950594 0.6583333333333333\n",
      "222 번째 loss, accuracy:  0.9346058639011602 0.6583333333333333\n",
      "223 번째 loss, accuracy:  0.9341783914021939 0.6583333333333333\n",
      "224 번째 loss, accuracy:  0.933752139812214 0.6583333333333333\n",
      "225 번째 loss, accuracy:  0.9333275738771044 0.6583333333333333\n",
      "226 번째 loss, accuracy:  0.9329042610308486 0.6583333333333333\n",
      "227 번째 loss, accuracy:  0.9324821586486837 0.6583333333333333\n",
      "228 번째 loss, accuracy:  0.9320614817355846 0.6583333333333333\n",
      "229 번째 loss, accuracy:  0.9316421122046356 0.6583333333333333\n",
      "230 번째 loss, accuracy:  0.93122403063945 0.6583333333333333\n",
      "231 번째 loss, accuracy:  0.9308072010391358 0.6583333333333333\n",
      "232 번째 loss, accuracy:  0.9303917385943693 0.6583333333333333\n",
      "233 번째 loss, accuracy:  0.929977400862064 0.6583333333333333\n",
      "234 번째 loss, accuracy:  0.92956432450039 0.6583333333333333\n",
      "235 번째 loss, accuracy:  0.9291524947261985 0.6583333333333333\n",
      "236 번째 loss, accuracy:  0.928741530243997 0.6583333333333333\n",
      "237 번째 loss, accuracy:  0.9283318888260755 0.6583333333333333\n",
      "238 번째 loss, accuracy:  0.9279233858387238 0.6583333333333333\n",
      "239 번째 loss, accuracy:  0.9275159988615637 0.6583333333333333\n",
      "240 번째 loss, accuracy:  0.92710969146579 0.6583333333333333\n",
      "241 번째 loss, accuracy:  0.9267044463246172 0.6583333333333333\n",
      "242 번째 loss, accuracy:  0.9263002400633779 0.6583333333333333\n",
      "243 번째 loss, accuracy:  0.9258970322758163 0.6583333333333333\n",
      "244 번째 loss, accuracy:  0.9254947356582993 0.6583333333333333\n",
      "245 번째 loss, accuracy:  0.9250936026546012 0.6583333333333333\n",
      "246 번째 loss, accuracy:  0.9246933233363592 0.6583333333333333\n",
      "247 번째 loss, accuracy:  0.9242942093951688 0.6583333333333333\n",
      "248 번째 loss, accuracy:  0.9238960316371893 0.6583333333333333\n",
      "249 번째 loss, accuracy:  0.923498744735262 0.6583333333333333\n",
      "250 번째 loss, accuracy:  0.9231022956134488 0.6583333333333333\n",
      "251 번째 loss, accuracy:  0.9227067992915944 0.6583333333333333\n",
      "252 번째 loss, accuracy:  0.9223121930383676 0.6583333333333333\n",
      "253 번째 loss, accuracy:  0.9219185757406992 0.6583333333333333\n",
      "254 번째 loss, accuracy:  0.9215258614620596 0.6583333333333333\n",
      "255 번째 loss, accuracy:  0.9211338676271646 0.6583333333333333\n",
      "256 번째 loss, accuracy:  0.9207427693640159 0.6583333333333333\n",
      "257 번째 loss, accuracy:  0.9203524228882802 0.6583333333333333\n",
      "258 번째 loss, accuracy:  0.9199629857430854 0.6583333333333333\n",
      "259 번째 loss, accuracy:  0.9195743835212674 0.6583333333333333\n",
      "260 번째 loss, accuracy:  0.9191865064414467 0.6583333333333333\n",
      "261 번째 loss, accuracy:  0.9187994419459393 0.6583333333333333\n",
      "262 번째 loss, accuracy:  0.9184131914714889 0.6583333333333333\n",
      "263 번째 loss, accuracy:  0.9180276646081276 0.6583333333333333\n",
      "264 번째 loss, accuracy:  0.9176428627511839 0.6583333333333333\n",
      "265 번째 loss, accuracy:  0.9172588263533441 0.6583333333333333\n",
      "266 번째 loss, accuracy:  0.9168755718837402 0.6583333333333333\n",
      "267 번째 loss, accuracy:  0.9164929458212272 0.6583333333333333\n",
      "268 번째 loss, accuracy:  0.9161111295523893 0.6583333333333333\n",
      "269 번째 loss, accuracy:  0.9157299999374452 0.6583333333333333\n",
      "270 번째 loss, accuracy:  0.91534963163521 0.6583333333333333\n",
      "271 번째 loss, accuracy:  0.9149698919249919 0.6583333333333333\n",
      "272 번째 loss, accuracy:  0.9145908496556037 0.6583333333333333\n",
      "273 번째 loss, accuracy:  0.9142125285434811 0.6583333333333333\n",
      "274 번째 loss, accuracy:  0.9138349030337903 0.6583333333333333\n",
      "275 번째 loss, accuracy:  0.9134578965639514 0.6583333333333333\n",
      "276 번째 loss, accuracy:  0.9130815454960518 0.6583333333333333\n",
      "277 번째 loss, accuracy:  0.912705833625075 0.6583333333333333\n",
      "278 번째 loss, accuracy:  0.9123307749944877 0.6583333333333333\n",
      "279 번째 loss, accuracy:  0.9119563764323836 0.6583333333333333\n",
      "280 번째 loss, accuracy:  0.9115825577188896 0.6583333333333333\n",
      "281 번째 loss, accuracy:  0.9112093976620653 0.6583333333333333\n",
      "282 번째 loss, accuracy:  0.9108368665766174 0.6583333333333333\n",
      "283 번째 loss, accuracy:  0.9104649232272995 0.6583333333333333\n",
      "284 번째 loss, accuracy:  0.9100936587395173 0.6583333333333333\n",
      "285 번째 loss, accuracy:  0.9097228905357414 0.6583333333333333\n",
      "286 번째 loss, accuracy:  0.9093527160491951 0.6583333333333333\n",
      "287 번째 loss, accuracy:  0.9089831697090489 0.6583333333333333\n",
      "288 번째 loss, accuracy:  0.908614195683231 0.6583333333333333\n",
      "289 번째 loss, accuracy:  0.9082458069868294 0.6583333333333333\n",
      "290 번째 loss, accuracy:  0.9078779140094544 0.6583333333333333\n",
      "291 번째 loss, accuracy:  0.9075106441885974 0.6583333333333333\n",
      "292 번째 loss, accuracy:  0.907143911304291 0.6583333333333333\n",
      "293 번째 loss, accuracy:  0.906777726794684 0.6583333333333333\n",
      "294 번째 loss, accuracy:  0.9064121223958266 0.6583333333333333\n",
      "295 번째 loss, accuracy:  0.9060470336840064 0.6583333333333333\n",
      "296 번째 loss, accuracy:  0.9056824965248574 0.6583333333333333\n",
      "297 번째 loss, accuracy:  0.905318458193496 0.6583333333333333\n",
      "298 번째 loss, accuracy:  0.9049550295454545 0.6583333333333333\n",
      "299 번째 loss, accuracy:  0.9045920574111413 0.6583333333333333\n",
      "300 번째 loss, accuracy:  0.9042296337954961 0.6583333333333333\n",
      "301 번째 loss, accuracy:  0.9038677395556584 0.6583333333333333\n",
      "302 번째 loss, accuracy:  0.9035063378857331 0.6583333333333333\n",
      "303 번째 loss, accuracy:  0.9031454151789821 0.6583333333333333\n",
      "304 번째 loss, accuracy:  0.9027850469054675 0.6583333333333333\n",
      "305 번째 loss, accuracy:  0.9024251997391503 0.6583333333333333\n",
      "306 번째 loss, accuracy:  0.9020658213948788 0.6583333333333333\n",
      "307 번째 loss, accuracy:  0.9017069113190845 0.6583333333333333\n",
      "308 번째 loss, accuracy:  0.9013484772973956 0.6583333333333333\n",
      "309 번째 loss, accuracy:  0.9009906103080738 0.6583333333333333\n",
      "310 번째 loss, accuracy:  0.9006331869941415 0.6583333333333333\n",
      "311 번째 loss, accuracy:  0.9002762209452295 0.6583333333333333\n",
      "312 번째 loss, accuracy:  0.8999197203760598 0.6583333333333333\n",
      "313 번째 loss, accuracy:  0.8995636949438693 0.6583333333333333\n",
      "314 번째 loss, accuracy:  0.8992081418718949 0.6583333333333333\n",
      "315 번째 loss, accuracy:  0.8988530786394987 0.6583333333333333\n",
      "316 번째 loss, accuracy:  0.8984984936191263 0.6583333333333333\n",
      "317 번째 loss, accuracy:  0.8981443624726647 0.6583333333333333\n",
      "318 번째 loss, accuracy:  0.8977906680675664 0.6583333333333333\n",
      "319 번째 loss, accuracy:  0.8974373983014097 0.6583333333333333\n",
      "320 번째 loss, accuracy:  0.8970846447425364 0.6583333333333333\n",
      "321 번째 loss, accuracy:  0.8967323104565555 0.6583333333333333\n",
      "322 번째 loss, accuracy:  0.8963803881109637 0.6583333333333333\n",
      "323 번째 loss, accuracy:  0.8960289437377214 0.6583333333333333\n",
      "324 번째 loss, accuracy:  0.8956779383739416 0.6583333333333333\n",
      "325 번째 loss, accuracy:  0.8953274105224514 0.6583333333333333\n",
      "326 번째 loss, accuracy:  0.8949772859344355 0.6583333333333333\n",
      "327 번째 loss, accuracy:  0.8946276036480098 0.6583333333333333\n",
      "328 번째 loss, accuracy:  0.894278352755466 0.6583333333333333\n",
      "329 번째 loss, accuracy:  0.8939295136856439 0.6583333333333333\n",
      "330 번째 loss, accuracy:  0.8935811423576948 0.6583333333333333\n",
      "331 번째 loss, accuracy:  0.8932331808882523 0.6583333333333333\n",
      "332 번째 loss, accuracy:  0.892885653824006 0.6583333333333333\n",
      "333 번째 loss, accuracy:  0.8925385272828208 0.6583333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 번째 loss, accuracy:  0.8921918245053161 0.6583333333333333\n",
      "335 번째 loss, accuracy:  0.8918455285523883 0.6583333333333333\n",
      "336 번째 loss, accuracy:  0.891499653537294 0.6583333333333333\n",
      "337 번째 loss, accuracy:  0.8911541865918742 0.6583333333333333\n",
      "338 번째 loss, accuracy:  0.8908091299361554 0.6583333333333333\n",
      "339 번째 loss, accuracy:  0.8904644505144234 0.6583333333333333\n",
      "340 번째 loss, accuracy:  0.8901202089016704 0.6583333333333333\n",
      "341 번째 loss, accuracy:  0.889776365510975 0.6583333333333333\n",
      "342 번째 loss, accuracy:  0.8894329327317589 0.6583333333333333\n",
      "343 번째 loss, accuracy:  0.8890898886552968 0.6583333333333333\n",
      "344 번째 loss, accuracy:  0.8887472362399558 0.6583333333333333\n",
      "345 번째 loss, accuracy:  0.8884050074502239 0.6583333333333333\n",
      "346 번째 loss, accuracy:  0.8880631824351827 0.6583333333333333\n",
      "347 번째 loss, accuracy:  0.8877217315632188 0.6583333333333333\n",
      "348 번째 loss, accuracy:  0.8873806747060973 0.6583333333333333\n",
      "349 번째 loss, accuracy:  0.8870400216864562 0.6583333333333333\n",
      "350 번째 loss, accuracy:  0.8866997107071908 0.6583333333333333\n",
      "351 번째 loss, accuracy:  0.8863598259896749 0.6583333333333333\n",
      "352 번째 loss, accuracy:  0.8860203333720276 0.6583333333333333\n",
      "353 번째 loss, accuracy:  0.8856812084997746 0.6583333333333333\n",
      "354 번째 loss, accuracy:  0.8853424863040563 0.6583333333333333\n",
      "355 번째 loss, accuracy:  0.8850041282953931 0.6583333333333333\n",
      "356 번째 loss, accuracy:  0.8846661541349571 0.6583333333333333\n",
      "357 번째 loss, accuracy:  0.8843285279433805 0.6583333333333333\n",
      "358 번째 loss, accuracy:  0.8839913029531811 0.6583333333333333\n",
      "359 번째 loss, accuracy:  0.8836544327212881 0.6583333333333333\n",
      "360 번째 loss, accuracy:  0.8833179580478354 0.6583333333333333\n",
      "361 번째 loss, accuracy:  0.8829818639437899 0.6583333333333333\n",
      "362 번째 loss, accuracy:  0.8826461368623892 0.6583333333333333\n",
      "363 번째 loss, accuracy:  0.8823107546774298 0.6583333333333333\n",
      "364 번째 loss, accuracy:  0.8819757504703101 0.6583333333333333\n",
      "365 번째 loss, accuracy:  0.8816411380992804 0.6583333333333333\n",
      "366 번째 loss, accuracy:  0.8813068599107955 0.6583333333333333\n",
      "367 번째 loss, accuracy:  0.8809729660515734 0.6583333333333333\n",
      "368 번째 loss, accuracy:  0.8806394234570123 0.6583333333333333\n",
      "369 번째 loss, accuracy:  0.8803062443716723 0.6583333333333333\n",
      "370 번째 loss, accuracy:  0.8799734189275596 0.6583333333333333\n",
      "371 번째 loss, accuracy:  0.8796409625548731 0.6583333333333333\n",
      "372 번째 loss, accuracy:  0.8793088831815914 0.6583333333333333\n",
      "373 번째 loss, accuracy:  0.8789771520899345 0.6583333333333333\n",
      "374 번째 loss, accuracy:  0.878645775478218 0.6583333333333333\n",
      "375 번째 loss, accuracy:  0.8783147616563424 0.6583333333333333\n",
      "376 번째 loss, accuracy:  0.8779840878995214 0.6583333333333333\n",
      "377 번째 loss, accuracy:  0.8776537729804005 0.6583333333333333\n",
      "378 번째 loss, accuracy:  0.8773238019658055 0.6583333333333333\n",
      "379 번째 loss, accuracy:  0.8769941627185321 0.6583333333333333\n",
      "380 번째 loss, accuracy:  0.8766648871651916 0.6583333333333333\n",
      "381 번째 loss, accuracy:  0.8763359832130642 0.6583333333333333\n",
      "382 번째 loss, accuracy:  0.8760073786640497 0.6583333333333333\n",
      "383 번째 loss, accuracy:  0.875679154171182 0.6583333333333333\n",
      "384 번째 loss, accuracy:  0.8753512813420036 0.6583333333333333\n",
      "385 번째 loss, accuracy:  0.875023741254035 0.6583333333333333\n",
      "386 번째 loss, accuracy:  0.8746965396247849 0.6583333333333333\n",
      "387 번째 loss, accuracy:  0.8743696990218155 0.6583333333333333\n",
      "388 번째 loss, accuracy:  0.874043180630929 0.6583333333333333\n",
      "389 번째 loss, accuracy:  0.8737170013397798 0.6583333333333333\n",
      "390 번째 loss, accuracy:  0.8733911775331891 0.6583333333333333\n",
      "391 번째 loss, accuracy:  0.87306568516734 0.6583333333333333\n",
      "392 번째 loss, accuracy:  0.8727405286092229 0.6583333333333333\n",
      "393 번째 loss, accuracy:  0.8724157180847744 0.6583333333333333\n",
      "394 번째 loss, accuracy:  0.8720912645349178 0.6583333333333333\n",
      "395 번째 loss, accuracy:  0.871767136829264 0.6583333333333333\n",
      "396 번째 loss, accuracy:  0.8714433032435783 0.6583333333333333\n",
      "397 번째 loss, accuracy:  0.8711198310218404 0.6583333333333333\n",
      "398 번째 loss, accuracy:  0.8707966820630701 0.6583333333333333\n",
      "399 번째 loss, accuracy:  0.8704738704594476 0.6583333333333333\n",
      "400 번째 loss, accuracy:  0.8701514038755921 0.6583333333333333\n",
      "401 번째 loss, accuracy:  0.8698292677482453 0.6583333333333333\n",
      "402 번째 loss, accuracy:  0.8695074518854856 0.6583333333333333\n",
      "403 번째 loss, accuracy:  0.8691859530341867 0.6583333333333333\n",
      "404 번째 loss, accuracy:  0.8688647890573304 0.6583333333333333\n",
      "405 번째 loss, accuracy:  0.8685439674896164 0.6583333333333333\n",
      "406 번째 loss, accuracy:  0.8682234249824474 0.6583333333333333\n",
      "407 번째 loss, accuracy:  0.8679032440518929 0.6583333333333333\n",
      "408 번째 loss, accuracy:  0.8675834003025197 0.6583333333333333\n",
      "409 번째 loss, accuracy:  0.8672638826967489 0.6583333333333333\n",
      "410 번째 loss, accuracy:  0.8669446850194917 0.6583333333333333\n",
      "411 번째 loss, accuracy:  0.8666258119438908 0.6583333333333333\n",
      "412 번째 loss, accuracy:  0.8663072513724767 0.6583333333333333\n",
      "413 번째 loss, accuracy:  0.86598901204821 0.6583333333333333\n",
      "414 번째 loss, accuracy:  0.8656711007918148 0.6583333333333333\n",
      "415 번째 loss, accuracy:  0.8653534932393491 0.6583333333333333\n",
      "416 번째 loss, accuracy:  0.8650362156351405 0.6583333333333333\n",
      "417 번째 loss, accuracy:  0.8647192664414277 0.6583333333333333\n",
      "418 번째 loss, accuracy:  0.864402637230121 0.6583333333333333\n",
      "419 번째 loss, accuracy:  0.8640863218274869 0.6583333333333333\n",
      "420 번째 loss, accuracy:  0.8637703321893099 0.6583333333333333\n",
      "421 번째 loss, accuracy:  0.8634546489334153 0.6583333333333333\n",
      "422 번째 loss, accuracy:  0.8631392822434582 0.6583333333333333\n",
      "423 번째 loss, accuracy:  0.8628242327380279 0.6583333333333333\n",
      "424 번째 loss, accuracy:  0.862509495880684 0.6583333333333333\n",
      "425 번째 loss, accuracy:  0.8621950725619477 0.6583333333333333\n",
      "426 번째 loss, accuracy:  0.8618809392734922 0.6583333333333333\n",
      "427 번째 loss, accuracy:  0.8615671082472943 0.6583333333333333\n",
      "428 번째 loss, accuracy:  0.86125360512976 0.6583333333333333\n",
      "429 번째 loss, accuracy:  0.8609404284743615 0.6583333333333333\n",
      "430 번째 loss, accuracy:  0.8606275577936209 0.6583333333333333\n",
      "431 번째 loss, accuracy:  0.8603149989895509 0.6583333333333333\n",
      "432 번째 loss, accuracy:  0.8600027507011166 0.6583333333333333\n",
      "433 번째 loss, accuracy:  0.8596907813912239 0.6583333333333333\n",
      "434 번째 loss, accuracy:  0.8593791452180656 0.6583333333333333\n",
      "435 번째 loss, accuracy:  0.8590678192183733 0.6583333333333333\n",
      "436 번째 loss, accuracy:  0.8587567867752531 0.6583333333333333\n",
      "437 번째 loss, accuracy:  0.858446083335738 0.6583333333333333\n",
      "438 번째 loss, accuracy:  0.858135673062165 0.6583333333333333\n",
      "439 번째 loss, accuracy:  0.8578255832780647 0.6583333333333333\n",
      "440 번째 loss, accuracy:  0.8575157699498417 0.6583333333333333\n",
      "441 번째 loss, accuracy:  0.8572062728518951 0.6583333333333333\n",
      "442 번째 loss, accuracy:  0.8568970857061099 0.6583333333333333\n",
      "443 번째 loss, accuracy:  0.8565881919693813 0.6583333333333333\n",
      "444 번째 loss, accuracy:  0.8562796003362224 0.6583333333333333\n",
      "445 번째 loss, accuracy:  0.8559713366949219 0.6583333333333333\n",
      "446 번째 loss, accuracy:  0.8556633670182243 0.6583333333333333\n",
      "447 번째 loss, accuracy:  0.8553556962038904 0.6583333333333333\n",
      "448 번째 loss, accuracy:  0.8550483280412131 0.6583333333333333\n",
      "449 번째 loss, accuracy:  0.8547412164037453 0.6583333333333333\n",
      "450 번째 loss, accuracy:  0.8544344532443645 0.6583333333333333\n",
      "451 번째 loss, accuracy:  0.8541279748889271 0.6583333333333333\n",
      "452 번째 loss, accuracy:  0.8538217937853708 0.6583333333333333\n",
      "453 번째 loss, accuracy:  0.8535159031465825 0.6583333333333333\n",
      "454 번째 loss, accuracy:  0.8532103344389296 0.6583333333333333\n",
      "455 번째 loss, accuracy:  0.8529050345697812 0.6583333333333333\n",
      "456 번째 loss, accuracy:  0.8526000538582894 0.6583333333333333\n",
      "457 번째 loss, accuracy:  0.8522953690057337 0.6583333333333333\n",
      "458 번째 loss, accuracy:  0.8519909868056383 0.6583333333333333\n",
      "459 번째 loss, accuracy:  0.8516868955107361 0.6583333333333333\n",
      "460 번째 loss, accuracy:  0.8513831005247738 0.6583333333333333\n",
      "461 번째 loss, accuracy:  0.851079601537357 0.6583333333333333\n",
      "462 번째 loss, accuracy:  0.8507763364945317 0.6583333333333333\n",
      "463 번째 loss, accuracy:  0.8504734291789195 0.6583333333333333\n",
      "464 번째 loss, accuracy:  0.8501708105922471 0.6583333333333333\n",
      "465 번째 loss, accuracy:  0.8498684877390996 0.6583333333333333\n",
      "466 번째 loss, accuracy:  0.8495664329064333 0.6583333333333333\n",
      "467 번째 loss, accuracy:  0.8492646838898129 0.6583333333333333\n",
      "468 번째 loss, accuracy:  0.8489632346659252 0.6583333333333333\n",
      "469 번째 loss, accuracy:  0.8486620654731069 0.6583333333333333\n",
      "470 번째 loss, accuracy:  0.8483611958572276 0.6583333333333333\n",
      "471 번째 loss, accuracy:  0.8480606113045812 0.6583333333333333\n",
      "472 번째 loss, accuracy:  0.8477603197175593 0.6583333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473 번째 loss, accuracy:  0.8474603187161347 0.6583333333333333\n",
      "474 번째 loss, accuracy:  0.8471606011415813 0.6583333333333333\n",
      "475 번째 loss, accuracy:  0.8468611674331136 0.6583333333333333\n",
      "476 번째 loss, accuracy:  0.8465620186119222 0.6583333333333333\n",
      "477 번째 loss, accuracy:  0.8462631699719351 0.6583333333333333\n",
      "478 번째 loss, accuracy:  0.8459646103252574 0.6583333333333333\n",
      "479 번째 loss, accuracy:  0.8456663375716448 0.6583333333333333\n",
      "480 번째 loss, accuracy:  0.8453683453750005 0.6583333333333333\n",
      "481 번째 loss, accuracy:  0.8450706378574606 0.6583333333333333\n",
      "482 번째 loss, accuracy:  0.8447732225237099 0.6583333333333333\n",
      "483 번째 loss, accuracy:  0.8444760870521215 0.6583333333333333\n",
      "484 번째 loss, accuracy:  0.8441792368987869 0.6583333333333333\n",
      "485 번째 loss, accuracy:  0.8438826776173453 0.6583333333333333\n",
      "486 번째 loss, accuracy:  0.8435863669542552 0.6583333333333333\n",
      "487 번째 loss, accuracy:  0.843290368948445 0.6583333333333333\n",
      "488 번째 loss, accuracy:  0.8429946498947293 0.6583333333333333\n",
      "489 번째 loss, accuracy:  0.8426992215600831 0.6583333333333333\n",
      "490 번째 loss, accuracy:  0.8424040672098239 0.6583333333333333\n",
      "491 번째 loss, accuracy:  0.8421092056092737 0.6583333333333333\n",
      "492 번째 loss, accuracy:  0.841814625007528 0.6583333333333333\n",
      "493 번째 loss, accuracy:  0.8415203302008192 0.6583333333333333\n",
      "494 번째 loss, accuracy:  0.8412263077448413 0.6583333333333333\n",
      "495 번째 loss, accuracy:  0.8409325711762965 0.6583333333333333\n",
      "496 번째 loss, accuracy:  0.8406391180769944 0.6583333333333333\n",
      "497 번째 loss, accuracy:  0.8403459343974636 0.6583333333333333\n",
      "498 번째 loss, accuracy:  0.8400530132761259 0.6583333333333333\n",
      "499 번째 loss, accuracy:  0.8397603741754547 0.6583333333333333\n",
      "500 번째 loss, accuracy:  0.8394680259216373 0.6583333333333333\n",
      "501 번째 loss, accuracy:  0.839175954186759 0.6583333333333333\n",
      "502 번째 loss, accuracy:  0.8388841731008675 0.6583333333333333\n",
      "503 번째 loss, accuracy:  0.8385926648705042 0.6583333333333333\n",
      "504 번째 loss, accuracy:  0.838301431634431 0.6583333333333333\n",
      "505 번째 loss, accuracy:  0.8380104840145148 0.6583333333333333\n",
      "506 번째 loss, accuracy:  0.8377198073724625 0.6583333333333333\n",
      "507 번째 loss, accuracy:  0.83742941142348 0.6583333333333333\n",
      "508 번째 loss, accuracy:  0.8371392870829397 0.6583333333333333\n",
      "509 번째 loss, accuracy:  0.8368494484203528 0.6583333333333333\n",
      "510 번째 loss, accuracy:  0.8365598835159654 0.6583333333333333\n",
      "511 번째 loss, accuracy:  0.8362705935560328 0.6583333333333333\n",
      "512 번째 loss, accuracy:  0.8359815738395747 0.6583333333333333\n",
      "513 번째 loss, accuracy:  0.8356928164448353 0.6583333333333333\n",
      "514 번째 loss, accuracy:  0.8354043388286247 0.6583333333333333\n",
      "515 번째 loss, accuracy:  0.8351161409284297 0.6583333333333333\n",
      "516 번째 loss, accuracy:  0.834828218811564 0.6583333333333333\n",
      "517 번째 loss, accuracy:  0.8345405760745853 0.6583333333333333\n",
      "518 번째 loss, accuracy:  0.8342532052634151 0.6583333333333333\n",
      "519 번째 loss, accuracy:  0.8339661072726796 0.6583333333333333\n",
      "520 번째 loss, accuracy:  0.8336792712375943 0.6583333333333333\n",
      "521 번째 loss, accuracy:  0.8333927163165848 0.6583333333333333\n",
      "522 번째 loss, accuracy:  0.8331064265493276 0.6583333333333333\n",
      "523 번째 loss, accuracy:  0.8328204161370425 0.6583333333333333\n",
      "524 번째 loss, accuracy:  0.8325346779994092 0.6583333333333333\n",
      "525 번째 loss, accuracy:  0.8322492012726826 0.6583333333333333\n",
      "526 번째 loss, accuracy:  0.8319639971147521 0.6583333333333333\n",
      "527 번째 loss, accuracy:  0.8316790557548934 0.6583333333333333\n",
      "528 번째 loss, accuracy:  0.831394374439699 0.6583333333333333\n",
      "529 번째 loss, accuracy:  0.8311099851697904 0.6583333333333333\n",
      "530 번째 loss, accuracy:  0.8308258687071636 0.6583333333333333\n",
      "531 번째 loss, accuracy:  0.830542014966468 0.6583333333333333\n",
      "532 번째 loss, accuracy:  0.8302584358471712 0.6583333333333333\n",
      "533 번째 loss, accuracy:  0.8299751271498207 0.6583333333333333\n",
      "534 번째 loss, accuracy:  0.8296920699859694 0.6583333333333333\n",
      "535 번째 loss, accuracy:  0.829409287504635 0.6583333333333333\n",
      "536 번째 loss, accuracy:  0.8291267803683169 0.6583333333333333\n",
      "537 번째 loss, accuracy:  0.8288445428170673 0.6583333333333333\n",
      "538 번째 loss, accuracy:  0.828562558082646 0.6583333333333333\n",
      "539 번째 loss, accuracy:  0.8282808542669748 0.6583333333333333\n",
      "540 번째 loss, accuracy:  0.8279994206163214 0.6583333333333333\n",
      "541 번째 loss, accuracy:  0.8277182486512698 0.6583333333333333\n",
      "542 번째 loss, accuracy:  0.82743733502766 0.6583333333333333\n",
      "543 번째 loss, accuracy:  0.827156695261142 0.6583333333333333\n",
      "544 번째 loss, accuracy:  0.8268763171641675 0.6583333333333333\n",
      "545 번째 loss, accuracy:  0.8265962035717289 0.6583333333333333\n",
      "546 번째 loss, accuracy:  0.8263163401448235 0.6583333333333333\n",
      "547 번째 loss, accuracy:  0.8260367590375324 0.6583333333333333\n",
      "548 번째 loss, accuracy:  0.8257574396059049 0.6583333333333333\n",
      "549 번째 loss, accuracy:  0.8254783791159735 0.6583333333333333\n",
      "550 번째 loss, accuracy:  0.8251995952239901 0.6583333333333333\n",
      "551 번째 loss, accuracy:  0.824921071998033 0.6583333333333333\n",
      "552 번째 loss, accuracy:  0.8246428120491199 0.6583333333333333\n",
      "553 번째 loss, accuracy:  0.8243648152867593 0.6583333333333333\n",
      "554 번째 loss, accuracy:  0.8240870830607233 0.6583333333333333\n",
      "555 번째 loss, accuracy:  0.8238096101018663 0.6583333333333333\n",
      "556 번째 loss, accuracy:  0.8235324030681543 0.6583333333333333\n",
      "557 번째 loss, accuracy:  0.8232554569532194 0.6583333333333333\n",
      "558 번째 loss, accuracy:  0.8229787551556361 0.6583333333333333\n",
      "559 번째 loss, accuracy:  0.8227023322924576 0.6583333333333333\n",
      "560 번째 loss, accuracy:  0.8224261722483346 0.6583333333333333\n",
      "561 번째 loss, accuracy:  0.8221502680100783 0.6583333333333333\n",
      "562 번째 loss, accuracy:  0.8218746285421905 0.6583333333333333\n",
      "563 번째 loss, accuracy:  0.8215992455287342 0.6583333333333333\n",
      "564 번째 loss, accuracy:  0.8213241254579186 0.6583333333333333\n",
      "565 번째 loss, accuracy:  0.8210492674339513 0.6583333333333333\n",
      "566 번째 loss, accuracy:  0.8207746557517471 0.6583333333333333\n",
      "567 번째 loss, accuracy:  0.8205003090104553 0.6583333333333333\n",
      "568 번째 loss, accuracy:  0.8202262284099054 0.6583333333333333\n",
      "569 번째 loss, accuracy:  0.8199524010160576 0.6583333333333333\n",
      "570 번째 loss, accuracy:  0.8196788290835497 0.6583333333333333\n",
      "571 번째 loss, accuracy:  0.8194055172996138 0.6583333333333333\n",
      "572 번째 loss, accuracy:  0.8191324640406458 0.6583333333333333\n",
      "573 번째 loss, accuracy:  0.8188596798994529 0.6583333333333333\n",
      "574 번째 loss, accuracy:  0.8185871531460219 0.6583333333333333\n",
      "575 번째 loss, accuracy:  0.8183148786588273 0.6583333333333333\n",
      "576 번째 loss, accuracy:  0.8180428600842545 0.6583333333333333\n",
      "577 번째 loss, accuracy:  0.8177710916614885 0.6583333333333333\n",
      "578 번째 loss, accuracy:  0.8174995910793502 0.6583333333333333\n",
      "579 번째 loss, accuracy:  0.8172283483728844 0.6583333333333333\n",
      "580 번째 loss, accuracy:  0.8169573593109961 0.6583333333333333\n",
      "581 번째 loss, accuracy:  0.8166866249844046 0.6583333333333333\n",
      "582 번째 loss, accuracy:  0.8164161510614693 0.6583333333333333\n",
      "583 번째 loss, accuracy:  0.8161459253015052 0.6583333333333333\n",
      "584 번째 loss, accuracy:  0.8158759620793172 0.6583333333333333\n",
      "585 번째 loss, accuracy:  0.8156062380389442 0.6583333333333333\n",
      "586 번째 loss, accuracy:  0.8153367792589565 0.6583333333333333\n",
      "587 번째 loss, accuracy:  0.8150675823677022 0.6583333333333333\n",
      "588 번째 loss, accuracy:  0.8147986378278401 0.6583333333333333\n",
      "589 번째 loss, accuracy:  0.8145299447657045 0.6583333333333333\n",
      "590 번째 loss, accuracy:  0.8142615108869379 0.6583333333333333\n",
      "591 번째 loss, accuracy:  0.8139933287535879 0.6583333333333333\n",
      "592 번째 loss, accuracy:  0.8137253970906281 0.6583333333333333\n",
      "593 번째 loss, accuracy:  0.813457718172326 0.6583333333333333\n",
      "594 번째 loss, accuracy:  0.8131902833359713 0.6583333333333333\n",
      "595 번째 loss, accuracy:  0.8129230905428119 0.6583333333333333\n",
      "596 번째 loss, accuracy:  0.812656155153337 0.6583333333333333\n",
      "597 번째 loss, accuracy:  0.8123894839034773 0.6583333333333333\n",
      "598 번째 loss, accuracy:  0.8121230672238942 0.6583333333333333\n",
      "599 번째 loss, accuracy:  0.8118568782442452 0.6583333333333333\n",
      "600 번째 loss, accuracy:  0.8115909508137419 0.6583333333333333\n",
      "601 번째 loss, accuracy:  0.8113252910961565 0.6583333333333333\n",
      "602 번째 loss, accuracy:  0.8110598835812719 0.6583333333333333\n",
      "603 번째 loss, accuracy:  0.8107947291962077 0.6583333333333333\n",
      "604 번째 loss, accuracy:  0.8105298005880338 0.6583333333333333\n",
      "605 번째 loss, accuracy:  0.8102651455958289 0.6583333333333333\n",
      "606 번째 loss, accuracy:  0.8100007406602417 0.6583333333333333\n",
      "607 번째 loss, accuracy:  0.80973657315911 0.6583333333333333\n",
      "608 번째 loss, accuracy:  0.8094726543307347 0.6583333333333333\n",
      "609 번째 loss, accuracy:  0.8092089994804822 0.6583333333333333\n",
      "610 번째 loss, accuracy:  0.8089455953570877 0.6583333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611 번째 loss, accuracy:  0.808682437121201 0.6583333333333333\n",
      "612 번째 loss, accuracy:  0.8084195276673268 0.6583333333333333\n",
      "613 번째 loss, accuracy:  0.8081568706941522 0.6583333333333333\n",
      "614 번째 loss, accuracy:  0.8078944577813754 0.6583333333333333\n",
      "615 번째 loss, accuracy:  0.8076322981100146 0.6583333333333333\n",
      "616 번째 loss, accuracy:  0.8073703852293412 0.6583333333333333\n",
      "617 번째 loss, accuracy:  0.8071086987259036 0.6583333333333333\n",
      "618 번째 loss, accuracy:  0.8068472789356015 0.6583333333333333\n",
      "619 번째 loss, accuracy:  0.8065860986193348 0.6583333333333333\n",
      "620 번째 loss, accuracy:  0.8063251712523337 0.6583333333333333\n",
      "621 번째 loss, accuracy:  0.806064491660252 0.6583333333333333\n",
      "622 번째 loss, accuracy:  0.8058040606971112 0.6583333333333333\n",
      "623 번째 loss, accuracy:  0.8055438766701613 0.6583333333333333\n",
      "624 번째 loss, accuracy:  0.805283938888423 0.6583333333333333\n",
      "625 번째 loss, accuracy:  0.8050242250632716 0.6583333333333333\n",
      "626 번째 loss, accuracy:  0.8047647742296102 0.6583333333333333\n",
      "627 번째 loss, accuracy:  0.8045055754157406 0.6583333333333333\n",
      "628 번째 loss, accuracy:  0.8042466190784403 0.6583333333333333\n",
      "629 번째 loss, accuracy:  0.8039878997321693 0.6583333333333333\n",
      "630 번째 loss, accuracy:  0.8037294385826724 0.6583333333333333\n",
      "631 번째 loss, accuracy:  0.8034712056119372 0.6583333333333333\n",
      "632 번째 loss, accuracy:  0.8032132304070557 0.6583333333333333\n",
      "633 번째 loss, accuracy:  0.8029555024247557 0.6583333333333333\n",
      "634 번째 loss, accuracy:  0.8026980150518476 0.6583333333333333\n",
      "635 번째 loss, accuracy:  0.8024407744494743 0.6583333333333333\n",
      "636 번째 loss, accuracy:  0.8021837735033398 0.6583333333333333\n",
      "637 번째 loss, accuracy:  0.8019270151769536 0.6583333333333333\n",
      "638 번째 loss, accuracy:  0.8016705027546924 0.6583333333333333\n",
      "639 번째 loss, accuracy:  0.8014142339349346 0.6583333333333333\n",
      "640 번째 loss, accuracy:  0.8011582094840887 0.6583333333333333\n",
      "641 번째 loss, accuracy:  0.8009024267306908 0.6583333333333333\n",
      "642 번째 loss, accuracy:  0.8006468914570636 0.6583333333333333\n",
      "643 번째 loss, accuracy:  0.8003915951036757 0.6583333333333333\n",
      "644 번째 loss, accuracy:  0.800136540640542 0.6583333333333333\n",
      "645 번째 loss, accuracy:  0.799881728069574 0.6583333333333333\n",
      "646 번째 loss, accuracy:  0.7996271531643312 0.6583333333333333\n",
      "647 번째 loss, accuracy:  0.7993728280525022 0.6583333333333333\n",
      "648 번째 loss, accuracy:  0.7991187149718345 0.6583333333333333\n",
      "649 번째 loss, accuracy:  0.7988648682026794 0.6583333333333333\n",
      "650 번째 loss, accuracy:  0.7986112635360961 0.6583333333333333\n",
      "651 번째 loss, accuracy:  0.7983578942353673 0.6583333333333333\n",
      "652 번째 loss, accuracy:  0.7981047639073079 0.6583333333333333\n",
      "653 번째 loss, accuracy:  0.797851881801585 0.6583333333333333\n",
      "654 번째 loss, accuracy:  0.7975992372270194 0.6583333333333333\n",
      "655 번째 loss, accuracy:  0.7973468319773845 0.6583333333333333\n",
      "656 번째 loss, accuracy:  0.7970946423456984 0.6583333333333333\n",
      "657 번째 loss, accuracy:  0.796842719982254 0.6583333333333333\n",
      "658 번째 loss, accuracy:  0.7965910342808331 0.6583333333333333\n",
      "659 번째 loss, accuracy:  0.796339590477766 0.6583333333333333\n",
      "660 번째 loss, accuracy:  0.796088384105375 0.6583333333333333\n",
      "661 번째 loss, accuracy:  0.7958374143965204 0.6583333333333333\n",
      "662 번째 loss, accuracy:  0.7955866843671172 0.6583333333333333\n",
      "663 번째 loss, accuracy:  0.7953361957810965 0.6583333333333333\n",
      "664 번째 loss, accuracy:  0.7950859437391743 0.6583333333333333\n",
      "665 번째 loss, accuracy:  0.7948359253114404 0.6583333333333333\n",
      "666 번째 loss, accuracy:  0.7945861341467465 0.6583333333333333\n",
      "667 번째 loss, accuracy:  0.79433659612814 0.6583333333333333\n",
      "668 번째 loss, accuracy:  0.7940872959982355 0.6583333333333333\n",
      "669 번째 loss, accuracy:  0.7938382305595523 0.6583333333333333\n",
      "670 번째 loss, accuracy:  0.7935893901068637 0.6583333333333333\n",
      "671 번째 loss, accuracy:  0.7933407903971962 0.6583333333333333\n",
      "672 번째 loss, accuracy:  0.7930924305557797 0.6583333333333333\n",
      "673 번째 loss, accuracy:  0.7928443074477999 0.6583333333333333\n",
      "674 번째 loss, accuracy:  0.7925964278685149 0.6583333333333333\n",
      "675 번째 loss, accuracy:  0.7923487773546747 0.6583333333333333\n",
      "676 번째 loss, accuracy:  0.7921013652908613 0.6583333333333333\n",
      "677 번째 loss, accuracy:  0.7918541864271087 0.6583333333333333\n",
      "678 번째 loss, accuracy:  0.7916072470561505 0.6583333333333333\n",
      "679 번째 loss, accuracy:  0.7913605433198285 0.6583333333333333\n",
      "680 번째 loss, accuracy:  0.7911140684909294 0.6583333333333333\n",
      "681 번째 loss, accuracy:  0.7908678336721576 0.6583333333333333\n",
      "682 번째 loss, accuracy:  0.7906218306101377 0.6583333333333333\n",
      "683 번째 loss, accuracy:  0.7903760649502011 0.6583333333333333\n",
      "684 번째 loss, accuracy:  0.7901305344008888 0.6583333333333333\n",
      "685 번째 loss, accuracy:  0.7898852329701685 0.6583333333333333\n",
      "686 번째 loss, accuracy:  0.7896401659192294 0.6583333333333333\n",
      "687 번째 loss, accuracy:  0.7893953180583982 0.6583333333333333\n",
      "688 번째 loss, accuracy:  0.7891507213607779 0.6583333333333333\n",
      "689 번째 loss, accuracy:  0.7889063387419584 0.6583333333333333\n",
      "690 번째 loss, accuracy:  0.788662195836789 0.6583333333333333\n",
      "691 번째 loss, accuracy:  0.7884182984666016 0.6583333333333333\n",
      "692 번째 loss, accuracy:  0.7881746131993083 0.6583333333333333\n",
      "693 번째 loss, accuracy:  0.7879311778431904 0.6583333333333333\n",
      "694 번째 loss, accuracy:  0.7876879713267347 0.6583333333333333\n",
      "695 번째 loss, accuracy:  0.7874449870927346 0.6583333333333333\n",
      "696 번째 loss, accuracy:  0.7872022447914759 0.6583333333333333\n",
      "697 번째 loss, accuracy:  0.7869597351475152 0.6583333333333333\n",
      "698 번째 loss, accuracy:  0.7867174609477608 0.6583333333333333\n",
      "699 번째 loss, accuracy:  0.7864754123505576 0.6583333333333333\n",
      "700 번째 loss, accuracy:  0.7862335985782117 0.6583333333333333\n",
      "701 번째 loss, accuracy:  0.7859920175098128 0.6583333333333333\n",
      "702 번째 loss, accuracy:  0.785750667960306 0.6583333333333333\n",
      "703 번째 loss, accuracy:  0.7855095472319379 0.6583333333333333\n",
      "704 번째 loss, accuracy:  0.7852686557062754 0.6583333333333333\n",
      "705 번째 loss, accuracy:  0.785027999691642 0.6583333333333333\n",
      "706 번째 loss, accuracy:  0.7847875678225243 0.6583333333333333\n",
      "707 번째 loss, accuracy:  0.7845473658913986 0.6583333333333333\n",
      "708 번째 loss, accuracy:  0.7843073928548775 0.6583333333333333\n",
      "709 번째 loss, accuracy:  0.784067651764412 0.6583333333333333\n",
      "710 번째 loss, accuracy:  0.7838281293037036 0.6583333333333333\n",
      "711 번째 loss, accuracy:  0.7835888445478977 0.6583333333333333\n",
      "712 번째 loss, accuracy:  0.7833497856137633 0.6583333333333333\n",
      "713 번째 loss, accuracy:  0.7831109591960426 0.6583333333333333\n",
      "714 번째 loss, accuracy:  0.7828723624299251 0.6583333333333333\n",
      "715 번째 loss, accuracy:  0.7826339920429424 0.6583333333333333\n",
      "716 번째 loss, accuracy:  0.7823958441930065 0.6583333333333333\n",
      "717 번째 loss, accuracy:  0.7821579293878789 0.6583333333333333\n",
      "718 번째 loss, accuracy:  0.7819202438920595 0.6583333333333333\n",
      "719 번째 loss, accuracy:  0.7816827824577643 0.6583333333333333\n",
      "720 번째 loss, accuracy:  0.7814455388433853 0.6583333333333333\n",
      "721 번째 loss, accuracy:  0.781208527654269 0.6583333333333333\n",
      "722 번째 loss, accuracy:  0.7809717340309575 0.6583333333333333\n",
      "723 번째 loss, accuracy:  0.7807351787503776 0.6583333333333333\n",
      "724 번째 loss, accuracy:  0.780498852237159 0.6583333333333333\n",
      "725 번째 loss, accuracy:  0.7802627476215979 0.6583333333333333\n",
      "726 번째 loss, accuracy:  0.7800268765134054 0.6583333333333333\n",
      "727 번째 loss, accuracy:  0.7797912210639403 0.6583333333333333\n",
      "728 번째 loss, accuracy:  0.7795557986830559 0.6583333333333333\n",
      "729 번째 loss, accuracy:  0.7793205944409497 0.6583333333333333\n",
      "730 번째 loss, accuracy:  0.7790856149834581 0.6583333333333333\n",
      "731 번째 loss, accuracy:  0.7788508677496251 0.6583333333333333\n",
      "732 번째 loss, accuracy:  0.778616346502819 0.6583333333333333\n",
      "733 번째 loss, accuracy:  0.7783820487735752 0.6583333333333333\n",
      "734 번째 loss, accuracy:  0.7781479773921754 0.6583333333333333\n",
      "735 번째 loss, accuracy:  0.7779141253054157 0.6583333333333333\n",
      "736 번째 loss, accuracy:  0.7776805016067788 0.6583333333333333\n",
      "737 번째 loss, accuracy:  0.7774471035438973 0.6583333333333333\n",
      "738 번째 loss, accuracy:  0.7772139256674606 0.6583333333333333\n",
      "739 번째 loss, accuracy:  0.7769809552244575 0.6583333333333333\n",
      "740 번째 loss, accuracy:  0.7767482278941071 0.6583333333333333\n",
      "741 번째 loss, accuracy:  0.7765157187608273 0.6583333333333333\n",
      "742 번째 loss, accuracy:  0.7762834301601306 0.6583333333333333\n",
      "743 번째 loss, accuracy:  0.7760513718534391 0.6583333333333333\n",
      "744 번째 loss, accuracy:  0.7758195322234033 0.6583333333333333\n",
      "745 번째 loss, accuracy:  0.7755879172603258 0.6583333333333333\n",
      "746 번째 loss, accuracy:  0.7753565221806951 0.6583333333333333\n",
      "747 번째 loss, accuracy:  0.775125349333677 0.6583333333333333\n",
      "748 번째 loss, accuracy:  0.77489439621812 0.6583333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749 번째 loss, accuracy:  0.7746636615566622 0.6583333333333333\n",
      "750 번째 loss, accuracy:  0.7744331405173143 0.6583333333333333\n",
      "751 번째 loss, accuracy:  0.7742028550279545 0.6583333333333333\n",
      "752 번째 loss, accuracy:  0.7739727912509676 0.6583333333333333\n",
      "753 번째 loss, accuracy:  0.7737429401499452 0.6583333333333333\n",
      "754 번째 loss, accuracy:  0.7735133148635431 0.6583333333333333\n",
      "755 번째 loss, accuracy:  0.773283909381565 0.6583333333333333\n",
      "756 번째 loss, accuracy:  0.7730547294198232 0.6583333333333333\n",
      "757 번째 loss, accuracy:  0.7728257626055682 0.6583333333333333\n",
      "758 번째 loss, accuracy:  0.7725970199869853 0.6583333333333333\n",
      "759 번째 loss, accuracy:  0.7723684969657719 0.6583333333333333\n",
      "760 번째 loss, accuracy:  0.7721401870041047 0.6583333333333333\n",
      "761 번째 loss, accuracy:  0.7719121027959627 0.6583333333333333\n",
      "762 번째 loss, accuracy:  0.7716842289968132 0.6583333333333333\n",
      "763 번째 loss, accuracy:  0.7714565860214257 0.6583333333333333\n",
      "764 번째 loss, accuracy:  0.7712291610074956 0.6583333333333333\n",
      "765 번째 loss, accuracy:  0.7710019485287454 0.6583333333333333\n",
      "766 번째 loss, accuracy:  0.7707749435079523 0.6583333333333333\n",
      "767 번째 loss, accuracy:  0.7705481742522831 0.6583333333333333\n",
      "768 번째 loss, accuracy:  0.7703216240681814 0.6583333333333333\n",
      "769 번째 loss, accuracy:  0.7700952899338999 0.6583333333333333\n",
      "770 번째 loss, accuracy:  0.7698691611719828 0.6583333333333333\n",
      "771 번째 loss, accuracy:  0.7696432621015623 0.6583333333333333\n",
      "772 번째 loss, accuracy:  0.7694175819774437 0.6583333333333333\n",
      "773 번째 loss, accuracy:  0.769192103307423 0.6583333333333333\n",
      "774 번째 loss, accuracy:  0.7689668333124451 0.6583333333333333\n",
      "775 번째 loss, accuracy:  0.7687418049752756 0.6583333333333333\n",
      "776 번째 loss, accuracy:  0.7685169865418793 0.6583333333333333\n",
      "777 번째 loss, accuracy:  0.7682923858013359 0.6583333333333333\n",
      "778 번째 loss, accuracy:  0.7680679990055517 0.6583333333333333\n",
      "779 번째 loss, accuracy:  0.7678438253120691 0.6583333333333333\n",
      "780 번째 loss, accuracy:  0.767619875891891 0.6583333333333333\n",
      "781 번째 loss, accuracy:  0.7673961353660308 0.6583333333333333\n",
      "782 번째 loss, accuracy:  0.7671726104326316 0.6583333333333333\n",
      "783 번째 loss, accuracy:  0.7669493049325278 0.6583333333333333\n",
      "784 번째 loss, accuracy:  0.7667262171130222 0.6583333333333333\n",
      "785 번째 loss, accuracy:  0.7665033452752656 0.6583333333333333\n",
      "786 번째 loss, accuracy:  0.76628067740942 0.6583333333333333\n",
      "787 번째 loss, accuracy:  0.7660582263800708 0.6583333333333333\n",
      "788 번째 loss, accuracy:  0.7658359927445748 0.6583333333333333\n",
      "789 번째 loss, accuracy:  0.7656139510447714 0.6583333333333333\n",
      "790 번째 loss, accuracy:  0.765392147104207 0.6583333333333333\n",
      "791 번째 loss, accuracy:  0.7651705411218732 0.6583333333333333\n",
      "792 번째 loss, accuracy:  0.764949170102626 0.6583333333333333\n",
      "793 번째 loss, accuracy:  0.764728011637272 0.6583333333333333\n",
      "794 번째 loss, accuracy:  0.7645070654289978 0.6583333333333333\n",
      "795 번째 loss, accuracy:  0.7642863335333767 0.6583333333333333\n",
      "796 번째 loss, accuracy:  0.7640658120409126 0.6583333333333333\n",
      "797 번째 loss, accuracy:  0.7638455024425129 0.6583333333333333\n",
      "798 번째 loss, accuracy:  0.7636254090993627 0.6583333333333333\n",
      "799 번째 loss, accuracy:  0.7634055221494129 0.6583333333333333\n",
      "800 번째 loss, accuracy:  0.7631858429031207 0.6583333333333333\n",
      "801 번째 loss, accuracy:  0.762966380216452 0.6583333333333333\n",
      "802 번째 loss, accuracy:  0.7627471303289518 0.6583333333333333\n",
      "803 번째 loss, accuracy:  0.7625280962011421 0.6583333333333333\n",
      "804 번째 loss, accuracy:  0.7623092675909563 0.6583333333333333\n",
      "805 번째 loss, accuracy:  0.7620906557586206 0.6583333333333333\n",
      "806 번째 loss, accuracy:  0.7618722532510582 0.6583333333333333\n",
      "807 번째 loss, accuracy:  0.76165406268226 0.6583333333333333\n",
      "808 번째 loss, accuracy:  0.7614360817135563 0.6583333333333333\n",
      "809 번째 loss, accuracy:  0.7612183120727197 0.6583333333333333\n",
      "810 번째 loss, accuracy:  0.761000751646472 0.6583333333333333\n",
      "811 번째 loss, accuracy:  0.7607834026568859 0.6583333333333333\n",
      "812 번째 loss, accuracy:  0.7605662481774661 0.6583333333333333\n",
      "813 번째 loss, accuracy:  0.7603493190829916 0.6583333333333333\n",
      "814 번째 loss, accuracy:  0.7601325984023395 0.6583333333333333\n",
      "815 번째 loss, accuracy:  0.759916089345203 0.6583333333333333\n",
      "816 번째 loss, accuracy:  0.7596997841047904 0.6583333333333333\n",
      "817 번째 loss, accuracy:  0.7594836896373186 0.6583333333333333\n",
      "818 번째 loss, accuracy:  0.759267801925634 0.6583333333333333\n",
      "819 번째 loss, accuracy:  0.7590521066461547 0.6583333333333333\n",
      "820 번째 loss, accuracy:  0.7588366335237418 0.6583333333333333\n",
      "821 번째 loss, accuracy:  0.7586213776545544 0.6583333333333333\n",
      "822 번째 loss, accuracy:  0.7584063148536314 0.6583333333333333\n",
      "823 번째 loss, accuracy:  0.7581914733908711 0.6583333333333333\n",
      "824 번째 loss, accuracy:  0.7579768352585112 0.6583333333333333\n",
      "825 번째 loss, accuracy:  0.7577624048389839 0.6583333333333333\n",
      "826 번째 loss, accuracy:  0.7575481825794937 0.6583333333333333\n",
      "827 번째 loss, accuracy:  0.7573341705205112 0.6583333333333333\n",
      "828 번째 loss, accuracy:  0.7571203608192555 0.6583333333333333\n",
      "829 번째 loss, accuracy:  0.7569067583227247 0.6583333333333333\n",
      "830 번째 loss, accuracy:  0.756693366559084 0.6583333333333333\n",
      "831 번째 loss, accuracy:  0.756480171875121 0.6583333333333333\n",
      "832 번째 loss, accuracy:  0.7562671924584284 0.6583333333333333\n",
      "833 번째 loss, accuracy:  0.7560544195250608 0.6583333333333333\n",
      "834 번째 loss, accuracy:  0.7558418508735714 0.6583333333333333\n",
      "835 번째 loss, accuracy:  0.7556294785453296 0.6583333333333333\n",
      "836 번째 loss, accuracy:  0.7554173223959457 0.6583333333333333\n",
      "837 번째 loss, accuracy:  0.7552053646832436 0.6583333333333333\n",
      "838 번째 loss, accuracy:  0.7549936114951386 0.6583333333333333\n",
      "839 번째 loss, accuracy:  0.7547820703918158 0.6583333333333333\n",
      "840 번째 loss, accuracy:  0.7545707307241617 0.6583333333333333\n",
      "841 번째 loss, accuracy:  0.7543596001194712 0.6583333333333333\n",
      "842 번째 loss, accuracy:  0.7541486722116694 0.6583333333333333\n",
      "843 번째 loss, accuracy:  0.7539379461946064 0.6583333333333333\n",
      "844 번째 loss, accuracy:  0.7537274280100165 0.6583333333333333\n",
      "845 번째 loss, accuracy:  0.7535171091422593 0.6583333333333333\n",
      "846 번째 loss, accuracy:  0.7533069968931757 0.6583333333333333\n",
      "847 번째 loss, accuracy:  0.7530970584533893 0.6583333333333333\n",
      "848 번째 loss, accuracy:  0.7528873498671788 0.6583333333333333\n",
      "849 번째 loss, accuracy:  0.7526778504980122 0.6583333333333333\n",
      "850 번째 loss, accuracy:  0.7524685514514454 0.6583333333333333\n",
      "851 번째 loss, accuracy:  0.7522594529025248 0.6583333333333333\n",
      "852 번째 loss, accuracy:  0.7520505559228939 0.6583333333333333\n",
      "853 번째 loss, accuracy:  0.7518418511989914 0.6583333333333333\n",
      "854 번째 loss, accuracy:  0.7516333590060621 0.6583333333333333\n",
      "855 번째 loss, accuracy:  0.7514250724007507 0.6583333333333333\n",
      "856 번째 loss, accuracy:  0.7512169813387934 0.6583333333333333\n",
      "857 번째 loss, accuracy:  0.7510090930581479 0.6583333333333333\n",
      "858 번째 loss, accuracy:  0.7508014081595485 0.6583333333333333\n",
      "859 번째 loss, accuracy:  0.7505939244114077 0.6583333333333333\n",
      "860 번째 loss, accuracy:  0.7503866434349521 0.6583333333333333\n",
      "861 번째 loss, accuracy:  0.7501795665470795 0.6583333333333333\n",
      "862 번째 loss, accuracy:  0.7499726867503429 0.6583333333333333\n",
      "863 번째 loss, accuracy:  0.7497660108013259 0.6583333333333333\n",
      "864 번째 loss, accuracy:  0.7495595280636365 0.6583333333333333\n",
      "865 번째 loss, accuracy:  0.7493532469299443 0.6583333333333333\n",
      "866 번째 loss, accuracy:  0.749147168189593 0.6583333333333333\n",
      "867 번째 loss, accuracy:  0.7489412896411041 0.6583333333333333\n",
      "868 번째 loss, accuracy:  0.7487356133308641 0.6583333333333333\n",
      "869 번째 loss, accuracy:  0.7485301390670057 0.6583333333333333\n",
      "870 번째 loss, accuracy:  0.7483248610929406 0.6583333333333333\n",
      "871 번째 loss, accuracy:  0.7481197800845524 0.6583333333333333\n",
      "872 번째 loss, accuracy:  0.7479148925595531 0.6583333333333333\n",
      "873 번째 loss, accuracy:  0.7477102045608597 0.6583333333333333\n",
      "874 번째 loss, accuracy:  0.7475057210968965 0.6583333333333333\n",
      "875 번째 loss, accuracy:  0.7473014326030376 0.6583333333333333\n",
      "876 번째 loss, accuracy:  0.7470973390578641 0.6583333333333333\n",
      "877 번째 loss, accuracy:  0.7468934400586604 0.6583333333333333\n",
      "878 번째 loss, accuracy:  0.746689749543329 0.6583333333333333\n",
      "879 번째 loss, accuracy:  0.7464862571589507 0.6583333333333333\n",
      "880 번째 loss, accuracy:  0.7462829588891146 0.6583333333333333\n",
      "881 번째 loss, accuracy:  0.7460798576642846 0.6583333333333333\n",
      "882 번째 loss, accuracy:  0.7458769593405304 0.6583333333333333\n",
      "883 번째 loss, accuracy:  0.7456742433757114 0.6583333333333333\n",
      "884 번째 loss, accuracy:  0.7454717351353501 0.6583333333333333\n",
      "885 번째 loss, accuracy:  0.7452694195592706 0.6583333333333333\n",
      "886 번째 loss, accuracy:  0.7450673057103256 0.6583333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887 번째 loss, accuracy:  0.744865388424941 0.6583333333333333\n",
      "888 번째 loss, accuracy:  0.7446636591365992 0.6583333333333333\n",
      "889 번째 loss, accuracy:  0.7444621185457483 0.6583333333333333\n",
      "890 번째 loss, accuracy:  0.7442607703325707 0.6583333333333333\n",
      "891 번째 loss, accuracy:  0.7440596328916484 0.6583333333333333\n",
      "892 번째 loss, accuracy:  0.7438586848400753 0.6583333333333333\n",
      "893 번째 loss, accuracy:  0.7436579271198288 0.6583333333333333\n",
      "894 번째 loss, accuracy:  0.7434573751081082 0.6583333333333333\n",
      "895 번째 loss, accuracy:  0.7432569984241848 0.6583333333333333\n",
      "896 번째 loss, accuracy:  0.7430568326136118 0.6583333333333333\n",
      "897 번째 loss, accuracy:  0.7428568608716132 0.6583333333333333\n",
      "898 번째 loss, accuracy:  0.7426570811763277 0.6583333333333333\n",
      "899 번째 loss, accuracy:  0.7424575018624894 0.6583333333333333\n",
      "900 번째 loss, accuracy:  0.7422581187661661 0.6583333333333333\n",
      "901 번째 loss, accuracy:  0.7420589246701883 0.6583333333333333\n",
      "902 번째 loss, accuracy:  0.7418599288299021 0.6583333333333333\n",
      "903 번째 loss, accuracy:  0.741661122547261 0.6583333333333333\n",
      "904 번째 loss, accuracy:  0.7414625124538934 0.6583333333333333\n",
      "905 번째 loss, accuracy:  0.7412640935522828 0.6583333333333333\n",
      "906 번째 loss, accuracy:  0.7410658674495914 0.6583333333333333\n",
      "907 번째 loss, accuracy:  0.7408678355413048 0.6583333333333333\n",
      "908 번째 loss, accuracy:  0.7406699962153613 0.6583333333333333\n",
      "909 번째 loss, accuracy:  0.7404723468842692 0.6583333333333333\n",
      "910 번째 loss, accuracy:  0.7402748903272285 0.6583333333333333\n",
      "911 번째 loss, accuracy:  0.7400776249248494 0.6583333333333333\n",
      "912 번째 loss, accuracy:  0.7398805468334935 0.6583333333333333\n",
      "913 번째 loss, accuracy:  0.7396836612449976 0.6583333333333333\n",
      "914 번째 loss, accuracy:  0.7394869684773184 0.6583333333333333\n",
      "915 번째 loss, accuracy:  0.7392904691556065 0.6583333333333333\n",
      "916 번째 loss, accuracy:  0.7390941587151526 0.6583333333333333\n",
      "917 번째 loss, accuracy:  0.7388980395351842 0.6583333333333333\n",
      "918 번째 loss, accuracy:  0.7387021126878558 0.6583333333333333\n",
      "919 번째 loss, accuracy:  0.7385063772137799 0.6583333333333333\n",
      "920 번째 loss, accuracy:  0.7383108261805102 0.6583333333333333\n",
      "921 번째 loss, accuracy:  0.7381154716002529 0.6583333333333333\n",
      "922 번째 loss, accuracy:  0.7379203018115913 0.6583333333333333\n",
      "923 번째 loss, accuracy:  0.7377253165769296 0.6583333333333333\n",
      "924 번째 loss, accuracy:  0.737530526945538 0.6583333333333333\n",
      "925 번째 loss, accuracy:  0.7373359284553374 0.6583333333333333\n",
      "926 번째 loss, accuracy:  0.7371415192159124 0.6583333333333333\n",
      "927 번째 loss, accuracy:  0.7369473012962592 0.6583333333333333\n",
      "928 번째 loss, accuracy:  0.7367532688566121 0.6583333333333333\n",
      "929 번째 loss, accuracy:  0.7365594257672418 0.6583333333333333\n",
      "930 번째 loss, accuracy:  0.7363657720850989 0.6583333333333333\n",
      "931 번째 loss, accuracy:  0.7361722997802812 0.6583333333333333\n",
      "932 번째 loss, accuracy:  0.7359790213383909 0.6583333333333333\n",
      "933 번째 loss, accuracy:  0.7357859277189452 0.6583333333333333\n",
      "934 번째 loss, accuracy:  0.7355930239050908 0.6583333333333333\n",
      "935 번째 loss, accuracy:  0.7354003060117879 0.6583333333333333\n",
      "936 번째 loss, accuracy:  0.7352077753190548 0.6583333333333333\n",
      "937 번째 loss, accuracy:  0.7350154349750089 0.6583333333333333\n",
      "938 번째 loss, accuracy:  0.7348232768307696 0.6583333333333333\n",
      "939 번째 loss, accuracy:  0.7346313141099415 0.6583333333333333\n",
      "940 번째 loss, accuracy:  0.7344395353264316 0.6583333333333333\n",
      "941 번째 loss, accuracy:  0.7342479316112898 0.6583333333333333\n",
      "942 번째 loss, accuracy:  0.7340565075368007 0.6583333333333333\n",
      "943 번째 loss, accuracy:  0.7338652835925848 0.6583333333333333\n",
      "944 번째 loss, accuracy:  0.733674242506934 0.6583333333333333\n",
      "945 번째 loss, accuracy:  0.7334833877574792 0.6583333333333333\n",
      "946 번째 loss, accuracy:  0.7332927225901779 0.6583333333333333\n",
      "947 번째 loss, accuracy:  0.7331022395937133 0.6583333333333333\n",
      "948 번째 loss, accuracy:  0.732911948540656 0.6583333333333333\n",
      "949 번째 loss, accuracy:  0.7327218295510939 0.6583333333333333\n",
      "950 번째 loss, accuracy:  0.7325319058980239 0.6583333333333333\n",
      "951 번째 loss, accuracy:  0.7323421690602975 0.6583333333333333\n",
      "952 번째 loss, accuracy:  0.7321526156067125 0.6583333333333333\n",
      "953 번째 loss, accuracy:  0.7319632429002458 0.6583333333333333\n",
      "954 번째 loss, accuracy:  0.7317740516660426 0.6583333333333333\n",
      "955 번째 loss, accuracy:  0.7315850475916027 0.6583333333333333\n",
      "956 번째 loss, accuracy:  0.7313962277349763 0.6583333333333333\n",
      "957 번째 loss, accuracy:  0.7312075905107683 0.6583333333333333\n",
      "958 번째 loss, accuracy:  0.7310191307069557 0.6583333333333333\n",
      "959 번째 loss, accuracy:  0.7308308572713346 0.6583333333333333\n",
      "960 번째 loss, accuracy:  0.7306427700443917 0.6583333333333333\n",
      "961 번째 loss, accuracy:  0.7304548538977047 0.6583333333333333\n",
      "962 번째 loss, accuracy:  0.7302671354183442 0.6583333333333333\n",
      "963 번째 loss, accuracy:  0.7300795975420816 0.6583333333333333\n",
      "964 번째 loss, accuracy:  0.729892242906579 0.6583333333333333\n",
      "965 번째 loss, accuracy:  0.729705066798099 0.6583333333333333\n",
      "966 번째 loss, accuracy:  0.7295180768354871 0.6583333333333333\n",
      "967 번째 loss, accuracy:  0.7293312676115746 0.6583333333333333\n",
      "968 번째 loss, accuracy:  0.7291446397098501 0.6583333333333333\n",
      "969 번째 loss, accuracy:  0.7289581954671916 0.6583333333333333\n",
      "970 번째 loss, accuracy:  0.7287719277969263 0.6583333333333333\n",
      "971 번째 loss, accuracy:  0.728585839936621 0.6583333333333333\n",
      "972 번째 loss, accuracy:  0.7283999394423046 0.6583333333333333\n",
      "973 번째 loss, accuracy:  0.7282142166829177 0.6583333333333333\n",
      "974 번째 loss, accuracy:  0.7280286739650056 0.6583333333333333\n",
      "975 번째 loss, accuracy:  0.7278433071041855 0.6583333333333333\n",
      "976 번째 loss, accuracy:  0.7276581278914598 0.65\n",
      "977 번째 loss, accuracy:  0.7274731280982053 0.65\n",
      "978 번째 loss, accuracy:  0.7272883099601444 0.65\n",
      "979 번째 loss, accuracy:  0.7271036672903974 0.65\n",
      "980 번째 loss, accuracy:  0.7269192020845899 0.65\n",
      "981 번째 loss, accuracy:  0.7267349239715296 0.65\n",
      "982 번째 loss, accuracy:  0.7265508131566212 0.65\n",
      "983 번째 loss, accuracy:  0.7263668897466865 0.65\n",
      "984 번째 loss, accuracy:  0.7261831468511465 0.65\n",
      "985 번째 loss, accuracy:  0.7259995743658116 0.65\n",
      "986 번째 loss, accuracy:  0.725816187226805 0.65\n",
      "987 번째 loss, accuracy:  0.7256329725364216 0.65\n",
      "988 번째 loss, accuracy:  0.7254499400002491 0.65\n",
      "989 번째 loss, accuracy:  0.7252670866492108 0.65\n",
      "990 번째 loss, accuracy:  0.7250844095497937 0.65\n",
      "991 번째 loss, accuracy:  0.7249019131065295 0.65\n",
      "992 번째 loss, accuracy:  0.7247195718559778 0.65\n",
      "993 번째 loss, accuracy:  0.7245374276531249 0.65\n",
      "994 번째 loss, accuracy:  0.7243554574041049 0.65\n",
      "995 번째 loss, accuracy:  0.7241736679324402 0.65\n",
      "996 번째 loss, accuracy:  0.723992053654458 0.65\n",
      "997 번째 loss, accuracy:  0.7238106183497551 0.65\n",
      "998 번째 loss, accuracy:  0.7236293590736469 0.65\n",
      "999 번째 loss, accuracy:  0.7234482710614254 0.65\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 loss, accuracy:  1.4956160807420733 0.35833333333333334\n",
      "1 번째 loss, accuracy:  1.4884735356307286 0.35833333333333334\n",
      "2 번째 loss, accuracy:  1.4812579079096182 0.35833333333333334\n",
      "3 번째 loss, accuracy:  1.4739748752431725 0.35833333333333334\n",
      "4 번째 loss, accuracy:  1.4666308821719725 0.35833333333333334\n",
      "5 번째 loss, accuracy:  1.4592331278806114 0.35833333333333334\n",
      "6 번째 loss, accuracy:  1.4517895404341237 0.35833333333333334\n",
      "7 번째 loss, accuracy:  1.444308736799535 0.35833333333333334\n",
      "8 번째 loss, accuracy:  1.4367999683814034 0.35833333333333334\n",
      "9 번째 loss, accuracy:  1.429273052275304 0.35833333333333334\n",
      "10 번째 loss, accuracy:  1.4217382889608337 0.35833333333333334\n",
      "11 번째 loss, accuracy:  1.4142063676911065 0.35833333333333334\n",
      "12 번째 loss, accuracy:  1.4066882613574117 0.35833333333333334\n",
      "13 번째 loss, accuracy:  1.3991951130845266 0.35833333333333334\n",
      "14 번째 loss, accuracy:  1.3917381172102943 0.35833333333333334\n",
      "15 번째 loss, accuracy:  1.3843283975947456 0.35833333333333334\n",
      "16 번째 loss, accuracy:  1.3769768863658494 0.35833333333333334\n",
      "17 번째 loss, accuracy:  1.3696942062273771 0.35833333333333334\n",
      "18 번째 loss, accuracy:  1.3624905593258754 0.35833333333333334\n",
      "19 번째 loss, accuracy:  1.3553756254050393 0.35833333333333334\n",
      "20 번째 loss, accuracy:  1.3483584715863166 0.35833333333333334\n",
      "21 번째 loss, accuracy:  1.341447475628786 0.35833333333333334\n",
      "22 번째 loss, accuracy:  1.3346502639731195 0.35833333333333334\n",
      "23 번째 loss, accuracy:  1.3279736652982346 0.35833333333333334\n",
      "24 번째 loss, accuracy:  1.321423679750007 0.35833333333333334\n",
      "25 번째 loss, accuracy:  1.3150054634707355 0.35833333333333334\n",
      "26 번째 loss, accuracy:  1.308723327591479 0.35833333333333334\n",
      "27 번째 loss, accuracy:  1.302580750465718 0.35833333333333334\n",
      "28 번째 loss, accuracy:  1.2965804016323144 0.35833333333333334\n",
      "29 번째 loss, accuracy:  1.2907241758026076 0.35833333333333334\n",
      "30 번째 loss, accuracy:  1.285013235065936 0.35833333333333334\n",
      "31 번째 loss, accuracy:  1.2794480574923752 0.35833333333333334\n",
      "32 번째 loss, accuracy:  1.2740284903677872 0.35833333333333334\n",
      "33 번째 loss, accuracy:  1.2687538064099961 0.35833333333333334\n",
      "34 번째 loss, accuracy:  1.2636227614711046 0.35833333333333334\n",
      "35 번째 loss, accuracy:  1.2586336524142383 0.35833333333333334\n",
      "36 번째 loss, accuracy:  1.2537843740508903 0.35833333333333334\n",
      "37 번째 loss, accuracy:  1.249072474225565 0.35833333333333334\n",
      "38 번째 loss, accuracy:  1.2444952063288315 0.35833333333333334\n",
      "39 번째 loss, accuracy:  1.2400495787016002 0.35833333333333334\n",
      "40 번째 loss, accuracy:  1.2357324005572896 0.35833333333333334\n",
      "41 번째 loss, accuracy:  1.231540324192658 0.35833333333333334\n",
      "42 번째 loss, accuracy:  1.2274698833804247 0.35833333333333334\n",
      "43 번째 loss, accuracy:  1.2235175279384158 0.35833333333333334\n",
      "44 번째 loss, accuracy:  1.219679654551087 0.35833333333333334\n",
      "45 번째 loss, accuracy:  1.215952633982146 0.35833333333333334\n",
      "46 번째 loss, accuracy:  1.212332834863379 0.35833333333333334\n",
      "47 번째 loss, accuracy:  1.2088166442767767 0.35833333333333334\n",
      "48 번째 loss, accuracy:  1.2054004853673466 0.35833333333333334\n",
      "49 번째 loss, accuracy:  1.2020808322340284 0.35833333333333334\n",
      "50 번째 loss, accuracy:  1.1988542223485132 0.35833333333333334\n",
      "51 번째 loss, accuracy:  1.1957172667479392 0.35833333333333334\n",
      "52 번째 loss, accuracy:  1.192666658238725 0.35833333333333334\n",
      "53 번째 loss, accuracy:  1.1896991778372585 0.35833333333333334\n",
      "54 번째 loss, accuracy:  1.1868116996588474 0.35833333333333334\n",
      "55 번째 loss, accuracy:  1.1840011944510744 0.35833333333333334\n",
      "56 번째 loss, accuracy:  1.1812647319514749 0.35833333333333334\n",
      "57 번째 loss, accuracy:  1.1785994822333743 0.35833333333333334\n",
      "58 번째 loss, accuracy:  1.1760027161878617 0.35833333333333334\n",
      "59 번째 loss, accuracy:  1.173471805274392 0.35833333333333334\n",
      "60 번째 loss, accuracy:  1.1710042206582898 0.35833333333333334\n",
      "61 번째 loss, accuracy:  1.1685975318397575 0.35833333333333334\n",
      "62 번째 loss, accuracy:  1.1662494048664789 0.35833333333333334\n",
      "63 번째 loss, accuracy:  1.1639576002104002 0.35833333333333334\n",
      "64 번째 loss, accuracy:  1.1617199703790957 0.35833333333333334\n",
      "65 번째 loss, accuracy:  1.1595344573222943 0.35833333333333334\n",
      "66 번째 loss, accuracy:  1.1573990896861726 0.35833333333333334\n",
      "67 번째 loss, accuracy:  1.155311979960085 0.35833333333333334\n",
      "68 번째 loss, accuracy:  1.1532713215540547 0.35833333333333334\n",
      "69 번째 loss, accuracy:  1.1512753858392528 0.35833333333333334\n",
      "70 번째 loss, accuracy:  1.1493225191785812 0.35833333333333334\n",
      "71 번째 loss, accuracy:  1.1474111399699245 0.35833333333333334\n",
      "72 번째 loss, accuracy:  1.1455397357207937 0.35833333333333334\n",
      "73 번째 loss, accuracy:  1.143706860169441 0.35833333333333334\n",
      "74 번째 loss, accuracy:  1.141911130464751 0.35833333333333334\n",
      "75 번째 loss, accuracy:  1.1401512244146181 0.35833333333333334\n",
      "76 번째 loss, accuracy:  1.1384258778102307 0.35833333333333334\n",
      "77 번째 loss, accuracy:  1.136733881831994 0.35833333333333334\n",
      "78 번째 loss, accuracy:  1.1350740805410477 0.35833333333333334\n",
      "79 번째 loss, accuracy:  1.133445368459141 0.35833333333333334\n",
      "80 번째 loss, accuracy:  1.1318466882384262 0.35833333333333334\n",
      "81 번째 loss, accuracy:  1.1302770284217876 0.35833333333333334\n",
      "82 번째 loss, accuracy:  1.1287354212936358 0.35833333333333334\n",
      "83 번째 loss, accuracy:  1.1272209408202791 0.35833333333333334\n",
      "84 번째 loss, accuracy:  1.1257327006786417 0.35833333333333334\n",
      "85 번째 loss, accuracy:  1.1242698523716101 0.35833333333333334\n",
      "86 번째 loss, accuracy:  1.1228315834278968 0.35833333333333334\n",
      "87 번째 loss, accuracy:  1.1214171156841466 0.35833333333333334\n",
      "88 번째 loss, accuracy:  1.1200257036468135 0.35833333333333334\n",
      "89 번째 loss, accuracy:  1.1186566329310703 0.35833333333333334\n",
      "90 번째 loss, accuracy:  1.1173092187741296 0.35833333333333334\n",
      "91 번째 loss, accuracy:  1.1159828046200553 0.35833333333333334\n",
      "92 번째 loss, accuracy:  1.1146767607733705 0.35833333333333334\n",
      "93 번째 loss, accuracy:  1.1133904831185437 0.35833333333333334\n",
      "94 번째 loss, accuracy:  1.1121233919026046 0.35833333333333334\n",
      "95 번째 loss, accuracy:  1.110874930578117 0.35833333333333334\n",
      "96 번째 loss, accuracy:  1.1096445647037794 0.35833333333333334\n",
      "97 번째 loss, accuracy:  1.1084317809000785 0.35833333333333334\n",
      "98 번째 loss, accuracy:  1.1072360858573314 0.35833333333333334\n",
      "99 번째 loss, accuracy:  1.1060570053937075 0.35833333333333334\n",
      "100 번째 loss, accuracy:  1.1048940835607572 0.35833333333333334\n",
      "101 번째 loss, accuracy:  1.1037468817941878 0.35833333333333334\n",
      "102 번째 loss, accuracy:  1.1026149781075827 0.35833333333333334\n",
      "103 번째 loss, accuracy:  1.1014979663269888 0.35833333333333334\n",
      "104 번째 loss, accuracy:  1.1003954553642497 0.35833333333333334\n",
      "105 번째 loss, accuracy:  1.0993070685271817 0.35833333333333334\n",
      "106 번째 loss, accuracy:  1.0982324428646553 0.35833333333333334\n",
      "107 번째 loss, accuracy:  1.097171228544831 0.35833333333333334\n",
      "108 번째 loss, accuracy:  1.0961230882648023 0.35833333333333334\n",
      "109 번째 loss, accuracy:  1.095087696690056 0.35833333333333334\n",
      "110 번째 loss, accuracy:  1.0940647399221453 0.35833333333333334\n",
      "111 번째 loss, accuracy:  1.0930539149931633 0.35833333333333334\n",
      "112 번째 loss, accuracy:  1.0920549293855693 0.35833333333333334\n",
      "113 번째 loss, accuracy:  1.0910675005760295 0.35833333333333334\n",
      "114 번째 loss, accuracy:  1.0900913556020702 0.35833333333333334\n",
      "115 번째 loss, accuracy:  1.089126230650282 0.35833333333333334\n",
      "116 번째 loss, accuracy:  1.0881718706649264 0.35833333333333334\n",
      "117 번째 loss, accuracy:  1.0872280289759773 0.35833333333333334\n",
      "118 번째 loss, accuracy:  1.0862944669454182 0.35833333333333334\n",
      "119 번째 loss, accuracy:  1.08537095363096 0.35833333333333334\n",
      "120 번째 loss, accuracy:  1.0844572654661837 0.35833333333333334\n",
      "121 번째 loss, accuracy:  1.0835531859562952 0.35833333333333334\n",
      "122 번째 loss, accuracy:  1.0826585053886097 0.35833333333333334\n",
      "123 번째 loss, accuracy:  1.081773020557047 0.35833333333333334\n",
      "124 번째 loss, accuracy:  1.0808965344998849 0.35833333333333334\n",
      "125 번째 loss, accuracy:  1.0800288562500397 0.35833333333333334\n",
      "126 번째 loss, accuracy:  1.0791698005972683 0.35833333333333334\n",
      "127 번째 loss, accuracy:  1.0783191878616378 0.35833333333333334\n",
      "128 번째 loss, accuracy:  1.0774768436776838 0.35833333333333334\n",
      "129 번째 loss, accuracy:  1.0766425987886876 0.35833333333333334\n",
      "130 번째 loss, accuracy:  1.075816288850585 0.35833333333333334\n",
      "131 번째 loss, accuracy:  1.0749977542449358 0.35833333333333334\n",
      "132 번째 loss, accuracy:  1.0741868399005667 0.35833333333333334\n",
      "133 번째 loss, accuracy:  1.0733833951233822 0.35833333333333334\n",
      "134 번째 loss, accuracy:  1.0725872734339432 0.35833333333333334\n",
      "135 번째 loss, accuracy:  1.0717983324124298 0.35833333333333334\n",
      "136 번째 loss, accuracy:  1.0710164335505645 0.35833333333333334\n",
      "137 번째 loss, accuracy:  1.0702414421102036 0.35833333333333334\n",
      "138 번째 loss, accuracy:  1.0694732269882077 0.35833333333333334\n",
      "139 번째 loss, accuracy:  1.0687116605872962 0.35833333333333334\n",
      "140 번째 loss, accuracy:  1.067956618692577 0.35833333333333334\n",
      "141 번째 loss, accuracy:  1.067207980353479 0.35833333333333334\n",
      "142 번째 loss, accuracy:  1.0664656277707927 0.35833333333333334\n",
      "143 번째 loss, accuracy:  1.0657294461885807 0.35833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 번째 loss, accuracy:  1.064999323790721 0.35833333333333334\n",
      "145 번째 loss, accuracy:  1.0642751516018436 0.35833333333333334\n",
      "146 번째 loss, accuracy:  1.0635568233924422 0.35833333333333334\n",
      "147 번째 loss, accuracy:  1.062844235587972 0.35833333333333334\n",
      "148 번째 loss, accuracy:  1.0621372871817092 0.35833333333333334\n",
      "149 번째 loss, accuracy:  1.061435879651231 0.35833333333333334\n",
      "150 번째 loss, accuracy:  1.0607399168783023 0.35833333333333334\n",
      "151 번째 loss, accuracy:  1.0600493050720305 0.35833333333333334\n",
      "152 번째 loss, accuracy:  1.0593639526951142 0.35833333333333334\n",
      "153 번째 loss, accuracy:  1.0586837703930558 0.35833333333333334\n",
      "154 번째 loss, accuracy:  1.058008670926189 0.35833333333333334\n",
      "155 번째 loss, accuracy:  1.0573385691043735 0.35833333333333334\n",
      "156 번째 loss, accuracy:  1.056673381724268 0.35833333333333334\n",
      "157 번째 loss, accuracy:  1.0560130275090243 0.35833333333333334\n",
      "158 번째 loss, accuracy:  1.0553574270503099 0.35833333333333334\n",
      "159 번째 loss, accuracy:  1.0547065027525566 0.35833333333333334\n",
      "160 번째 loss, accuracy:  1.0540601787792991 0.35833333333333334\n",
      "161 번째 loss, accuracy:  1.0534183810015687 0.35833333333333334\n",
      "162 번째 loss, accuracy:  1.0527810369481776 0.35833333333333334\n",
      "163 번째 loss, accuracy:  1.0521480757578716 0.35833333333333334\n",
      "164 번째 loss, accuracy:  1.0515194281332303 0.35833333333333334\n",
      "165 번째 loss, accuracy:  1.0508950262962333 0.35833333333333334\n",
      "166 번째 loss, accuracy:  1.0502748039454586 0.35833333333333334\n",
      "167 번째 loss, accuracy:  1.0496586962148007 0.35833333333333334\n",
      "168 번째 loss, accuracy:  1.0490466396336469 0.35833333333333334\n",
      "169 번째 loss, accuracy:  1.0484385720884775 0.35833333333333334\n",
      "170 번째 loss, accuracy:  1.0478344327857994 0.35833333333333334\n",
      "171 번째 loss, accuracy:  1.0472341622163672 0.35833333333333334\n",
      "172 번째 loss, accuracy:  1.0466377021206328 0.35833333333333334\n",
      "173 번째 loss, accuracy:  1.046044995455378 0.35833333333333334\n",
      "174 번째 loss, accuracy:  1.0454559863614743 0.35833333333333334\n",
      "175 번째 loss, accuracy:  1.0448706201327251 0.35833333333333334\n",
      "176 번째 loss, accuracy:  1.0442888431857518 0.35833333333333334\n",
      "177 번째 loss, accuracy:  1.0437106030308627 0.35833333333333334\n",
      "178 번째 loss, accuracy:  1.0431358482438926 0.35833333333333334\n",
      "179 번째 loss, accuracy:  1.0425645284389398 0.35833333333333334\n",
      "180 번째 loss, accuracy:  1.041996594241998 0.35833333333333334\n",
      "181 번째 loss, accuracy:  1.0414319972654247 0.35833333333333334\n",
      "182 번째 loss, accuracy:  1.0408706900832232 0.35833333333333334\n",
      "183 번째 loss, accuracy:  1.0403126262070954 0.35833333333333334\n",
      "184 번째 loss, accuracy:  1.0397577600632508 0.35833333333333334\n",
      "185 번째 loss, accuracy:  1.0392060469699358 0.35833333333333334\n",
      "186 번째 loss, accuracy:  1.0386574431156423 0.35833333333333334\n",
      "187 번째 loss, accuracy:  1.0381119055379944 0.35833333333333334\n",
      "188 번째 loss, accuracy:  1.0375693921032696 0.35833333333333334\n",
      "189 번째 loss, accuracy:  1.037029861486539 0.35833333333333334\n",
      "190 번째 loss, accuracy:  1.0364932731523837 0.35833333333333334\n",
      "191 번째 loss, accuracy:  1.0359595873362137 0.35833333333333334\n",
      "192 번째 loss, accuracy:  1.035428765026102 0.35833333333333334\n",
      "193 번째 loss, accuracy:  1.0349007679451665 0.35833333333333334\n",
      "194 번째 loss, accuracy:  1.0343755585344716 0.35833333333333334\n",
      "195 번째 loss, accuracy:  1.03385309993639 0.35833333333333334\n",
      "196 번째 loss, accuracy:  1.0333333559784736 0.35833333333333334\n",
      "197 번째 loss, accuracy:  1.0328162911577636 0.35833333333333334\n",
      "198 번째 loss, accuracy:  1.0323018706255387 0.35833333333333334\n",
      "199 번째 loss, accuracy:  1.0317900601725067 0.35833333333333334\n",
      "200 번째 loss, accuracy:  1.0312808262143833 0.35833333333333334\n",
      "201 번째 loss, accuracy:  1.0307741357778926 0.35833333333333334\n",
      "202 번째 loss, accuracy:  1.030269956487134 0.35833333333333334\n",
      "203 번째 loss, accuracy:  1.029768256550329 0.35833333333333334\n",
      "204 번째 loss, accuracy:  1.0292690047469195 0.35833333333333334\n",
      "205 번째 loss, accuracy:  1.0287721704150168 0.35833333333333334\n",
      "206 번째 loss, accuracy:  1.0282777234391849 0.35833333333333334\n",
      "207 번째 loss, accuracy:  1.0277856342385456 0.35833333333333334\n",
      "208 번째 loss, accuracy:  1.0272958737551952 0.35833333333333334\n",
      "209 번째 loss, accuracy:  1.0268084134429352 0.35833333333333334\n",
      "210 번째 loss, accuracy:  1.0263232252562793 0.35833333333333334\n",
      "211 번째 loss, accuracy:  1.0258402816397547 0.35833333333333334\n",
      "212 번째 loss, accuracy:  1.025359555517479 0.35833333333333334\n",
      "213 번째 loss, accuracy:  1.0248810202829994 0.35833333333333334\n",
      "214 번째 loss, accuracy:  1.02440464978939 0.35833333333333334\n",
      "215 번째 loss, accuracy:  1.0239304183396047 0.35833333333333334\n",
      "216 번째 loss, accuracy:  1.0234583006770568 0.35833333333333334\n",
      "217 번째 loss, accuracy:  1.02298827197646 0.35833333333333334\n",
      "218 번째 loss, accuracy:  1.0225203078348761 0.35833333333333334\n",
      "219 번째 loss, accuracy:  1.0220543842629772 0.35833333333333334\n",
      "220 번째 loss, accuracy:  1.0215904776765512 0.35833333333333334\n",
      "221 번째 loss, accuracy:  1.021128564888185 0.35833333333333334\n",
      "222 번째 loss, accuracy:  1.0206686230991762 0.35833333333333334\n",
      "223 번째 loss, accuracy:  1.0202106298916094 0.35833333333333334\n",
      "224 번째 loss, accuracy:  1.0197545632206568 0.35833333333333334\n",
      "225 번째 loss, accuracy:  1.0193004014070344 0.35833333333333334\n",
      "226 번째 loss, accuracy:  1.0188481231296522 0.35833333333333334\n",
      "227 번째 loss, accuracy:  1.0183977074184425 0.35833333333333334\n",
      "228 번째 loss, accuracy:  1.0179491336473439 0.35833333333333334\n",
      "229 번째 loss, accuracy:  1.0175023815274562 0.35833333333333334\n",
      "230 번째 loss, accuracy:  1.0170574311003628 0.35833333333333334\n",
      "231 번째 loss, accuracy:  1.0166142627315906 0.35833333333333334\n",
      "232 번째 loss, accuracy:  1.0161728571042405 0.35833333333333334\n",
      "233 번째 loss, accuracy:  1.0157331952127526 0.35833333333333334\n",
      "234 번째 loss, accuracy:  1.0152952583568198 0.35833333333333334\n",
      "235 번째 loss, accuracy:  1.0148590281354286 0.35833333333333334\n",
      "236 번째 loss, accuracy:  1.0144244864410579 0.35833333333333334\n",
      "237 번째 loss, accuracy:  1.013991615453989 0.35833333333333334\n",
      "238 번째 loss, accuracy:  1.01356039763675 0.35833333333333334\n",
      "239 번째 loss, accuracy:  1.0131308157286874 0.35833333333333334\n",
      "240 번째 loss, accuracy:  1.0127028527406579 0.35833333333333334\n",
      "241 번째 loss, accuracy:  1.0122764919498364 0.35833333333333334\n",
      "242 번째 loss, accuracy:  1.0118517168946395 0.35833333333333334\n",
      "243 번째 loss, accuracy:  1.0114285113697663 0.35833333333333334\n",
      "244 번째 loss, accuracy:  1.011006859421339 0.35833333333333334\n",
      "245 번째 loss, accuracy:  1.0105867453421598 0.35833333333333334\n",
      "246 번째 loss, accuracy:  1.0101681536670604 0.35833333333333334\n",
      "247 번째 loss, accuracy:  1.0097510691683622 0.35833333333333334\n",
      "248 번째 loss, accuracy:  1.0093354768514367 0.35833333333333334\n",
      "249 번째 loss, accuracy:  1.0089213619503468 0.35833333333333334\n",
      "250 번째 loss, accuracy:  1.0085087099235972 0.35833333333333334\n",
      "251 번째 loss, accuracy:  1.0080975064499698 0.35833333333333334\n",
      "252 번째 loss, accuracy:  1.00768773742445 0.35833333333333334\n",
      "253 번째 loss, accuracy:  1.0072793889542362 0.35833333333333334\n",
      "254 번째 loss, accuracy:  1.0068724473548363 0.35833333333333334\n",
      "255 번째 loss, accuracy:  1.0064668991462435 0.35833333333333334\n",
      "256 번째 loss, accuracy:  1.0060627310492072 0.35833333333333334\n",
      "257 번째 loss, accuracy:  1.0056599299815545 0.35833333333333334\n",
      "258 번째 loss, accuracy:  1.005258483054623 0.35833333333333334\n",
      "259 번째 loss, accuracy:  1.0048583775697304 0.35833333333333334\n",
      "260 번째 loss, accuracy:  1.0044596010147548 0.35833333333333334\n",
      "261 번째 loss, accuracy:  1.00406214106076 0.35833333333333334\n",
      "262 번째 loss, accuracy:  1.0036659855586978 0.35833333333333334\n",
      "263 번째 loss, accuracy:  1.0032711225361859 0.35833333333333334\n",
      "264 번째 loss, accuracy:  1.00287754019434 0.35833333333333334\n",
      "265 번째 loss, accuracy:  1.0024852269046773 0.35833333333333334\n",
      "266 번째 loss, accuracy:  1.002094171206086 0.35833333333333334\n",
      "267 번째 loss, accuracy:  1.0017043618018469 0.35833333333333334\n",
      "268 번째 loss, accuracy:  1.0013157875567287 0.35833333333333334\n",
      "269 번째 loss, accuracy:  1.0009284374941338 0.35833333333333334\n",
      "270 번째 loss, accuracy:  1.0005423007932939 0.35833333333333334\n",
      "271 번째 loss, accuracy:  1.0001573667865424 0.35833333333333334\n",
      "272 번째 loss, accuracy:  0.9997736249566263 0.35833333333333334\n",
      "273 번째 loss, accuracy:  0.9993910649340758 0.35833333333333334\n",
      "274 번째 loss, accuracy:  0.9990096764946264 0.35833333333333334\n",
      "275 번째 loss, accuracy:  0.9986294495566937 0.35833333333333334\n",
      "276 번째 loss, accuracy:  0.9982503741788972 0.35833333333333334\n",
      "277 번째 loss, accuracy:  0.9978724405576325 0.35833333333333334\n",
      "278 번째 loss, accuracy:  0.9974956390246923 0.35833333333333334\n",
      "279 번째 loss, accuracy:  0.9971199600449351 0.35833333333333334\n",
      "280 번째 loss, accuracy:  0.9967453942140005 0.35833333333333334\n",
      "281 번째 loss, accuracy:  0.9963719322560661 0.35833333333333334\n",
      "282 번째 loss, accuracy:  0.9959995650216488 0.35833333333333334\n",
      "283 번째 loss, accuracy:  0.9956282834854541 0.35833333333333334\n",
      "284 번째 loss, accuracy:  0.9952580787442594 0.35833333333333334\n",
      "285 번째 loss, accuracy:  0.994888942014843 0.35833333333333334\n",
      "286 번째 loss, accuracy:  0.9945208646319585 0.35833333333333334\n",
      "287 번째 loss, accuracy:  0.994153838046334 0.35833333333333334\n",
      "288 번째 loss, accuracy:  0.9937878538227213 0.35833333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289 번째 loss, accuracy:  0.9934229036379887 0.35833333333333334\n",
      "290 번째 loss, accuracy:  0.9930589792792262 0.35833333333333334\n",
      "291 번째 loss, accuracy:  0.9926960726419122 0.35833333333333334\n",
      "292 번째 loss, accuracy:  0.992334175728105 0.35833333333333334\n",
      "293 번째 loss, accuracy:  0.991973280644665 0.35833333333333334\n",
      "294 번째 loss, accuracy:  0.9916133796015211 0.35833333333333334\n",
      "295 번째 loss, accuracy:  0.9912544649099552 0.35833333333333334\n",
      "296 번째 loss, accuracy:  0.9908965289809338 0.35833333333333334\n",
      "297 번째 loss, accuracy:  0.9905395643234677 0.35833333333333334\n",
      "298 번째 loss, accuracy:  0.9901835635429888 0.35833333333333334\n",
      "299 번째 loss, accuracy:  0.9898285193397797 0.35833333333333334\n",
      "300 번째 loss, accuracy:  0.9894744245074188 0.35833333333333334\n",
      "301 번째 loss, accuracy:  0.9891212719312551 0.35833333333333334\n",
      "302 번째 loss, accuracy:  0.988769054586914 0.35833333333333334\n",
      "303 번째 loss, accuracy:  0.9884177655388343 0.35833333333333334\n",
      "304 번째 loss, accuracy:  0.9880673979388239 0.35833333333333334\n",
      "305 번째 loss, accuracy:  0.9877179450246485 0.35833333333333334\n",
      "306 번째 loss, accuracy:  0.9873694001186526 0.35833333333333334\n",
      "307 번째 loss, accuracy:  0.9870217566263869 0.35833333333333334\n",
      "308 번째 loss, accuracy:  0.9866750080352805 0.35833333333333334\n",
      "309 번째 loss, accuracy:  0.9863291479133284 0.35833333333333334\n",
      "310 번째 loss, accuracy:  0.985984169907802 0.35833333333333334\n",
      "311 번째 loss, accuracy:  0.9856400677439914 0.35833333333333334\n",
      "312 번째 loss, accuracy:  0.9852968352239581 0.35833333333333334\n",
      "313 번째 loss, accuracy:  0.9849544662253217 0.35833333333333334\n",
      "314 번째 loss, accuracy:  0.9846129547000669 0.35833333333333334\n",
      "315 번째 loss, accuracy:  0.9842722946733619 0.35833333333333334\n",
      "316 번째 loss, accuracy:  0.9839324802424128 0.35833333333333334\n",
      "317 번째 loss, accuracy:  0.9835935055753322 0.35833333333333334\n",
      "318 번째 loss, accuracy:  0.9832553649100226 0.35833333333333334\n",
      "319 번째 loss, accuracy:  0.9829180525530927 0.35833333333333334\n",
      "320 번째 loss, accuracy:  0.98258156287878 0.35833333333333334\n",
      "321 번째 loss, accuracy:  0.9822458903279014 0.35833333333333334\n",
      "322 번째 loss, accuracy:  0.981911029406821 0.35833333333333334\n",
      "323 번째 loss, accuracy:  0.9815769746864348 0.35833333333333334\n",
      "324 번째 loss, accuracy:  0.9812437208011696 0.35833333333333334\n",
      "325 번째 loss, accuracy:  0.9809112624480144 0.35833333333333334\n",
      "326 번째 loss, accuracy:  0.9805795943855471 0.35833333333333334\n",
      "327 번째 loss, accuracy:  0.980248711432998 0.35833333333333334\n",
      "328 번째 loss, accuracy:  0.9799186084693223 0.35833333333333334\n",
      "329 번째 loss, accuracy:  0.9795892804322841 0.35833333333333334\n",
      "330 번째 loss, accuracy:  0.9792607223175632 0.35833333333333334\n",
      "331 번째 loss, accuracy:  0.9789329291778801 0.35833333333333334\n",
      "332 번째 loss, accuracy:  0.978605896122128 0.35833333333333334\n",
      "333 번째 loss, accuracy:  0.978279618314524 0.35833333333333334\n",
      "334 번째 loss, accuracy:  0.9779540909737752 0.35833333333333334\n",
      "335 번째 loss, accuracy:  0.9776293093722617 0.35833333333333334\n",
      "336 번째 loss, accuracy:  0.9773052688352316 0.35833333333333334\n",
      "337 번째 loss, accuracy:  0.9769819647400093 0.35833333333333334\n",
      "338 번째 loss, accuracy:  0.9766593925152169 0.35833333333333334\n",
      "339 번째 loss, accuracy:  0.9763375476400128 0.36666666666666664\n",
      "340 번째 loss, accuracy:  0.976016425643341 0.375\n",
      "341 번째 loss, accuracy:  0.9756960221031911 0.375\n",
      "342 번째 loss, accuracy:  0.9753763326458774 0.375\n",
      "343 번째 loss, accuracy:  0.9750573529453254 0.375\n",
      "344 번째 loss, accuracy:  0.9747390787223706 0.375\n",
      "345 번째 loss, accuracy:  0.9744215057440722 0.375\n",
      "346 번째 loss, accuracy:  0.9741046298230375 0.375\n",
      "347 번째 loss, accuracy:  0.9737884468167516 0.375\n",
      "348 번째 loss, accuracy:  0.9734729526269333 0.38333333333333336\n",
      "349 번째 loss, accuracy:  0.9731581431988874 0.4\n",
      "350 번째 loss, accuracy:  0.9728440145208731 0.4\n",
      "351 번째 loss, accuracy:  0.9725305626234854 0.4\n",
      "352 번째 loss, accuracy:  0.9722177835790443 0.4\n",
      "353 번째 loss, accuracy:  0.9719056735009975 0.4\n",
      "354 번째 loss, accuracy:  0.971594228543325 0.4083333333333333\n",
      "355 번째 loss, accuracy:  0.9712834448999683 0.4083333333333333\n",
      "356 번째 loss, accuracy:  0.970973318804252 0.4083333333333333\n",
      "357 번째 loss, accuracy:  0.9706638465283269 0.4083333333333333\n",
      "358 번째 loss, accuracy:  0.9703550243826229 0.4083333333333333\n",
      "359 번째 loss, accuracy:  0.9700468487153 0.4166666666666667\n",
      "360 번째 loss, accuracy:  0.9697393159117215 0.4166666666666667\n",
      "361 번째 loss, accuracy:  0.9694324223939289 0.4166666666666667\n",
      "362 번째 loss, accuracy:  0.9691261646201227 0.425\n",
      "363 번째 loss, accuracy:  0.9688205390841617 0.425\n",
      "364 번째 loss, accuracy:  0.9685155423150617 0.425\n",
      "365 번째 loss, accuracy:  0.9682111708765079 0.425\n",
      "366 번째 loss, accuracy:  0.9679074213663693 0.425\n",
      "367 번째 loss, accuracy:  0.9676042904162292 0.425\n",
      "368 번째 loss, accuracy:  0.9673017746909159 0.425\n",
      "369 번째 loss, accuracy:  0.9669998708880444 0.425\n",
      "370 번째 loss, accuracy:  0.9666985757375689 0.425\n",
      "371 번째 loss, accuracy:  0.9663978860013347 0.425\n",
      "372 번째 loss, accuracy:  0.9660977984726458 0.43333333333333335\n",
      "373 번째 loss, accuracy:  0.9657983099758322 0.44166666666666665\n",
      "374 번째 loss, accuracy:  0.9654994173658302 0.44166666666666665\n",
      "375 번째 loss, accuracy:  0.965201117527765 0.44166666666666665\n",
      "376 번째 loss, accuracy:  0.9649034073765453 0.44166666666666665\n",
      "377 번째 loss, accuracy:  0.9646062838564613 0.44166666666666665\n",
      "378 번째 loss, accuracy:  0.9643097439407804 0.45\n",
      "379 번째 loss, accuracy:  0.9640137846313726 0.45\n",
      "380 번째 loss, accuracy:  0.9637184029583139 0.45\n",
      "381 번째 loss, accuracy:  0.963423595979517 0.45\n",
      "382 번째 loss, accuracy:  0.9631293607803603 0.45\n",
      "383 번째 loss, accuracy:  0.9628356944733208 0.45\n",
      "384 번째 loss, accuracy:  0.9625425941976155 0.45\n",
      "385 번째 loss, accuracy:  0.9622500571188496 0.45\n",
      "386 번째 loss, accuracy:  0.9619580804286689 0.45\n",
      "387 번째 loss, accuracy:  0.9616666613444134 0.45\n",
      "388 번째 loss, accuracy:  0.9613757971087863 0.45\n",
      "389 번째 loss, accuracy:  0.96108548498952 0.45\n",
      "390 번째 loss, accuracy:  0.9607957222790484 0.45\n",
      "391 번째 loss, accuracy:  0.9605065062941901 0.45\n",
      "392 번째 loss, accuracy:  0.9602178343758272 0.45\n",
      "393 번째 loss, accuracy:  0.9599297038886003 0.45\n",
      "394 번째 loss, accuracy:  0.9596421122205949 0.45\n",
      "395 번째 loss, accuracy:  0.9593550567830454 0.45\n",
      "396 번째 loss, accuracy:  0.959068535010037 0.45\n",
      "397 번째 loss, accuracy:  0.9587825443582159 0.45\n",
      "398 번째 loss, accuracy:  0.9584970823064957 0.45\n",
      "399 번째 loss, accuracy:  0.9582121463557811 0.45\n",
      "400 번째 loss, accuracy:  0.9579277340286849 0.45\n",
      "401 번째 loss, accuracy:  0.957643842869253 0.45\n",
      "402 번째 loss, accuracy:  0.9573604704426996 0.45\n",
      "403 번째 loss, accuracy:  0.9570776143351366 0.4583333333333333\n",
      "404 번째 loss, accuracy:  0.9567952721533095 0.4666666666666667\n",
      "405 번째 loss, accuracy:  0.9565134415243502 0.4666666666666667\n",
      "406 번째 loss, accuracy:  0.9562321200955084 0.48333333333333334\n",
      "407 번째 loss, accuracy:  0.9559513055339117 0.48333333333333334\n",
      "408 번째 loss, accuracy:  0.9556709955263126 0.5\n",
      "409 번째 loss, accuracy:  0.9553911877788566 0.5083333333333333\n",
      "410 번째 loss, accuracy:  0.955111880016831 0.5083333333333333\n",
      "411 번째 loss, accuracy:  0.9548330699844352 0.5083333333333333\n",
      "412 번째 loss, accuracy:  0.9545547554445529 0.5083333333333333\n",
      "413 번째 loss, accuracy:  0.9542769341785166 0.5166666666666667\n",
      "414 번째 loss, accuracy:  0.9539996039858839 0.5166666666666667\n",
      "415 번째 loss, accuracy:  0.9537227626842222 0.5166666666666667\n",
      "416 번째 loss, accuracy:  0.953446408108885 0.5166666666666667\n",
      "417 번째 loss, accuracy:  0.9531705381128004 0.525\n",
      "418 번째 loss, accuracy:  0.9528951505662533 0.5333333333333333\n",
      "419 번째 loss, accuracy:  0.9526202433566863 0.5333333333333333\n",
      "420 번째 loss, accuracy:  0.9523458143884879 0.5333333333333333\n",
      "421 번째 loss, accuracy:  0.9520718615827921 0.5416666666666666\n",
      "422 번째 loss, accuracy:  0.9517983828772778 0.55\n",
      "423 번째 loss, accuracy:  0.9515253762259734 0.55\n",
      "424 번째 loss, accuracy:  0.9512528395990659 0.55\n",
      "425 번째 loss, accuracy:  0.9509807709827044 0.55\n",
      "426 번째 loss, accuracy:  0.9507091683788189 0.55\n",
      "427 번째 loss, accuracy:  0.9504380298049291 0.55\n",
      "428 번째 loss, accuracy:  0.9501673532939648 0.55\n",
      "429 번째 loss, accuracy:  0.9498971368940897 0.55\n",
      "430 번째 loss, accuracy:  0.9496273786685188 0.5666666666666667\n",
      "431 번째 loss, accuracy:  0.9493580766953481 0.5666666666666667\n",
      "432 번째 loss, accuracy:  0.9490892290673756 0.5666666666666667\n",
      "433 번째 loss, accuracy:  0.9488208338919405 0.575\n",
      "434 번째 loss, accuracy:  0.9485528892907524 0.575\n",
      "435 번째 loss, accuracy:  0.9482853933997265 0.575\n",
      "436 번째 loss, accuracy:  0.948018344368821 0.575\n",
      "437 번째 loss, accuracy:  0.9477517403618785 0.575\n",
      "438 번째 loss, accuracy:  0.9474855795564653 0.5833333333333334\n",
      "439 번째 loss, accuracy:  0.9472198601437218 0.5833333333333334\n",
      "440 번째 loss, accuracy:  0.9469545803282053 0.5833333333333334\n",
      "441 번째 loss, accuracy:  0.9466897383277375 0.5833333333333334\n",
      "442 번째 loss, accuracy:  0.9464253323732585 0.5833333333333334\n",
      "443 번째 loss, accuracy:  0.9461613607086812 0.5833333333333334\n",
      "444 번째 loss, accuracy:  0.9458978215907415 0.5833333333333334\n",
      "445 번째 loss, accuracy:  0.9456347132888638 0.5833333333333334\n",
      "446 번째 loss, accuracy:  0.9453720340850109 0.5833333333333334\n",
      "447 번째 loss, accuracy:  0.9451097822735514 0.5833333333333334\n",
      "448 번째 loss, accuracy:  0.9448479561611244 0.5916666666666667\n",
      "449 번째 loss, accuracy:  0.9445865540664985 0.6\n",
      "450 번째 loss, accuracy:  0.944325574320444 0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451 번째 loss, accuracy:  0.9440650152655994 0.6\n",
      "452 번째 loss, accuracy:  0.9438048752563468 0.6\n",
      "453 번째 loss, accuracy:  0.9435451526586754 0.6\n",
      "454 번째 loss, accuracy:  0.9432858458500667 0.6\n",
      "455 번째 loss, accuracy:  0.9430269532193605 0.6083333333333333\n",
      "456 번째 loss, accuracy:  0.9427684731666381 0.6083333333333333\n",
      "457 번째 loss, accuracy:  0.9425104041031062 0.6083333333333333\n",
      "458 번째 loss, accuracy:  0.942252744450967 0.6083333333333333\n",
      "459 번째 loss, accuracy:  0.9419954926433104 0.6083333333333333\n",
      "460 번째 loss, accuracy:  0.941738647123992 0.6083333333333333\n",
      "461 번째 loss, accuracy:  0.9414822063475239 0.6083333333333333\n",
      "462 번째 loss, accuracy:  0.9412261687789593 0.625\n",
      "463 번째 loss, accuracy:  0.940970532893784 0.625\n",
      "464 번째 loss, accuracy:  0.9407152971778038 0.625\n",
      "465 번째 loss, accuracy:  0.9404604601270404 0.625\n",
      "466 번째 loss, accuracy:  0.9402060202476186 0.625\n",
      "467 번째 loss, accuracy:  0.9399519760556682 0.625\n",
      "468 번째 loss, accuracy:  0.9396983260772174 0.625\n",
      "469 번째 loss, accuracy:  0.9394450688480888 0.625\n",
      "470 번째 loss, accuracy:  0.9391922029138009 0.625\n",
      "471 번째 loss, accuracy:  0.9389397268294704 0.625\n",
      "472 번째 loss, accuracy:  0.9386876391597059 0.625\n",
      "473 번째 loss, accuracy:  0.9384359384785206 0.625\n",
      "474 번째 loss, accuracy:  0.9381846233692294 0.625\n",
      "475 번째 loss, accuracy:  0.9379336924243605 0.625\n",
      "476 번째 loss, accuracy:  0.937683144245558 0.625\n",
      "477 번째 loss, accuracy:  0.9374329774434911 0.6333333333333333\n",
      "478 번째 loss, accuracy:  0.9371831906377636 0.6333333333333333\n",
      "479 번째 loss, accuracy:  0.9369337824568233 0.6333333333333333\n",
      "480 번째 loss, accuracy:  0.9366847515378782 0.6333333333333333\n",
      "481 번째 loss, accuracy:  0.9364360965268015 0.6333333333333333\n",
      "482 번째 loss, accuracy:  0.9361878160780527 0.6333333333333333\n",
      "483 번째 loss, accuracy:  0.9359399088545886 0.6333333333333333\n",
      "484 번째 loss, accuracy:  0.9356923735277817 0.6333333333333333\n",
      "485 번째 loss, accuracy:  0.9354452087773331 0.6333333333333333\n",
      "486 번째 loss, accuracy:  0.9351984132911989 0.6333333333333333\n",
      "487 번째 loss, accuracy:  0.9349519857655025 0.6333333333333333\n",
      "488 번째 loss, accuracy:  0.9347059249044555 0.6333333333333333\n",
      "489 번째 loss, accuracy:  0.9344602294202856 0.6333333333333333\n",
      "490 번째 loss, accuracy:  0.93421489803315 0.6333333333333333\n",
      "491 번째 loss, accuracy:  0.9339699294710646 0.6333333333333333\n",
      "492 번째 loss, accuracy:  0.9337253224698282 0.6333333333333333\n",
      "493 번째 loss, accuracy:  0.9334810757729471 0.6333333333333333\n",
      "494 번째 loss, accuracy:  0.9332371881315559 0.6333333333333333\n",
      "495 번째 loss, accuracy:  0.9329936583043561 0.6333333333333333\n",
      "496 번째 loss, accuracy:  0.9327504850575363 0.6333333333333333\n",
      "497 번째 loss, accuracy:  0.932507667164701 0.6333333333333333\n",
      "498 번째 loss, accuracy:  0.9322652034068031 0.65\n",
      "499 번째 loss, accuracy:  0.932023092572074 0.65\n",
      "500 번째 loss, accuracy:  0.9317813334559574 0.65\n",
      "501 번째 loss, accuracy:  0.9315399248610385 0.65\n",
      "502 번째 loss, accuracy:  0.9312988655969787 0.65\n",
      "503 번째 loss, accuracy:  0.9310581544804489 0.65\n",
      "504 번째 loss, accuracy:  0.9308177903350678 0.65\n",
      "505 번째 loss, accuracy:  0.9305777719913327 0.65\n",
      "506 번째 loss, accuracy:  0.9303380982865608 0.65\n",
      "507 번째 loss, accuracy:  0.9300987680648255 0.65\n",
      "508 번째 loss, accuracy:  0.9298597801768914 0.65\n",
      "509 번째 loss, accuracy:  0.9296211334801566 0.65\n",
      "510 번째 loss, accuracy:  0.9293828268385912 0.65\n",
      "511 번째 loss, accuracy:  0.9291448591226789 0.65\n",
      "512 번째 loss, accuracy:  0.9289072292093574 0.65\n",
      "513 번째 loss, accuracy:  0.9286699359819578 0.65\n",
      "514 번째 loss, accuracy:  0.9284329783301514 0.65\n",
      "515 번째 loss, accuracy:  0.9281963551498901 0.65\n",
      "516 번째 loss, accuracy:  0.9279600653433495 0.6583333333333333\n",
      "517 번째 loss, accuracy:  0.9277241078188787 0.6583333333333333\n",
      "518 번째 loss, accuracy:  0.9274884814909392 0.6583333333333333\n",
      "519 번째 loss, accuracy:  0.9272531852800568 0.6583333333333333\n",
      "520 번째 loss, accuracy:  0.9270182181127606 0.6583333333333333\n",
      "521 번째 loss, accuracy:  0.9267835789215388 0.6583333333333333\n",
      "522 번째 loss, accuracy:  0.9265492666447818 0.6583333333333333\n",
      "523 번째 loss, accuracy:  0.9263152802267282 0.6583333333333333\n",
      "524 번째 loss, accuracy:  0.9260816186174201 0.6583333333333333\n",
      "525 번째 loss, accuracy:  0.9258482807726477 0.6583333333333333\n",
      "526 번째 loss, accuracy:  0.9256152656539044 0.6583333333333333\n",
      "527 번째 loss, accuracy:  0.9253825722283325 0.6583333333333333\n",
      "528 번째 loss, accuracy:  0.9251501994686763 0.6583333333333333\n",
      "529 번째 loss, accuracy:  0.9249181463532342 0.6583333333333333\n",
      "530 번째 loss, accuracy:  0.9246864118658176 0.6583333333333333\n",
      "531 번째 loss, accuracy:  0.9244549949956904 0.6583333333333333\n",
      "532 번째 loss, accuracy:  0.924223894737537 0.6583333333333333\n",
      "533 번째 loss, accuracy:  0.9239931100914054 0.6583333333333333\n",
      "534 번째 loss, accuracy:  0.9237626400626683 0.6583333333333333\n",
      "535 번째 loss, accuracy:  0.9235324836619765 0.6583333333333333\n",
      "536 번째 loss, accuracy:  0.9233026399052129 0.6583333333333333\n",
      "537 번째 loss, accuracy:  0.9230731078134546 0.6583333333333333\n",
      "538 번째 loss, accuracy:  0.9228438864129224 0.6583333333333333\n",
      "539 번째 loss, accuracy:  0.9226149747349406 0.6583333333333333\n",
      "540 번째 loss, accuracy:  0.922386371815899 0.6583333333333333\n",
      "541 번째 loss, accuracy:  0.9221580766972041 0.6583333333333333\n",
      "542 번째 loss, accuracy:  0.9219300884252426 0.6583333333333333\n",
      "543 번째 loss, accuracy:  0.9217024060513389 0.6666666666666666\n",
      "544 번째 loss, accuracy:  0.9214750286317167 0.6666666666666666\n",
      "545 번째 loss, accuracy:  0.9212479552274554 0.6666666666666666\n",
      "546 번째 loss, accuracy:  0.9210211849044545 0.6666666666666666\n",
      "547 번째 loss, accuracy:  0.9207947167333929 0.6666666666666666\n",
      "548 번째 loss, accuracy:  0.9205685497896866 0.675\n",
      "549 번째 loss, accuracy:  0.9203426831534581 0.6833333333333333\n",
      "550 번째 loss, accuracy:  0.9201171159094937 0.6833333333333333\n",
      "551 번째 loss, accuracy:  0.9198918471472073 0.6833333333333333\n",
      "552 번째 loss, accuracy:  0.9196668759606031 0.6833333333333333\n",
      "553 번째 loss, accuracy:  0.9194422014482395 0.6833333333333333\n",
      "554 번째 loss, accuracy:  0.9192178227131934 0.6833333333333333\n",
      "555 번째 loss, accuracy:  0.9189937388630242 0.6833333333333333\n",
      "556 번째 loss, accuracy:  0.9187699490097384 0.6833333333333333\n",
      "557 번째 loss, accuracy:  0.918546452269753 0.6833333333333333\n",
      "558 번째 loss, accuracy:  0.9183232477638652 0.6833333333333333\n",
      "559 번째 loss, accuracy:  0.9181003346172151 0.6833333333333333\n",
      "560 번째 loss, accuracy:  0.9178777119592524 0.6833333333333333\n",
      "561 번째 loss, accuracy:  0.9176553789237016 0.6833333333333333\n",
      "562 번째 loss, accuracy:  0.9174333346485327 0.6833333333333333\n",
      "563 번째 loss, accuracy:  0.9172115782759261 0.6833333333333333\n",
      "564 번째 loss, accuracy:  0.9169901089522402 0.6833333333333333\n",
      "565 번째 loss, accuracy:  0.9167689258279795 0.6833333333333333\n",
      "566 번째 loss, accuracy:  0.9165480280577624 0.6833333333333333\n",
      "567 번째 loss, accuracy:  0.9163274148002923 0.6833333333333333\n",
      "568 번째 loss, accuracy:  0.9161070852183227 0.6833333333333333\n",
      "569 번째 loss, accuracy:  0.9158870384786322 0.6833333333333333\n",
      "570 번째 loss, accuracy:  0.9156672737519889 0.6833333333333333\n",
      "571 번째 loss, accuracy:  0.9154477902131205 0.6833333333333333\n",
      "572 번째 loss, accuracy:  0.9152285870406908 0.6833333333333333\n",
      "573 번째 loss, accuracy:  0.9150096634172624 0.6833333333333333\n",
      "574 번째 loss, accuracy:  0.9147910185292732 0.6833333333333333\n",
      "575 번째 loss, accuracy:  0.9145726515670047 0.6833333333333333\n",
      "576 번째 loss, accuracy:  0.9143545617245548 0.6833333333333333\n",
      "577 번째 loss, accuracy:  0.9141367481998108 0.6833333333333333\n",
      "578 번째 loss, accuracy:  0.9139192101944162 0.6833333333333333\n",
      "579 번째 loss, accuracy:  0.9137019469137521 0.6833333333333333\n",
      "580 번째 loss, accuracy:  0.9134849575669006 0.6833333333333333\n",
      "581 번째 loss, accuracy:  0.9132682413666265 0.6833333333333333\n",
      "582 번째 loss, accuracy:  0.9130517975293431 0.6833333333333333\n",
      "583 번째 loss, accuracy:  0.9128356252750875 0.6833333333333333\n",
      "584 번째 loss, accuracy:  0.9126197238275 0.6833333333333333\n",
      "585 번째 loss, accuracy:  0.9124040924137908 0.6833333333333333\n",
      "586 번째 loss, accuracy:  0.9121887302647184 0.6833333333333333\n",
      "587 번째 loss, accuracy:  0.9119736366145633 0.6833333333333333\n",
      "588 번째 loss, accuracy:  0.911758810701104 0.6833333333333333\n",
      "589 번째 loss, accuracy:  0.9115442517655896 0.6833333333333333\n",
      "590 번째 loss, accuracy:  0.9113299590527167 0.6833333333333333\n",
      "591 번째 loss, accuracy:  0.9111159318106061 0.6833333333333333\n",
      "592 번째 loss, accuracy:  0.9109021692907765 0.6833333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593 번째 loss, accuracy:  0.9106886707481211 0.6833333333333333\n",
      "594 번째 loss, accuracy:  0.9104754354408875 0.6833333333333333\n",
      "595 번째 loss, accuracy:  0.910262462630649 0.6833333333333333\n",
      "596 번째 loss, accuracy:  0.9100497515822826 0.6833333333333333\n",
      "597 번째 loss, accuracy:  0.9098373015639503 0.6833333333333333\n",
      "598 번째 loss, accuracy:  0.9096251118470724 0.6833333333333333\n",
      "599 번째 loss, accuracy:  0.9094131817063044 0.6833333333333333\n",
      "600 번째 loss, accuracy:  0.9092015104195189 0.6833333333333333\n",
      "601 번째 loss, accuracy:  0.9089900972677782 0.6833333333333333\n",
      "602 번째 loss, accuracy:  0.9087789415353174 0.6833333333333333\n",
      "603 번째 loss, accuracy:  0.908568042509521 0.6833333333333333\n",
      "604 번째 loss, accuracy:  0.9083573994808998 0.6833333333333333\n",
      "605 번째 loss, accuracy:  0.9081470117430718 0.6833333333333333\n",
      "606 번째 loss, accuracy:  0.9079368785927385 0.6833333333333333\n",
      "607 번째 loss, accuracy:  0.9077269993296697 0.6833333333333333\n",
      "608 번째 loss, accuracy:  0.907517373256676 0.6833333333333333\n",
      "609 번째 loss, accuracy:  0.9073079996795954 0.6833333333333333\n",
      "610 번째 loss, accuracy:  0.9070988779072657 0.6833333333333333\n",
      "611 번째 loss, accuracy:  0.9068900072515113 0.6833333333333333\n",
      "612 번째 loss, accuracy:  0.9066813870271184 0.6833333333333333\n",
      "613 번째 loss, accuracy:  0.906473016551819 0.6833333333333333\n",
      "614 번째 loss, accuracy:  0.9062648951462698 0.6833333333333333\n",
      "615 번째 loss, accuracy:  0.9060570221340315 0.6833333333333333\n",
      "616 번째 loss, accuracy:  0.905849396841555 0.6833333333333333\n",
      "617 번째 loss, accuracy:  0.9056420185981539 0.6833333333333333\n",
      "618 번째 loss, accuracy:  0.9054348867359945 0.6916666666666667\n",
      "619 번째 loss, accuracy:  0.9052280005900715 0.6916666666666667\n",
      "620 번째 loss, accuracy:  0.9050213594981938 0.6916666666666667\n",
      "621 번째 loss, accuracy:  0.9048149628009637 0.6916666666666667\n",
      "622 번째 loss, accuracy:  0.904608809841759 0.6916666666666667\n",
      "623 번째 loss, accuracy:  0.9044028999667156 0.6916666666666667\n",
      "624 번째 loss, accuracy:  0.9041972325247115 0.6916666666666667\n",
      "625 번째 loss, accuracy:  0.9039918068673465 0.6916666666666667\n",
      "626 번째 loss, accuracy:  0.9037866223489258 0.6916666666666667\n",
      "627 번째 loss, accuracy:  0.9035816783264449 0.6916666666666667\n",
      "628 번째 loss, accuracy:  0.9033769741595692 0.6916666666666667\n",
      "629 번째 loss, accuracy:  0.9031725092106199 0.6916666666666667\n",
      "630 번째 loss, accuracy:  0.9029682828445554 0.6916666666666667\n",
      "631 번째 loss, accuracy:  0.902764294428956 0.6916666666666667\n",
      "632 번째 loss, accuracy:  0.9025605433340078 0.6916666666666667\n",
      "633 번째 loss, accuracy:  0.9023570289324847 0.6916666666666667\n",
      "634 번째 loss, accuracy:  0.9021537505997348 0.6916666666666667\n",
      "635 번째 loss, accuracy:  0.9019507077136613 0.6916666666666667\n",
      "636 번째 loss, accuracy:  0.9017478996547107 0.6916666666666667\n",
      "637 번째 loss, accuracy:  0.9015453258058521 0.6916666666666667\n",
      "638 번째 loss, accuracy:  0.9013429855525671 0.6916666666666667\n",
      "639 번째 loss, accuracy:  0.9011408782828314 0.6916666666666667\n",
      "640 번째 loss, accuracy:  0.9009390033870981 0.6916666666666667\n",
      "641 번째 loss, accuracy:  0.9007373602582873 0.6916666666666667\n",
      "642 번째 loss, accuracy:  0.9005359482917654 0.6916666666666667\n",
      "643 번째 loss, accuracy:  0.9003347668853344 0.6916666666666667\n",
      "644 번째 loss, accuracy:  0.9001338154392177 0.6916666666666667\n",
      "645 번째 loss, accuracy:  0.8999330933560424 0.6916666666666667\n",
      "646 번째 loss, accuracy:  0.8997326000408251 0.6916666666666667\n",
      "647 번째 loss, accuracy:  0.8995323349009605 0.6916666666666667\n",
      "648 번째 loss, accuracy:  0.8993322973462043 0.6916666666666667\n",
      "649 번째 loss, accuracy:  0.8991324867886628 0.6916666666666667\n",
      "650 번째 loss, accuracy:  0.898932902642772 0.6916666666666667\n",
      "651 번째 loss, accuracy:  0.8987335443252937 0.6916666666666667\n",
      "652 번째 loss, accuracy:  0.898534411255292 0.6916666666666667\n",
      "653 번째 loss, accuracy:  0.8983355028541279 0.6916666666666667\n",
      "654 번째 loss, accuracy:  0.8981368185454379 0.6916666666666667\n",
      "655 번째 loss, accuracy:  0.8979383577551282 0.6916666666666667\n",
      "656 번째 loss, accuracy:  0.897740119911356 0.6916666666666667\n",
      "657 번째 loss, accuracy:  0.8975421044445192 0.6916666666666667\n",
      "658 번째 loss, accuracy:  0.8973443107872431 0.6916666666666667\n",
      "659 번째 loss, accuracy:  0.8971467383743686 0.6916666666666667\n",
      "660 번째 loss, accuracy:  0.8969493866429351 0.6916666666666667\n",
      "661 번째 loss, accuracy:  0.896752255032172 0.6916666666666667\n",
      "662 번째 loss, accuracy:  0.8965553429834839 0.6916666666666667\n",
      "663 번째 loss, accuracy:  0.89635864994044 0.6916666666666667\n",
      "664 번째 loss, accuracy:  0.8961621753487619 0.6916666666666667\n",
      "665 번째 loss, accuracy:  0.8959659186563075 0.6916666666666667\n",
      "666 번째 loss, accuracy:  0.8957698793130632 0.6916666666666667\n",
      "667 번째 loss, accuracy:  0.8955740567711314 0.6916666666666667\n",
      "668 번째 loss, accuracy:  0.8953784504847153 0.6916666666666667\n",
      "669 번째 loss, accuracy:  0.8951830599101105 0.6916666666666667\n",
      "670 번째 loss, accuracy:  0.8949878845056934 0.6916666666666667\n",
      "671 번째 loss, accuracy:  0.8947929237319059 0.6916666666666667\n",
      "672 번째 loss, accuracy:  0.8945981770512456 0.6916666666666667\n",
      "673 번째 loss, accuracy:  0.8944036439282572 0.6916666666666667\n",
      "674 번째 loss, accuracy:  0.8942093238295181 0.6916666666666667\n",
      "675 번째 loss, accuracy:  0.8940152162236269 0.6916666666666667\n",
      "676 번째 loss, accuracy:  0.8938213205811938 0.6916666666666667\n",
      "677 번째 loss, accuracy:  0.8936276363748299 0.6916666666666667\n",
      "678 번째 loss, accuracy:  0.8934341630791347 0.6916666666666667\n",
      "679 번째 loss, accuracy:  0.893240900170685 0.6916666666666667\n",
      "680 번째 loss, accuracy:  0.893047847128024 0.6916666666666667\n",
      "681 번째 loss, accuracy:  0.8928550034316554 0.6916666666666667\n",
      "682 번째 loss, accuracy:  0.8926623685640249 0.6916666666666667\n",
      "683 번째 loss, accuracy:  0.8924699420095152 0.6916666666666667\n",
      "684 번째 loss, accuracy:  0.8922777232544336 0.6916666666666667\n",
      "685 번째 loss, accuracy:  0.8920857117870027 0.6916666666666667\n",
      "686 번째 loss, accuracy:  0.8918939070973496 0.6916666666666667\n",
      "687 번째 loss, accuracy:  0.8917023086774929 0.6916666666666667\n",
      "688 번째 loss, accuracy:  0.8915109160213384 0.6916666666666667\n",
      "689 번째 loss, accuracy:  0.8913197286246628 0.6916666666666667\n",
      "690 번째 loss, accuracy:  0.8911287459851104 0.6916666666666667\n",
      "691 번째 loss, accuracy:  0.8909379676021759 0.6916666666666667\n",
      "692 번째 loss, accuracy:  0.8907473929772011 0.6916666666666667\n",
      "693 번째 loss, accuracy:  0.8905570216133606 0.6916666666666667\n",
      "694 번째 loss, accuracy:  0.8903668530156538 0.6916666666666667\n",
      "695 번째 loss, accuracy:  0.8901768866908976 0.6916666666666667\n",
      "696 번째 loss, accuracy:  0.8899871221477116 0.6916666666666667\n",
      "697 번째 loss, accuracy:  0.8897975588965142 0.6916666666666667\n",
      "698 번째 loss, accuracy:  0.8896081964495096 0.6916666666666667\n",
      "699 번째 loss, accuracy:  0.8894190343206804 0.6916666666666667\n",
      "700 번째 loss, accuracy:  0.8892300720257776 0.6916666666666667\n",
      "701 번째 loss, accuracy:  0.8890413090823117 0.6916666666666667\n",
      "702 번째 loss, accuracy:  0.8888527450095434 0.6916666666666667\n",
      "703 번째 loss, accuracy:  0.8886643793284729 0.6916666666666667\n",
      "704 번째 loss, accuracy:  0.8884762115618358 0.6916666666666667\n",
      "705 번째 loss, accuracy:  0.8882882412340911 0.6916666666666667\n",
      "706 번째 loss, accuracy:  0.88810046787141 0.6916666666666667\n",
      "707 번째 loss, accuracy:  0.8879128910016721 0.6916666666666667\n",
      "708 번째 loss, accuracy:  0.8877255101544534 0.6916666666666667\n",
      "709 번째 loss, accuracy:  0.8875383248610191 0.6916666666666667\n",
      "710 번째 loss, accuracy:  0.887351334654314 0.6916666666666667\n",
      "711 번째 loss, accuracy:  0.8871645390689551 0.6916666666666667\n",
      "712 번째 loss, accuracy:  0.886977937641223 0.6916666666666667\n",
      "713 번째 loss, accuracy:  0.8867915299090531 0.6916666666666667\n",
      "714 번째 loss, accuracy:  0.8866053154120287 0.6916666666666667\n",
      "715 번째 loss, accuracy:  0.8864192936913696 0.6916666666666667\n",
      "716 번째 loss, accuracy:  0.886233464289928 0.6916666666666667\n",
      "717 번째 loss, accuracy:  0.8860478267521772 0.6916666666666667\n",
      "718 번째 loss, accuracy:  0.8858623806242053 0.6916666666666667\n",
      "719 번째 loss, accuracy:  0.8856771254537074 0.6916666666666667\n",
      "720 번째 loss, accuracy:  0.8854920607899764 0.6916666666666667\n",
      "721 번째 loss, accuracy:  0.8853071861838948 0.6916666666666667\n",
      "722 번째 loss, accuracy:  0.88512250118793 0.6916666666666667\n",
      "723 번째 loss, accuracy:  0.8849380053561232 0.6916666666666667\n",
      "724 번째 loss, accuracy:  0.884753698244083 0.6916666666666667\n",
      "725 번째 loss, accuracy:  0.8845695794089777 0.6916666666666667\n",
      "726 번째 loss, accuracy:  0.884385648409528 0.6916666666666667\n",
      "727 번째 loss, accuracy:  0.8842019048059998 0.6916666666666667\n",
      "728 번째 loss, accuracy:  0.8840183481601954 0.6916666666666667\n",
      "729 번째 loss, accuracy:  0.8838349780354483 0.6916666666666667\n",
      "730 번째 loss, accuracy:  0.8836517939966128 0.6916666666666667\n",
      "731 번째 loss, accuracy:  0.8834687956100583 0.6916666666666667\n",
      "732 번째 loss, accuracy:  0.8832859824436649 0.6916666666666667\n",
      "733 번째 loss, accuracy:  0.88310335406681 0.6916666666666667\n",
      "734 번째 loss, accuracy:  0.8829209100503647 0.6916666666666667\n",
      "735 번째 loss, accuracy:  0.8827386499666896 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736 번째 loss, accuracy:  0.882556573389622 0.6916666666666667\n",
      "737 번째 loss, accuracy:  0.8823746798944726 0.6916666666666667\n",
      "738 번째 loss, accuracy:  0.8821929690580189 0.6916666666666667\n",
      "739 번째 loss, accuracy:  0.8820114404584934 0.6916666666666667\n",
      "740 번째 loss, accuracy:  0.8818300936755847 0.6916666666666667\n",
      "741 번째 loss, accuracy:  0.8816489282904239 0.6916666666666667\n",
      "742 번째 loss, accuracy:  0.8814679438855803 0.6916666666666667\n",
      "743 번째 loss, accuracy:  0.8812871400450552 0.6916666666666667\n",
      "744 번째 loss, accuracy:  0.881106516354275 0.6916666666666667\n",
      "745 번째 loss, accuracy:  0.8809260724000839 0.6916666666666667\n",
      "746 번째 loss, accuracy:  0.8807458077707381 0.6916666666666667\n",
      "747 번째 loss, accuracy:  0.880565722055898 0.6916666666666667\n",
      "748 번째 loss, accuracy:  0.8803858148466244 0.6916666666666667\n",
      "749 번째 loss, accuracy:  0.880206085735369 0.6916666666666667\n",
      "750 번째 loss, accuracy:  0.8800265343159699 0.6916666666666667\n",
      "751 번째 loss, accuracy:  0.8798471601836437 0.6916666666666667\n",
      "752 번째 loss, accuracy:  0.8796679629349813 0.6916666666666667\n",
      "753 번째 loss, accuracy:  0.8794889421679412 0.6916666666666667\n",
      "754 번째 loss, accuracy:  0.8793100974818416 0.6916666666666667\n",
      "755 번째 loss, accuracy:  0.8791314284773546 0.6916666666666667\n",
      "756 번째 loss, accuracy:  0.8789529347565017 0.6916666666666667\n",
      "757 번째 loss, accuracy:  0.8787746159226476 0.6916666666666667\n",
      "758 번째 loss, accuracy:  0.8785964715804911 0.6916666666666667\n",
      "759 번째 loss, accuracy:  0.8784185013360627 0.6916666666666667\n",
      "760 번째 loss, accuracy:  0.8782407047967167 0.6916666666666667\n",
      "761 번째 loss, accuracy:  0.8780630815711272 0.6916666666666667\n",
      "762 번째 loss, accuracy:  0.8778856312692784 0.6916666666666667\n",
      "763 번째 loss, accuracy:  0.877708353502463 0.6916666666666667\n",
      "764 번째 loss, accuracy:  0.8775312478832745 0.6916666666666667\n",
      "765 번째 loss, accuracy:  0.8773543140256003 0.6916666666666667\n",
      "766 번째 loss, accuracy:  0.8771775515446177 0.6916666666666667\n",
      "767 번째 loss, accuracy:  0.8770009600567874 0.6916666666666667\n",
      "768 번째 loss, accuracy:  0.8768245391798489 0.6916666666666667\n",
      "769 번째 loss, accuracy:  0.8766482885328135 0.6916666666666667\n",
      "770 번째 loss, accuracy:  0.8764722077359581 0.6916666666666667\n",
      "771 번째 loss, accuracy:  0.8762962964108227 0.6916666666666667\n",
      "772 번째 loss, accuracy:  0.8761205541802022 0.6916666666666667\n",
      "773 번째 loss, accuracy:  0.8759449806681412 0.6916666666666667\n",
      "774 번째 loss, accuracy:  0.8757695754999297 0.6916666666666667\n",
      "775 번째 loss, accuracy:  0.8755943383020967 0.6916666666666667\n",
      "776 번째 loss, accuracy:  0.8754192687024054 0.6916666666666667\n",
      "777 번째 loss, accuracy:  0.8752443663298496 0.6916666666666667\n",
      "778 번째 loss, accuracy:  0.875069630814642 0.6916666666666667\n",
      "779 번째 loss, accuracy:  0.8748950617882159 0.6916666666666667\n",
      "780 번째 loss, accuracy:  0.8747206588832176 0.6916666666666667\n",
      "781 번째 loss, accuracy:  0.8745464217335006 0.6916666666666667\n",
      "782 번째 loss, accuracy:  0.874372349974121 0.6916666666666667\n",
      "783 번째 loss, accuracy:  0.8741984432413319 0.6916666666666667\n",
      "784 번째 loss, accuracy:  0.8740247011725787 0.6916666666666667\n",
      "785 번째 loss, accuracy:  0.8738511234064941 0.6916666666666667\n",
      "786 번째 loss, accuracy:  0.8736777095828916 0.6916666666666667\n",
      "787 번째 loss, accuracy:  0.8735044593427648 0.6916666666666667\n",
      "788 번째 loss, accuracy:  0.8733313723282764 0.6916666666666667\n",
      "789 번째 loss, accuracy:  0.8731584481827556 0.6916666666666667\n",
      "790 번째 loss, accuracy:  0.8729856865506981 0.6916666666666667\n",
      "791 번째 loss, accuracy:  0.8728130870777517 0.6916666666666667\n",
      "792 번째 loss, accuracy:  0.872640649410721 0.6916666666666667\n",
      "793 번째 loss, accuracy:  0.8724683731975557 0.6916666666666667\n",
      "794 번째 loss, accuracy:  0.872296258087349 0.6916666666666667\n",
      "795 번째 loss, accuracy:  0.8721243037303315 0.6916666666666667\n",
      "796 번째 loss, accuracy:  0.8719525097778679 0.6916666666666667\n",
      "797 번째 loss, accuracy:  0.8717808758824507 0.6916666666666667\n",
      "798 번째 loss, accuracy:  0.871609401697697 0.6916666666666667\n",
      "799 번째 loss, accuracy:  0.8714380868783422 0.6916666666666667\n",
      "800 번째 loss, accuracy:  0.8712669310802373 0.6916666666666667\n",
      "801 번째 loss, accuracy:  0.8710959339603431 0.6916666666666667\n",
      "802 번째 loss, accuracy:  0.870925095176726 0.6916666666666667\n",
      "803 번째 loss, accuracy:  0.8707544143885523 0.6916666666666667\n",
      "804 번째 loss, accuracy:  0.8705838912560855 0.6916666666666667\n",
      "805 번째 loss, accuracy:  0.8704135254406793 0.6916666666666667\n",
      "806 번째 loss, accuracy:  0.8702433166047786 0.6916666666666667\n",
      "807 번째 loss, accuracy:  0.8700732644119071 0.6916666666666667\n",
      "808 번째 loss, accuracy:  0.8699033685266694 0.6916666666666667\n",
      "809 번째 loss, accuracy:  0.8697336286147436 0.6916666666666667\n",
      "810 번째 loss, accuracy:  0.8695640443428763 0.6916666666666667\n",
      "811 번째 loss, accuracy:  0.8693946153788812 0.6916666666666667\n",
      "812 번째 loss, accuracy:  0.8692253413916333 0.6916666666666667\n",
      "813 번째 loss, accuracy:  0.8690562220510629 0.6916666666666667\n",
      "814 번째 loss, accuracy:  0.8688872570281558 0.6916666666666667\n",
      "815 번째 loss, accuracy:  0.8687184459949421 0.6916666666666667\n",
      "816 번째 loss, accuracy:  0.8685497886244989 0.6916666666666667\n",
      "817 번째 loss, accuracy:  0.868381284590942 0.6916666666666667\n",
      "818 번째 loss, accuracy:  0.8682129335694242 0.6916666666666667\n",
      "819 번째 loss, accuracy:  0.8680447352361287 0.6916666666666667\n",
      "820 번째 loss, accuracy:  0.8678766892682662 0.6916666666666667\n",
      "821 번째 loss, accuracy:  0.8677087953440714 0.6916666666666667\n",
      "822 번째 loss, accuracy:  0.8675410531427988 0.6916666666666667\n",
      "823 번째 loss, accuracy:  0.867373462344717 0.6916666666666667\n",
      "824 번째 loss, accuracy:  0.8672060226311061 0.6916666666666667\n",
      "825 번째 loss, accuracy:  0.8670387336842548 0.6916666666666667\n",
      "826 번째 loss, accuracy:  0.8668715951874543 0.6916666666666667\n",
      "827 번째 loss, accuracy:  0.8667046068249944 0.6916666666666667\n",
      "828 번째 loss, accuracy:  0.8665377682821607 0.6916666666666667\n",
      "829 번째 loss, accuracy:  0.8663710792452313 0.6916666666666667\n",
      "830 번째 loss, accuracy:  0.8662045394014698 0.6916666666666667\n",
      "831 번째 loss, accuracy:  0.8660381484391259 0.6916666666666667\n",
      "832 번째 loss, accuracy:  0.865871906047429 0.6916666666666667\n",
      "833 번째 loss, accuracy:  0.8657058119165827 0.6916666666666667\n",
      "834 번째 loss, accuracy:  0.8655398657377636 0.6916666666666667\n",
      "835 번째 loss, accuracy:  0.8653740672031172 0.6916666666666667\n",
      "836 번째 loss, accuracy:  0.8652084160057532 0.6916666666666667\n",
      "837 번째 loss, accuracy:  0.8650429118397421 0.6916666666666667\n",
      "838 번째 loss, accuracy:  0.864877554400115 0.6916666666666667\n",
      "839 번째 loss, accuracy:  0.8647123433828509 0.6916666666666667\n",
      "840 번째 loss, accuracy:  0.8645472784848828 0.6916666666666667\n",
      "841 번째 loss, accuracy:  0.8643823594040875 0.6916666666666667\n",
      "842 번째 loss, accuracy:  0.8642175858392872 0.6916666666666667\n",
      "843 번째 loss, accuracy:  0.8640529574902405 0.6916666666666667\n",
      "844 번째 loss, accuracy:  0.863888474057642 0.6916666666666667\n",
      "845 번째 loss, accuracy:  0.8637241352431176 0.6916666666666667\n",
      "846 번째 loss, accuracy:  0.863559940749223 0.6916666666666667\n",
      "847 번째 loss, accuracy:  0.8633958902794362 0.6916666666666667\n",
      "848 번째 loss, accuracy:  0.8632319835381582 0.6916666666666667\n",
      "849 번째 loss, accuracy:  0.8630682202307064 0.6916666666666667\n",
      "850 번째 loss, accuracy:  0.8629046000633134 0.6916666666666667\n",
      "851 번째 loss, accuracy:  0.8627411227431218 0.6916666666666667\n",
      "852 번째 loss, accuracy:  0.8625777879781812 0.6916666666666667\n",
      "853 번째 loss, accuracy:  0.8624145954774461 0.6916666666666667\n",
      "854 번째 loss, accuracy:  0.8622515449507707 0.6916666666666667\n",
      "855 번째 loss, accuracy:  0.8620886361089037 0.6916666666666667\n",
      "856 번째 loss, accuracy:  0.8619258686634914 0.6916666666666667\n",
      "857 번째 loss, accuracy:  0.8617632423270689 0.6916666666666667\n",
      "858 번째 loss, accuracy:  0.8616007568130565 0.6916666666666667\n",
      "859 번째 loss, accuracy:  0.8614384118357599 0.6916666666666667\n",
      "860 번째 loss, accuracy:  0.8612762071103649 0.6916666666666667\n",
      "861 번째 loss, accuracy:  0.8611141423529323 0.6916666666666667\n",
      "862 번째 loss, accuracy:  0.8609522172803984 0.6916666666666667\n",
      "863 번째 loss, accuracy:  0.8607904316105689 0.6916666666666667\n",
      "864 번째 loss, accuracy:  0.8606287850621176 0.6916666666666667\n",
      "865 번째 loss, accuracy:  0.8604672773545822 0.6916666666666667\n",
      "866 번째 loss, accuracy:  0.8603059082083603 0.6916666666666667\n",
      "867 번째 loss, accuracy:  0.8601446773447066 0.6916666666666667\n",
      "868 번째 loss, accuracy:  0.8599835844857314 0.6916666666666667\n",
      "869 번째 loss, accuracy:  0.8598226293543952 0.6916666666666667\n",
      "870 번째 loss, accuracy:  0.8596618116745068 0.6916666666666667\n",
      "871 번째 loss, accuracy:  0.8595011311707207 0.6916666666666667\n",
      "872 번째 loss, accuracy:  0.8593405875685322 0.6916666666666667\n",
      "873 번째 loss, accuracy:  0.8591801805942746 0.6916666666666667\n",
      "874 번째 loss, accuracy:  0.8590199099751181 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875 번째 loss, accuracy:  0.8588597754390654 0.6916666666666667\n",
      "876 번째 loss, accuracy:  0.8586997767149491 0.6916666666666667\n",
      "877 번째 loss, accuracy:  0.8585399135324253 0.6916666666666667\n",
      "878 번째 loss, accuracy:  0.8583801856219767 0.6916666666666667\n",
      "879 번째 loss, accuracy:  0.8582205927149045 0.6916666666666667\n",
      "880 번째 loss, accuracy:  0.8580611345433286 0.6916666666666667\n",
      "881 번째 loss, accuracy:  0.857901810840183 0.6916666666666667\n",
      "882 번째 loss, accuracy:  0.8577426213392125 0.6916666666666667\n",
      "883 번째 loss, accuracy:  0.8575835657749712 0.6916666666666667\n",
      "884 번째 loss, accuracy:  0.8574246438828176 0.6916666666666667\n",
      "885 번째 loss, accuracy:  0.8572658553989149 0.6916666666666667\n",
      "886 번째 loss, accuracy:  0.8571072000602239 0.6916666666666667\n",
      "887 번째 loss, accuracy:  0.856948677604503 0.6916666666666667\n",
      "888 번째 loss, accuracy:  0.8567902877703053 0.6916666666666667\n",
      "889 번째 loss, accuracy:  0.8566320302969735 0.6916666666666667\n",
      "890 번째 loss, accuracy:  0.8564739049246407 0.6916666666666667\n",
      "891 번째 loss, accuracy:  0.8563159113942223 0.6916666666666667\n",
      "892 번째 loss, accuracy:  0.856158049447419 0.6916666666666667\n",
      "893 번째 loss, accuracy:  0.8560003188267115 0.6916666666666667\n",
      "894 번째 loss, accuracy:  0.8558427192753556 0.6916666666666667\n",
      "895 번째 loss, accuracy:  0.8556852505373833 0.6916666666666667\n",
      "896 번째 loss, accuracy:  0.8555279123575945 0.6916666666666667\n",
      "897 번째 loss, accuracy:  0.8553707044815616 0.6916666666666667\n",
      "898 번째 loss, accuracy:  0.8552136266556226 0.6916666666666667\n",
      "899 번째 loss, accuracy:  0.8550566786268755 0.6916666666666667\n",
      "900 번째 loss, accuracy:  0.8548998601431812 0.6916666666666667\n",
      "901 번째 loss, accuracy:  0.8547431709531587 0.6916666666666667\n",
      "902 번째 loss, accuracy:  0.8545866108061806 0.6916666666666667\n",
      "903 번째 loss, accuracy:  0.8544301794523735 0.6916666666666667\n",
      "904 번째 loss, accuracy:  0.8542738766426132 0.6916666666666667\n",
      "905 번째 loss, accuracy:  0.8541177021285223 0.6916666666666667\n",
      "906 번째 loss, accuracy:  0.8539616556624665 0.6916666666666667\n",
      "907 번째 loss, accuracy:  0.8538057369975559 0.6916666666666667\n",
      "908 번째 loss, accuracy:  0.8536499458876381 0.6916666666666667\n",
      "909 번째 loss, accuracy:  0.8534942820872983 0.6916666666666667\n",
      "910 번째 loss, accuracy:  0.8533387453518541 0.6916666666666667\n",
      "911 번째 loss, accuracy:  0.8531833354373559 0.6916666666666667\n",
      "912 번째 loss, accuracy:  0.8530280521005831 0.6916666666666667\n",
      "913 번째 loss, accuracy:  0.8528728950990405 0.6916666666666667\n",
      "914 번째 loss, accuracy:  0.8527178641909573 0.6916666666666667\n",
      "915 번째 loss, accuracy:  0.8525629591352826 0.6916666666666667\n",
      "916 번째 loss, accuracy:  0.8524081796916848 0.6916666666666667\n",
      "917 번째 loss, accuracy:  0.8522535256205503 0.6916666666666667\n",
      "918 번째 loss, accuracy:  0.8520989966829756 0.6916666666666667\n",
      "919 번째 loss, accuracy:  0.8519445926407706 0.6916666666666667\n",
      "920 번째 loss, accuracy:  0.8517903132564548 0.6916666666666667\n",
      "921 번째 loss, accuracy:  0.8516361582932526 0.6916666666666667\n",
      "922 번째 loss, accuracy:  0.8514821275150914 0.6916666666666667\n",
      "923 번째 loss, accuracy:  0.8513282206866004 0.6916666666666667\n",
      "924 번째 loss, accuracy:  0.8511744375731088 0.6916666666666667\n",
      "925 번째 loss, accuracy:  0.8510207779406407 0.6916666666666667\n",
      "926 번째 loss, accuracy:  0.8508672415559153 0.6916666666666667\n",
      "927 번째 loss, accuracy:  0.8507138281863426 0.6916666666666667\n",
      "928 번째 loss, accuracy:  0.8505605376000223 0.6916666666666667\n",
      "929 번째 loss, accuracy:  0.8504073695657415 0.6916666666666667\n",
      "930 번째 loss, accuracy:  0.8502543238529706 0.6916666666666667\n",
      "931 번째 loss, accuracy:  0.8501014002318629 0.6916666666666667\n",
      "932 번째 loss, accuracy:  0.8499485984732511 0.6916666666666667\n",
      "933 번째 loss, accuracy:  0.8497959183486451 0.6916666666666667\n",
      "934 번째 loss, accuracy:  0.849643359630231 0.6916666666666667\n",
      "935 번째 loss, accuracy:  0.8494909220908657 0.6916666666666667\n",
      "936 번째 loss, accuracy:  0.8493386055040794 0.6916666666666667\n",
      "937 번째 loss, accuracy:  0.8491864096440672 0.6916666666666667\n",
      "938 번째 loss, accuracy:  0.8490343342856925 0.6916666666666667\n",
      "939 번째 loss, accuracy:  0.8488823792044807 0.6916666666666667\n",
      "940 번째 loss, accuracy:  0.8487305441766204 0.6916666666666667\n",
      "941 번째 loss, accuracy:  0.8485788289789574 0.6916666666666667\n",
      "942 번째 loss, accuracy:  0.8484272333889942 0.6916666666666667\n",
      "943 번째 loss, accuracy:  0.8482757571848898 0.6916666666666667\n",
      "944 번째 loss, accuracy:  0.8481244001454536 0.6916666666666667\n",
      "945 번째 loss, accuracy:  0.847973162050147 0.6916666666666667\n",
      "946 번째 loss, accuracy:  0.8478220426790771 0.6916666666666667\n",
      "947 번째 loss, accuracy:  0.847671041812998 0.6916666666666667\n",
      "948 번째 loss, accuracy:  0.8475201592333079 0.6916666666666667\n",
      "949 번째 loss, accuracy:  0.8473693947220442 0.6916666666666667\n",
      "950 번째 loss, accuracy:  0.847218748061884 0.6916666666666667\n",
      "951 번째 loss, accuracy:  0.8470682190361429 0.6916666666666667\n",
      "952 번째 loss, accuracy:  0.846917807428771 0.6916666666666667\n",
      "953 번째 loss, accuracy:  0.8467675130243486 0.6916666666666667\n",
      "954 번째 loss, accuracy:  0.8466173356080906 0.6916666666666667\n",
      "955 번째 loss, accuracy:  0.8464672749658366 0.6916666666666667\n",
      "956 번째 loss, accuracy:  0.8463173308840536 0.6916666666666667\n",
      "957 번째 loss, accuracy:  0.8461675031498336 0.6916666666666667\n",
      "958 번째 loss, accuracy:  0.846017791550891 0.6916666666666667\n",
      "959 번째 loss, accuracy:  0.8458681958755575 0.6916666666666667\n",
      "960 번째 loss, accuracy:  0.8457187159127854 0.6916666666666667\n",
      "961 번째 loss, accuracy:  0.8455693514521412 0.6916666666666667\n",
      "962 번째 loss, accuracy:  0.8454201022838054 0.6916666666666667\n",
      "963 번째 loss, accuracy:  0.8452709681985711 0.6916666666666667\n",
      "964 번째 loss, accuracy:  0.8451219489878404 0.6916666666666667\n",
      "965 번째 loss, accuracy:  0.844973044443622 0.6916666666666667\n",
      "966 번째 loss, accuracy:  0.8448242543585316 0.6916666666666667\n",
      "967 번째 loss, accuracy:  0.8446755785257861 0.6916666666666667\n",
      "968 번째 loss, accuracy:  0.8445270167392068 0.6916666666666667\n",
      "969 번째 loss, accuracy:  0.8443785687932122 0.6916666666666667\n",
      "970 번째 loss, accuracy:  0.8442302344828209 0.6916666666666667\n",
      "971 번째 loss, accuracy:  0.8440820136036441 0.6916666666666667\n",
      "972 번째 loss, accuracy:  0.843933905951887 0.6916666666666667\n",
      "973 번째 loss, accuracy:  0.8437859113243474 0.6916666666666667\n",
      "974 번째 loss, accuracy:  0.8436380295184123 0.6916666666666667\n",
      "975 번째 loss, accuracy:  0.8434902603320557 0.6916666666666667\n",
      "976 번째 loss, accuracy:  0.8433426035638373 0.6916666666666667\n",
      "977 번째 loss, accuracy:  0.8431950590129006 0.6916666666666667\n",
      "978 번째 loss, accuracy:  0.843047626478971 0.6916666666666667\n",
      "979 번째 loss, accuracy:  0.8429003057623538 0.6916666666666667\n",
      "980 번째 loss, accuracy:  0.8427530966639307 0.6916666666666667\n",
      "981 번째 loss, accuracy:  0.8426059989851612 0.6916666666666667\n",
      "982 번째 loss, accuracy:  0.8424590125280785 0.6916666666666667\n",
      "983 번째 loss, accuracy:  0.842312137095287 0.6916666666666667\n",
      "984 번째 loss, accuracy:  0.842165372489962 0.6916666666666667\n",
      "985 번째 loss, accuracy:  0.8420187185158461 0.6916666666666667\n",
      "986 번째 loss, accuracy:  0.8418721749772502 0.6916666666666667\n",
      "987 번째 loss, accuracy:  0.8417257416790487 0.6916666666666667\n",
      "988 번째 loss, accuracy:  0.8415794184266782 0.6916666666666667\n",
      "989 번째 loss, accuracy:  0.8414332050261372 0.6916666666666667\n",
      "990 번째 loss, accuracy:  0.8412871012839831 0.6916666666666667\n",
      "991 번째 loss, accuracy:  0.8411411070073295 0.6916666666666667\n",
      "992 번째 loss, accuracy:  0.8409952220038467 0.6916666666666667\n",
      "993 번째 loss, accuracy:  0.8408494460817567 0.6916666666666667\n",
      "994 번째 loss, accuracy:  0.8407037790498351 0.6916666666666667\n",
      "995 번째 loss, accuracy:  0.8405582207174063 0.6916666666666667\n",
      "996 번째 loss, accuracy:  0.8404127708943421 0.6916666666666667\n",
      "997 번째 loss, accuracy:  0.840267429391062 0.6916666666666667\n",
      "998 번째 loss, accuracy:  0.8401221960185303 0.6916666666666667\n",
      "999 번째 loss, accuracy:  0.839977070588252 0.6916666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 loss, accuracy:  1.9029775616253757 0.3333333333333333\n",
      "1 번째 loss, accuracy:  1.823437173908355 0.3333333333333333\n",
      "2 번째 loss, accuracy:  1.7363476919395826 0.3333333333333333\n",
      "3 번째 loss, accuracy:  1.6397434815061085 0.3333333333333333\n",
      "4 번째 loss, accuracy:  1.540950491944863 0.3333333333333333\n",
      "5 번째 loss, accuracy:  1.4548076461217896 0.3333333333333333\n",
      "6 번째 loss, accuracy:  1.3883639362713247 0.3333333333333333\n",
      "7 번째 loss, accuracy:  1.3420768198959792 0.3333333333333333\n",
      "8 번째 loss, accuracy:  1.3075828326024614 0.3333333333333333\n",
      "9 번째 loss, accuracy:  1.2813677819981608 0.3333333333333333\n",
      "10 번째 loss, accuracy:  1.2595547537680711 0.325\n",
      "11 번째 loss, accuracy:  1.2401375733746167 0.31666666666666665\n",
      "12 번째 loss, accuracy:  1.2225314044969906 0.31666666666666665\n",
      "13 번째 loss, accuracy:  1.2062481296853855 0.31666666666666665\n",
      "14 번째 loss, accuracy:  1.1908314845655157 0.31666666666666665\n",
      "15 번째 loss, accuracy:  1.175993422930213 0.31666666666666665\n",
      "16 번째 loss, accuracy:  1.1618156872348957 0.325\n",
      "17 번째 loss, accuracy:  1.1481973535278935 0.325\n",
      "18 번째 loss, accuracy:  1.135101875700731 0.325\n",
      "19 번째 loss, accuracy:  1.1225177519534886 0.325\n",
      "20 번째 loss, accuracy:  1.1104157273267472 0.325\n",
      "21 번째 loss, accuracy:  1.098777341308196 0.3333333333333333\n",
      "22 번째 loss, accuracy:  1.087634333618712 0.3333333333333333\n",
      "23 번째 loss, accuracy:  1.076952886599166 0.3333333333333333\n",
      "24 번째 loss, accuracy:  1.0667854461313895 0.35\n",
      "25 번째 loss, accuracy:  1.0570621666812268 0.36666666666666664\n",
      "26 번째 loss, accuracy:  1.0477018631503925 0.36666666666666664\n",
      "27 번째 loss, accuracy:  1.038798514104941 0.38333333333333336\n",
      "28 번째 loss, accuracy:  1.03025528621937 0.4083333333333333\n",
      "29 번째 loss, accuracy:  1.0220775298488953 0.425\n",
      "30 번째 loss, accuracy:  1.0142848740937436 0.44166666666666665\n",
      "31 번째 loss, accuracy:  1.006811849059136 0.5083333333333333\n",
      "32 번째 loss, accuracy:  0.9996408694351983 0.5166666666666667\n",
      "33 번째 loss, accuracy:  0.9927690916079178 0.55\n",
      "34 번째 loss, accuracy:  0.9861401384331681 0.5583333333333333\n",
      "35 번째 loss, accuracy:  0.979769973152363 0.5833333333333334\n",
      "36 번째 loss, accuracy:  0.9736554634849017 0.6083333333333333\n",
      "37 번째 loss, accuracy:  0.9677660974839938 0.625\n",
      "38 번째 loss, accuracy:  0.9620834300060134 0.65\n",
      "39 번째 loss, accuracy:  0.9566164978166735 0.675\n",
      "40 번째 loss, accuracy:  0.951341902304851 0.675\n",
      "41 번째 loss, accuracy:  0.9462520181837978 0.675\n",
      "42 번째 loss, accuracy:  0.9413172061770076 0.675\n",
      "43 번째 loss, accuracy:  0.9365654925631336 0.6833333333333333\n",
      "44 번째 loss, accuracy:  0.9319689231337802 0.6833333333333333\n",
      "45 번째 loss, accuracy:  0.9275184867680117 0.6833333333333333\n",
      "46 번째 loss, accuracy:  0.92320873750099 0.6916666666666667\n",
      "47 번째 loss, accuracy:  0.9190246598727079 0.6916666666666667\n",
      "48 번째 loss, accuracy:  0.9149774947012222 0.6916666666666667\n",
      "49 번째 loss, accuracy:  0.9110348951874486 0.6916666666666667\n",
      "50 번째 loss, accuracy:  0.9072182572841977 0.6916666666666667\n",
      "51 번째 loss, accuracy:  0.9035109972596745 0.6916666666666667\n",
      "52 번째 loss, accuracy:  0.8999040486028211 0.6916666666666667\n",
      "53 번째 loss, accuracy:  0.8964017223548628 0.6916666666666667\n",
      "54 번째 loss, accuracy:  0.8929891269671858 0.6916666666666667\n",
      "55 번째 loss, accuracy:  0.8896610991231412 0.6916666666666667\n",
      "56 번째 loss, accuracy:  0.8864180820202978 0.6916666666666667\n",
      "57 번째 loss, accuracy:  0.8832538136072988 0.6916666666666667\n",
      "58 번째 loss, accuracy:  0.8801685475625177 0.6916666666666667\n",
      "59 번째 loss, accuracy:  0.8771524820204267 0.6916666666666667\n",
      "60 번째 loss, accuracy:  0.8742092717723007 0.6916666666666667\n",
      "61 번째 loss, accuracy:  0.8713348653584816 0.6916666666666667\n",
      "62 번째 loss, accuracy:  0.868520600626708 0.6916666666666667\n",
      "63 번째 loss, accuracy:  0.865769326943911 0.6916666666666667\n",
      "64 번째 loss, accuracy:  0.8630753315278891 0.6916666666666667\n",
      "65 번째 loss, accuracy:  0.8604397373895265 0.6916666666666667\n",
      "66 번째 loss, accuracy:  0.8578480347381183 0.6916666666666667\n",
      "67 번째 loss, accuracy:  0.8553146540453649 0.6916666666666667\n",
      "68 번째 loss, accuracy:  0.8528284511786791 0.6916666666666667\n",
      "69 번째 loss, accuracy:  0.8503892498923573 0.6916666666666667\n",
      "70 번째 loss, accuracy:  0.8479929487048269 0.6916666666666667\n",
      "71 번째 loss, accuracy:  0.8456398863027383 0.6916666666666667\n",
      "72 번째 loss, accuracy:  0.8433280007915359 0.6916666666666667\n",
      "73 번째 loss, accuracy:  0.841055009444096 0.6916666666666667\n",
      "74 번째 loss, accuracy:  0.8388199476558277 0.6916666666666667\n",
      "75 번째 loss, accuracy:  0.836624084714053 0.6916666666666667\n",
      "76 번째 loss, accuracy:  0.8344633749931092 0.6916666666666667\n",
      "77 번째 loss, accuracy:  0.8323325309418882 0.6916666666666667\n",
      "78 번째 loss, accuracy:  0.8302370965690786 0.6916666666666667\n",
      "79 번째 loss, accuracy:  0.8281709185704396 0.6916666666666667\n",
      "80 번째 loss, accuracy:  0.8261371413814326 0.6916666666666667\n",
      "81 번째 loss, accuracy:  0.824130636748268 0.6916666666666667\n",
      "82 번째 loss, accuracy:  0.822153232619615 0.6916666666666667\n",
      "83 번째 loss, accuracy:  0.8202031470917108 0.6916666666666667\n",
      "84 번째 loss, accuracy:  0.8182800372508544 0.6916666666666667\n",
      "85 번째 loss, accuracy:  0.8163807909138474 0.6916666666666667\n",
      "86 번째 loss, accuracy:  0.8145085122727767 0.6916666666666667\n",
      "87 번째 loss, accuracy:  0.8126489670999277 0.6916666666666667\n",
      "88 번째 loss, accuracy:  0.8108219364401476 0.6916666666666667\n",
      "89 번째 loss, accuracy:  0.8090165811985315 0.6916666666666667\n",
      "90 번째 loss, accuracy:  0.8072321080894962 0.6916666666666667\n",
      "91 번째 loss, accuracy:  0.805470974330624 0.6916666666666667\n",
      "92 번째 loss, accuracy:  0.8037294701788464 0.6916666666666667\n",
      "93 번째 loss, accuracy:  0.8020068042297621 0.6916666666666667\n",
      "94 번째 loss, accuracy:  0.8003029841668681 0.6916666666666667\n",
      "95 번째 loss, accuracy:  0.7986158413878086 0.6916666666666667\n",
      "96 번째 loss, accuracy:  0.7969491872272824 0.6916666666666667\n",
      "97 번째 loss, accuracy:  0.7953001424514738 0.6916666666666667\n",
      "98 번째 loss, accuracy:  0.7936683163534102 0.6916666666666667\n",
      "99 번째 loss, accuracy:  0.7920523554303861 0.6916666666666667\n",
      "100 번째 loss, accuracy:  0.7904538317913133 0.6916666666666667\n",
      "101 번째 loss, accuracy:  0.7888713808481119 0.6916666666666667\n",
      "102 번째 loss, accuracy:  0.7873037142300107 0.6916666666666667\n",
      "103 번째 loss, accuracy:  0.7857492966330952 0.6916666666666667\n",
      "104 번째 loss, accuracy:  0.784208391900386 0.6916666666666667\n",
      "105 번째 loss, accuracy:  0.7826852900243008 0.6916666666666667\n",
      "106 번째 loss, accuracy:  0.78117656831926 0.6916666666666667\n",
      "107 번째 loss, accuracy:  0.7796780622160194 0.6916666666666667\n",
      "108 번째 loss, accuracy:  0.7781951582311378 0.6916666666666667\n",
      "109 번째 loss, accuracy:  0.7767265920525427 0.6916666666666667\n",
      "110 번째 loss, accuracy:  0.7752710892727273 0.6916666666666667\n",
      "111 번째 loss, accuracy:  0.7738275347677105 0.6916666666666667\n",
      "112 번째 loss, accuracy:  0.7723970965728066 0.6916666666666667\n",
      "113 번째 loss, accuracy:  0.7709792112851159 0.6916666666666667\n",
      "114 번째 loss, accuracy:  0.7695718740328347 0.6916666666666667\n",
      "115 번째 loss, accuracy:  0.7681763159496543 0.6916666666666667\n",
      "116 번째 loss, accuracy:  0.7667921407715678 0.6916666666666667\n",
      "117 번째 loss, accuracy:  0.7654197330474674 0.6916666666666667\n",
      "118 번째 loss, accuracy:  0.7640581307559388 0.6916666666666667\n",
      "119 번째 loss, accuracy:  0.7627064482168876 0.6916666666666667\n",
      "120 번째 loss, accuracy:  0.7613659119403065 0.6916666666666667\n",
      "121 번째 loss, accuracy:  0.7600356052708709 0.6916666666666667\n",
      "122 번째 loss, accuracy:  0.7587160682248034 0.6916666666666667\n",
      "123 번째 loss, accuracy:  0.7574065413964219 0.6916666666666667\n",
      "124 번째 loss, accuracy:  0.7561060620979381 0.6916666666666667\n",
      "125 번째 loss, accuracy:  0.7548165006800406 0.6916666666666667\n",
      "126 번째 loss, accuracy:  0.7535369312124973 0.6916666666666667\n",
      "127 번째 loss, accuracy:  0.7522661528073051 0.6916666666666667\n",
      "128 번째 loss, accuracy:  0.7510045370199016 0.6916666666666667\n",
      "129 번째 loss, accuracy:  0.7497524698733714 0.6916666666666667\n",
      "130 번째 loss, accuracy:  0.748508104567428 0.6916666666666667\n",
      "131 번째 loss, accuracy:  0.7472738284530701 0.6916666666666667\n",
      "132 번째 loss, accuracy:  0.7460485780222578 0.6916666666666667\n",
      "133 번째 loss, accuracy:  0.7448306085851322 0.6916666666666667\n",
      "134 번째 loss, accuracy:  0.7436220263881791 0.6916666666666667\n",
      "135 번째 loss, accuracy:  0.7424220594832628 0.6916666666666667\n",
      "136 번째 loss, accuracy:  0.7412304660247725 0.6916666666666667\n",
      "137 번째 loss, accuracy:  0.7400463704252611 0.6916666666666667\n",
      "138 번째 loss, accuracy:  0.7388707373508538 0.6916666666666667\n",
      "139 번째 loss, accuracy:  0.7377027250059339 0.6916666666666667\n",
      "140 번째 loss, accuracy:  0.7365424908792311 0.6916666666666667\n",
      "141 번째 loss, accuracy:  0.7353906502910922 0.6916666666666667\n",
      "142 번째 loss, accuracy:  0.7342459310901428 0.6916666666666667\n",
      "143 번째 loss, accuracy:  0.7331086236336242 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 번째 loss, accuracy:  0.7319789512082309 0.6916666666666667\n",
      "145 번째 loss, accuracy:  0.7308559609640846 0.6916666666666667\n",
      "146 번째 loss, accuracy:  0.7297403040599846 0.6916666666666667\n",
      "147 번째 loss, accuracy:  0.7286328326710241 0.6916666666666667\n",
      "148 번째 loss, accuracy:  0.7275316787620919 0.6916666666666667\n",
      "149 번째 loss, accuracy:  0.7264380992445507 0.6916666666666667\n",
      "150 번째 loss, accuracy:  0.7253516308441481 0.6916666666666667\n",
      "151 번째 loss, accuracy:  0.7242718421824107 0.6916666666666667\n",
      "152 번째 loss, accuracy:  0.7231990468196866 0.6916666666666667\n",
      "153 번째 loss, accuracy:  0.7221330167080259 0.6916666666666667\n",
      "154 번째 loss, accuracy:  0.7210732852531514 0.6916666666666667\n",
      "155 번째 loss, accuracy:  0.72002012288491 0.6916666666666667\n",
      "156 번째 loss, accuracy:  0.718973646683471 0.6916666666666667\n",
      "157 번째 loss, accuracy:  0.7179337460209785 0.6916666666666667\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1ee03bfbd276>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;31m# 학습: epoch번 반복한 loss와 accuracy의 List를 return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[0mlossPlt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccPlt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtn2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mlo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtn2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\tlnn2.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, lr, epoch, batch_size, check, idx)\u001b[0m\n\u001b[0;32m    123\u001b[0m                 \u001b[1;31m# 미니 배치 훈련 데이터를 통해 기울기를 구한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m                 \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m                 \u001b[1;31m# 기울어진 방향으로 가중치(W1, b1, W2, b2)의 값을 갱신한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\tlnn2.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# 새 딕셔너리 params에 현재 상태에 대한 Weight값과 bias값의 기울기를 저장한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, X)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# idx에 해당하는 행마다 기울기를 구해준다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient_no_batch_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient_no_batch_\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# f(x-h) 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mfxh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfxh1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfxh2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 미분값(기울기)을 구한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\tlnn2.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;34m\"\"\" 가중치 매개변수의 기울기를 구한다. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;31m# lambda를 이용하여 loss(x, t)의 함수 f를 구한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# 새 딕셔너리 params에 현재 상태에 대한 Weight값과 bias값의 기울기를 저장한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\tlnn2.py\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;34m\"\"\" x를 통해 들어온 데이터를 계산하여 도출한 y값과 실제 target값의 loss를 계산 \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# 만약 target으로 넘어온 값이 1차원 array인 label이라면\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\tlnn2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# input --> hidden layer : sigmoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mz2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0ma2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import load_iris\n",
    "from tlnn2 import TwoLayerNeuralNetwork2\n",
    "\n",
    "\"\"\" iris Data \"\"\"\n",
    "iris = load_iris()\n",
    "X = iris.data # iris data input\n",
    "y = iris.target # iris target (label)\n",
    "\n",
    "# 데이터 Split Use training : testing = 8 : 2 => 120 : 30\n",
    "suffle = np.random.choice(X.shape[0], X.shape[0], replace=False)\n",
    "for_train = suffle[:120]\n",
    "for_test = suffle[120:]\n",
    "\n",
    "# for training data (X, y)\n",
    "X_train = X[for_train]\n",
    "y_train = y[for_train]\n",
    "# for testing data (X, y)\n",
    "X_test = X[for_test]\n",
    "y_test = y[for_test]\n",
    "\n",
    "\"\"\" hidden layer의 Unit 수 = 3, 5, 7\"\"\"\n",
    "hidden_size = [3, 5, 7]\n",
    "input_size = 4\n",
    "output_size = 3\n",
    "\n",
    "\"\"\" hyperParameter 값 변화하며 Test \"\"\"\n",
    "lr = [0.02, 0.01, 0.005] # learningRate 0.02, 0.01, 0.005\n",
    "epoch = [1000, 5000, 10000] # epoch 1000, 5000, 10000\n",
    "batch_size = [40, 60, 120] # batchSize 40, 60, 120\n",
    "check = 1 # loss, accuracy file에 저장\n",
    "\n",
    "for hid in hidden_size:\n",
    "    tn2 = TwoLayerNeuralNetwork2(input_size, hid, output_size)\n",
    "    tn2.init_data(X_train, y_train) # Set Training Data\n",
    "    for e in epoch:\n",
    "        for l in lr:\n",
    "            for b in batch_size:\n",
    "                lossPlt = []\n",
    "                accPlt = []\n",
    "                \n",
    "                # 학습: epoch번 반복한 loss와 accuracy의 List를 return \n",
    "                lossPlt, accPlt = tn2.learn(l, e, b, check) \n",
    "                \n",
    "                lo = round(tn2.loss(X_train, y_train), 5)\n",
    "                Tr = round(tn2.accuracy(X_train, y_train), 5)\n",
    "                Te = round(tn2.accuracy(X_test, y_test), 5)\n",
    "                tn2.reset() # parameter값들 Reset 해주어야 한다.\n",
    "                \n",
    "                # loss와 training accuracy를 Plot\n",
    "                x = np.arange(e)\n",
    "                plt.plot(x, lossPlt, x, accPlt)\n",
    "                plt.legend([\"loss\", \"training accuracy\"]) # 각주\n",
    "                plt.title('hiddenSize: {}, lr: {}, epoch: {}, batchSize: {}\\n loss: {}, Train_Acc: {}, Test_Acc: {}'\\\n",
    "                          .format(hid, l, e, b, lo, Tr, Te))\n",
    "#                 plt.savefig('./Result/[hi_{}]_[lr_{}]_[ep_{}]_[ba_{}].png'.format(hid, l, e, b), dpi=100)\n",
    "                plt.show()\n",
    "                plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
