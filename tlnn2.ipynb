{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from functions import sigmoid, softmax\n",
    "from gradient import cross_entropy_error_label, cross_entropy_error, numerical_gradient\n",
    "\n",
    "class TwoLayerNeuralNetwork2:\n",
    "    \"\"\" a neural network with one hidden layer \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\" initialize parameters \"\"\"\n",
    "        self.params = {}\n",
    "        # input --> hidden layer\n",
    "        self.params['W1'] = np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.random.randn(hidden_size)\n",
    "        # hidden layer --> output layer\n",
    "        self.params['W2'] = np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.random.randn(output_size)\n",
    "        self.input_size, self.hidden_size, self.output_size = input_size, hidden_size, output_size # input, output size 저장\n",
    "        self.x, self.t = [], []\n",
    "        \n",
    "    def init_data(self, x_train, t_train):\n",
    "        self.x = x_train\n",
    "        # 1차원 벡터라면 원 핫 인코딩을 해준다.\n",
    "        if t_train.ndim == 1:\n",
    "            t_train = np.eye(np.unique(t_train).shape[0])[t_train]\n",
    "        self.t = t_train\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"calculate output given input and current parameters: W1, b1, W2, b2 \"\"\"\n",
    "        W1, W2, b1, b2 = self.params['W1'], self.params['W2'], self.params['b1'], self.params['b2']\n",
    "        \n",
    "        # input --> hidden layer : sigmoid\n",
    "        z2 = np.dot(x, W1) + b1\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        # hidden layer --> output : softmax\n",
    "        z3 = np.dot(a2, W2) + b2\n",
    "        y = softmax(z3)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        if t.ndim==1:\n",
    "            return cross_entropy_error_label(y, t)\n",
    "        return cross_entropy_error(y, t)\n",
    "        \n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\" testData로 실제 target과 계산된 y를 비교해서 정확도를 구한다. \"\"\"\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        \n",
    "        # target_y값이 원 핫 인코딩이 되어있다면 1차원의 ndarray로 변환한다.\n",
    "        if t.ndim == 2:\n",
    "            t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        # 정확도는 target값과 계산된 y값이 같은 갯수를 test데이터 수로 나눈 확률 값\n",
    "        accuracy = np.sum(y-t == 0) / t.shape[0]\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\" 가중치 매개변수의 기울기를 구한다. \"\"\"\n",
    "        # lambda를 이용하여 loss(x, t)의 함수 f를 구한다.\n",
    "        f = lambda W: self.loss(x, t)\n",
    "\n",
    "        # 새 딕셔너리 params에 각 layer의 Weight값과 bias값의 기울기를 저장한다.\n",
    "        params = {}\n",
    "        params['W1'] = numerical_gradient(f, self.params['W1'])\n",
    "        params['b1'] = numerical_gradient(f, self.params['b1'])\n",
    "        params['W2'] = numerical_gradient(f, self.params['W2'])\n",
    "        params['b2'] = numerical_gradient(f, self.params['b2'])\n",
    "        \n",
    "        return params\n",
    "        \n",
    "    # batch = True이면 batch 사용, check = True라면 값의 추이, plt를 확인한다.\n",
    "    def learn(self, lr = 0.01, epoch = 100, batch_size = 1, batch = True, check = True):\n",
    "        \"\"\" pre-requisite: x, t are stored in the local attribute\"\"\"\n",
    "        # Plt 추이를 보기 위한 list 선언\n",
    "        lossPlt, accPlt = [], []\n",
    "        \n",
    "        # epoch 만큼 반복 수행\n",
    "        for i in range(epoch):\n",
    "            # Plt를 보고 싶다면 check=True\n",
    "            # lr,epoch,batchsize 변화 비교를 위해 불필요한 연산을 Skip하고 싶다면 check=False\n",
    "            if check:\n",
    "                # 훈련데이터의 현재 loss값과 accuracy를 구한다.\n",
    "                lo = self.loss(self.x, self.t)\n",
    "                ac = self.accuracy(self.x, self.t)\n",
    "                \n",
    "                # 각 list에 추가한다. (plot을 보기 위함)\n",
    "                lossPlt.append(lo)\n",
    "                accPlt.append(ac)\n",
    "                print(i, \"번째 loss, accuracy: \" , lo, ac)\n",
    "            \n",
    "            # 훈련할 데이터 x값과 target을 x_train, t_train에 배정한다.\n",
    "            x_train = self.x\n",
    "            t_train = self.t\n",
    "            \n",
    "            # batch를 사용한다면\n",
    "            if batch: \n",
    "                # 0 ~ 훈련Data 중에 batch_size만큼 random으로 뽑아낸다. 이때, 중복된 값도 허용된다.\n",
    "                batch_mask = np.random.choice(self.x.shape[0], batch_size)\n",
    "                x_train = self.x[batch_mask]\n",
    "                t_train = self.t[batch_mask]\n",
    "            \n",
    "            # 훈련 데이터를 통해 기울기를 구한다.\n",
    "            params = self.numerical_gradient(x_train, t_train)\n",
    "            # 기울어진 방향으로 가중치의 값을 조정하고 learningRate를 곱함으로 overshooting을 막는다.\n",
    "            for key in self.params:\n",
    "                self.params[key] -= lr * params[key]\n",
    "        \n",
    "        # 만약 check=True라면 loss와 accuracy의 Plt List들을 return\n",
    "        if check:\n",
    "            return lossPlt, accPlt\n",
    "        \n",
    "    # Class의 변수들 초기화 -> lr, epoch, batchSize 변화 비교 실험 시 필요하다.\n",
    "    def reset(self):\n",
    "        \"\"\" reset parameters \"\"\"\n",
    "        self.params['W1'] = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.params['b1'] = np.random.randn(self.hidden_size)\n",
    "        self.params['W2'] = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.params['b2'] = np.random.randn(self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 loss, accuracy:  1.3366839897242788 0.325\n",
      "1 번째 loss, accuracy:  1.3322864611777718 0.325\n",
      "2 번째 loss, accuracy:  1.3283775685522434 0.325\n",
      "3 번째 loss, accuracy:  1.3240255895420552 0.325\n",
      "4 번째 loss, accuracy:  1.319548519786326 0.325\n",
      "5 번째 loss, accuracy:  1.3150776093035508 0.325\n",
      "6 번째 loss, accuracy:  1.3111561170201746 0.325\n",
      "7 번째 loss, accuracy:  1.3069865021686022 0.325\n",
      "8 번째 loss, accuracy:  1.3029342995750521 0.325\n",
      "9 번째 loss, accuracy:  1.2983699324842803 0.325\n",
      "10 번째 loss, accuracy:  1.2945889227733067 0.325\n",
      "11 번째 loss, accuracy:  1.2909414854662775 0.325\n",
      "12 번째 loss, accuracy:  1.287103694634655 0.325\n",
      "13 번째 loss, accuracy:  1.2836170899748578 0.325\n",
      "14 번째 loss, accuracy:  1.27995418503903 0.325\n",
      "15 번째 loss, accuracy:  1.2764974028058547 0.325\n",
      "16 번째 loss, accuracy:  1.2722754956639764 0.325\n",
      "17 번째 loss, accuracy:  1.2688091241389814 0.325\n",
      "18 번째 loss, accuracy:  1.2650273732226818 0.325\n",
      "19 번째 loss, accuracy:  1.261789845578931 0.325\n",
      "20 번째 loss, accuracy:  1.2580809189815159 0.325\n",
      "21 번째 loss, accuracy:  1.2541674938688776 0.325\n",
      "22 번째 loss, accuracy:  1.250268493552482 0.325\n",
      "23 번째 loss, accuracy:  1.246789585885986 0.325\n",
      "24 번째 loss, accuracy:  1.243658908249849 0.325\n",
      "25 번째 loss, accuracy:  1.2400778460439374 0.325\n",
      "26 번째 loss, accuracy:  1.2362903833588026 0.325\n",
      "27 번째 loss, accuracy:  1.2332765919263253 0.325\n",
      "28 번째 loss, accuracy:  1.2304754782765108 0.325\n",
      "29 번째 loss, accuracy:  1.2268977523651037 0.325\n",
      "30 번째 loss, accuracy:  1.224021711930493 0.325\n",
      "31 번째 loss, accuracy:  1.2209896326793086 0.325\n",
      "32 번째 loss, accuracy:  1.2185076884581316 0.325\n",
      "33 번째 loss, accuracy:  1.2161351331628596 0.325\n",
      "34 번째 loss, accuracy:  1.213261209753435 0.325\n",
      "35 번째 loss, accuracy:  1.2103980741886782 0.325\n",
      "36 번째 loss, accuracy:  1.2075986455199248 0.325\n",
      "37 번째 loss, accuracy:  1.20399808497655 0.325\n",
      "38 번째 loss, accuracy:  1.2012183715308116 0.325\n",
      "39 번째 loss, accuracy:  1.1984883669774775 0.325\n",
      "40 번째 loss, accuracy:  1.1964411390183447 0.325\n",
      "41 번째 loss, accuracy:  1.1939146581798252 0.325\n",
      "42 번째 loss, accuracy:  1.191028168461049 0.325\n",
      "43 번째 loss, accuracy:  1.188100106792677 0.325\n",
      "44 번째 loss, accuracy:  1.1858558376235233 0.325\n",
      "45 번째 loss, accuracy:  1.183784573791801 0.325\n",
      "46 번째 loss, accuracy:  1.1818653420130554 0.325\n",
      "47 번째 loss, accuracy:  1.1795181150505565 0.325\n",
      "48 번째 loss, accuracy:  1.1775232948538035 0.325\n",
      "49 번째 loss, accuracy:  1.1755140865135754 0.325\n",
      "50 번째 loss, accuracy:  1.1727264506914128 0.325\n",
      "51 번째 loss, accuracy:  1.1699643509953608 0.325\n",
      "52 번째 loss, accuracy:  1.1673756442178473 0.325\n",
      "53 번째 loss, accuracy:  1.1651629177378713 0.325\n",
      "54 번째 loss, accuracy:  1.1622318855729274 0.325\n",
      "55 번째 loss, accuracy:  1.1601596154739373 0.325\n",
      "56 번째 loss, accuracy:  1.1586418303113148 0.325\n",
      "57 번째 loss, accuracy:  1.1562908561880418 0.325\n",
      "58 번째 loss, accuracy:  1.1542386675634049 0.325\n",
      "59 번째 loss, accuracy:  1.1523127235756865 0.325\n",
      "60 번째 loss, accuracy:  1.1500740929604882 0.325\n",
      "61 번째 loss, accuracy:  1.1482459530772162 0.325\n",
      "62 번째 loss, accuracy:  1.146225455850048 0.325\n",
      "63 번째 loss, accuracy:  1.1444934180627737 0.325\n",
      "64 번째 loss, accuracy:  1.1423381833631432 0.325\n",
      "65 번째 loss, accuracy:  1.1402744376501248 0.325\n",
      "66 번째 loss, accuracy:  1.1384289829240923 0.325\n",
      "67 번째 loss, accuracy:  1.136447458345535 0.325\n",
      "68 번째 loss, accuracy:  1.1341105529840327 0.325\n",
      "69 번째 loss, accuracy:  1.1316591703766046 0.325\n",
      "70 번째 loss, accuracy:  1.130953528847354 0.325\n",
      "71 번째 loss, accuracy:  1.1290000272449878 0.325\n",
      "72 번째 loss, accuracy:  1.1278204376418788 0.325\n",
      "73 번째 loss, accuracy:  1.1251741478426094 0.325\n",
      "74 번째 loss, accuracy:  1.123373997930009 0.325\n",
      "75 번째 loss, accuracy:  1.1213311587223174 0.325\n",
      "76 번째 loss, accuracy:  1.1199611737501214 0.325\n",
      "77 번째 loss, accuracy:  1.1176945666522313 0.325\n",
      "78 번째 loss, accuracy:  1.1158291010243653 0.325\n",
      "79 번째 loss, accuracy:  1.114452929106141 0.325\n",
      "80 번째 loss, accuracy:  1.1125439462605362 0.325\n",
      "81 번째 loss, accuracy:  1.1109284963191721 0.325\n",
      "82 번째 loss, accuracy:  1.1091969563910238 0.325\n",
      "83 번째 loss, accuracy:  1.107578903408331 0.325\n",
      "84 번째 loss, accuracy:  1.105482557670501 0.325\n",
      "85 번째 loss, accuracy:  1.1042863545766584 0.325\n",
      "86 번째 loss, accuracy:  1.1020202618325539 0.325\n",
      "87 번째 loss, accuracy:  1.0997048226500017 0.325\n",
      "88 번째 loss, accuracy:  1.098464125384056 0.325\n",
      "89 번째 loss, accuracy:  1.0966268135168526 0.325\n",
      "90 번째 loss, accuracy:  1.0943630199051595 0.325\n",
      "91 번째 loss, accuracy:  1.0925664048269959 0.325\n",
      "92 번째 loss, accuracy:  1.0910569806155317 0.325\n",
      "93 번째 loss, accuracy:  1.0893496600197776 0.325\n",
      "94 번째 loss, accuracy:  1.0880635893665234 0.325\n",
      "95 번째 loss, accuracy:  1.0863398950189516 0.325\n",
      "96 번째 loss, accuracy:  1.084262251976553 0.325\n",
      "97 번째 loss, accuracy:  1.0828501132281554 0.325\n",
      "98 번째 loss, accuracy:  1.08048931142018 0.325\n",
      "99 번째 loss, accuracy:  1.0789303337328577 0.325\n",
      "100 번째 loss, accuracy:  1.076859262507898 0.325\n",
      "101 번째 loss, accuracy:  1.075683237184363 0.325\n",
      "102 번째 loss, accuracy:  1.074271688233782 0.325\n",
      "103 번째 loss, accuracy:  1.072335160667609 0.325\n",
      "104 번째 loss, accuracy:  1.0701848618825158 0.325\n",
      "105 번째 loss, accuracy:  1.0681735114480315 0.325\n",
      "106 번째 loss, accuracy:  1.0665421908570307 0.325\n",
      "107 번째 loss, accuracy:  1.06553490010939 0.325\n",
      "108 번째 loss, accuracy:  1.0635747631322399 0.325\n",
      "109 번째 loss, accuracy:  1.0621885493218868 0.325\n",
      "110 번째 loss, accuracy:  1.0606240360573667 0.325\n",
      "111 번째 loss, accuracy:  1.0584850250369089 0.325\n",
      "112 번째 loss, accuracy:  1.0565138786600214 0.325\n",
      "113 번째 loss, accuracy:  1.055145165470691 0.325\n",
      "114 번째 loss, accuracy:  1.0540119783807216 0.325\n",
      "115 번째 loss, accuracy:  1.053028137042893 0.325\n",
      "116 번째 loss, accuracy:  1.0517777951198577 0.325\n",
      "117 번째 loss, accuracy:  1.0497219153658628 0.325\n",
      "118 번째 loss, accuracy:  1.0481229130471377 0.325\n",
      "119 번째 loss, accuracy:  1.0466372436868712 0.325\n",
      "120 번째 loss, accuracy:  1.0454350435537598 0.325\n",
      "121 번째 loss, accuracy:  1.044696406499386 0.325\n",
      "122 번째 loss, accuracy:  1.0431609152438317 0.325\n",
      "123 번째 loss, accuracy:  1.0415516192889924 0.325\n",
      "124 번째 loss, accuracy:  1.0401038153703999 0.325\n",
      "125 번째 loss, accuracy:  1.0387343377599094 0.325\n",
      "126 번째 loss, accuracy:  1.0371039291668527 0.325\n",
      "127 번째 loss, accuracy:  1.0356389300889877 0.325\n",
      "128 번째 loss, accuracy:  1.0343989320766418 0.325\n",
      "129 번째 loss, accuracy:  1.0333351580476988 0.325\n",
      "130 번째 loss, accuracy:  1.031899765311512 0.325\n",
      "131 번째 loss, accuracy:  1.030518176582332 0.325\n",
      "132 번째 loss, accuracy:  1.0289857680388492 0.325\n",
      "133 번째 loss, accuracy:  1.0282415839110286 0.325\n",
      "134 번째 loss, accuracy:  1.02648462408306 0.325\n",
      "135 번째 loss, accuracy:  1.0250966806202761 0.325\n",
      "136 번째 loss, accuracy:  1.0247026667173116 0.325\n",
      "137 번째 loss, accuracy:  1.023038439916473 0.325\n",
      "138 번째 loss, accuracy:  1.0217871332147672 0.325\n",
      "139 번째 loss, accuracy:  1.0206118365082497 0.325\n",
      "140 번째 loss, accuracy:  1.0192744435702834 0.325\n",
      "141 번째 loss, accuracy:  1.0178639112787389 0.325\n",
      "142 번째 loss, accuracy:  1.016880328925985 0.325\n",
      "143 번째 loss, accuracy:  1.015736524397737 0.325\n",
      "144 번째 loss, accuracy:  1.0141997085671777 0.325\n",
      "145 번째 loss, accuracy:  1.0131733404879089 0.325\n",
      "146 번째 loss, accuracy:  1.012324853267961 0.325\n",
      "147 번째 loss, accuracy:  1.0113346921107607 0.325\n",
      "148 번째 loss, accuracy:  1.0100970749750429 0.325\n",
      "149 번째 loss, accuracy:  1.0092441036224176 0.325\n",
      "150 번째 loss, accuracy:  1.0084059063155792 0.325\n",
      "151 번째 loss, accuracy:  1.0077450388792897 0.325\n",
      "152 번째 loss, accuracy:  1.006557052230935 0.325\n",
      "153 번째 loss, accuracy:  1.0052841146261067 0.325\n",
      "154 번째 loss, accuracy:  1.0041508558459853 0.325\n",
      "155 번째 loss, accuracy:  1.0029818067553005 0.325\n",
      "156 번째 loss, accuracy:  1.0017840637063629 0.325\n",
      "157 번째 loss, accuracy:  1.0008044794911337 0.325\n",
      "158 번째 loss, accuracy:  0.9996646904455908 0.325\n",
      "159 번째 loss, accuracy:  0.9992263195921918 0.325\n",
      "160 번째 loss, accuracy:  0.9981610582323807 0.325\n",
      "161 번째 loss, accuracy:  0.9971396286106636 0.325\n",
      "162 번째 loss, accuracy:  0.9962897133499123 0.325\n",
      "163 번째 loss, accuracy:  0.995075119497186 0.325\n",
      "164 번째 loss, accuracy:  0.9936223682182682 0.325\n",
      "165 번째 loss, accuracy:  0.9926192592922877 0.325\n",
      "166 번째 loss, accuracy:  0.9912679353948768 0.325\n",
      "167 번째 loss, accuracy:  0.9905647257174639 0.325\n",
      "168 번째 loss, accuracy:  0.9893142292032233 0.325\n",
      "169 번째 loss, accuracy:  0.9886019283117536 0.325\n",
      "170 번째 loss, accuracy:  0.9874054783923292 0.325\n",
      "171 번째 loss, accuracy:  0.9867890591506947 0.325\n",
      "172 번째 loss, accuracy:  0.9857216964961159 0.325\n",
      "173 번째 loss, accuracy:  0.9849065034305684 0.325\n",
      "174 번째 loss, accuracy:  0.9837714730541449 0.325\n",
      "175 번째 loss, accuracy:  0.9826470921000644 0.325\n",
      "176 번째 loss, accuracy:  0.9820132840639656 0.325\n",
      "177 번째 loss, accuracy:  0.9812018423389082 0.325\n",
      "178 번째 loss, accuracy:  0.9802516117962234 0.325\n",
      "179 번째 loss, accuracy:  0.9789816699883249 0.325\n",
      "180 번째 loss, accuracy:  0.9776941870254762 0.325\n",
      "181 번째 loss, accuracy:  0.9768047301728999 0.325\n",
      "182 번째 loss, accuracy:  0.9753896478899791 0.325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 번째 loss, accuracy:  0.9743839626605622 0.325\n",
      "184 번째 loss, accuracy:  0.9734788169776755 0.325\n",
      "185 번째 loss, accuracy:  0.9723800989113905 0.3333333333333333\n",
      "186 번째 loss, accuracy:  0.971690117740639 0.3333333333333333\n",
      "187 번째 loss, accuracy:  0.970362737549967 0.3333333333333333\n",
      "188 번째 loss, accuracy:  0.9693920966397774 0.3333333333333333\n",
      "189 번째 loss, accuracy:  0.9683719095219262 0.3416666666666667\n",
      "190 번째 loss, accuracy:  0.9671288263311068 0.35833333333333334\n",
      "191 번째 loss, accuracy:  0.9659422832032267 0.36666666666666664\n",
      "192 번째 loss, accuracy:  0.9648848122062093 0.375\n",
      "193 번째 loss, accuracy:  0.9642500881748407 0.375\n",
      "194 번째 loss, accuracy:  0.9632449047213497 0.375\n",
      "195 번째 loss, accuracy:  0.9618772869218531 0.44166666666666665\n",
      "196 번째 loss, accuracy:  0.9605554013025036 0.48333333333333334\n",
      "197 번째 loss, accuracy:  0.9593051453287292 0.5166666666666667\n",
      "198 번째 loss, accuracy:  0.958626753414693 0.5166666666666667\n",
      "199 번째 loss, accuracy:  0.9575624267879426 0.5416666666666666\n",
      "200 번째 loss, accuracy:  0.9565849146267039 0.5583333333333333\n",
      "201 번째 loss, accuracy:  0.9556397426407756 0.6\n",
      "202 번째 loss, accuracy:  0.9543514333335051 0.6083333333333333\n",
      "203 번째 loss, accuracy:  0.9537270709580582 0.625\n",
      "204 번째 loss, accuracy:  0.9531872324804934 0.625\n",
      "205 번째 loss, accuracy:  0.9520955323788965 0.6333333333333333\n",
      "206 번째 loss, accuracy:  0.951295498860749 0.6333333333333333\n",
      "207 번째 loss, accuracy:  0.9506734085949724 0.6333333333333333\n",
      "208 번째 loss, accuracy:  0.9498183933076109 0.6333333333333333\n",
      "209 번째 loss, accuracy:  0.9484118859029113 0.6583333333333333\n",
      "210 번째 loss, accuracy:  0.9474178853015994 0.6583333333333333\n",
      "211 번째 loss, accuracy:  0.9460416132284358 0.6666666666666666\n",
      "212 번째 loss, accuracy:  0.9451372385489809 0.6666666666666666\n",
      "213 번째 loss, accuracy:  0.9442719060294131 0.6666666666666666\n",
      "214 번째 loss, accuracy:  0.9436145300043456 0.6666666666666666\n",
      "215 번째 loss, accuracy:  0.9428587828737043 0.6666666666666666\n",
      "216 번째 loss, accuracy:  0.9421631862161971 0.6666666666666666\n",
      "217 번째 loss, accuracy:  0.9412250651689889 0.6666666666666666\n",
      "218 번째 loss, accuracy:  0.9401776810369502 0.6666666666666666\n",
      "219 번째 loss, accuracy:  0.9393811289613165 0.6666666666666666\n",
      "220 번째 loss, accuracy:  0.9384550516498052 0.6666666666666666\n",
      "221 번째 loss, accuracy:  0.9373717233125833 0.6666666666666666\n",
      "222 번째 loss, accuracy:  0.9363956429051037 0.6666666666666666\n",
      "223 번째 loss, accuracy:  0.935351428292223 0.6666666666666666\n",
      "224 번째 loss, accuracy:  0.9346012703637082 0.6666666666666666\n",
      "225 번째 loss, accuracy:  0.9340208721662371 0.6666666666666666\n",
      "226 번째 loss, accuracy:  0.9330663293213556 0.6666666666666666\n",
      "227 번째 loss, accuracy:  0.9322242565530239 0.6666666666666666\n",
      "228 번째 loss, accuracy:  0.931539459489251 0.6666666666666666\n",
      "229 번째 loss, accuracy:  0.9309006467011945 0.6666666666666666\n",
      "230 번째 loss, accuracy:  0.9296985044452047 0.6666666666666666\n",
      "231 번째 loss, accuracy:  0.9288504445405962 0.6666666666666666\n",
      "232 번째 loss, accuracy:  0.9279494879055256 0.6666666666666666\n",
      "233 번째 loss, accuracy:  0.9270742567307573 0.6666666666666666\n",
      "234 번째 loss, accuracy:  0.9263621549807763 0.6666666666666666\n",
      "235 번째 loss, accuracy:  0.9253692680979538 0.6666666666666666\n",
      "236 번째 loss, accuracy:  0.9246094117814951 0.6666666666666666\n",
      "237 번째 loss, accuracy:  0.9237394443364991 0.675\n",
      "238 번째 loss, accuracy:  0.9226349933161215 0.675\n",
      "239 번째 loss, accuracy:  0.9214944208465649 0.675\n",
      "240 번째 loss, accuracy:  0.9206214281522904 0.675\n",
      "241 번째 loss, accuracy:  0.9198938192313968 0.675\n",
      "242 번째 loss, accuracy:  0.9190595631277739 0.675\n",
      "243 번째 loss, accuracy:  0.9179844847155689 0.675\n",
      "244 번째 loss, accuracy:  0.9169775927373542 0.675\n",
      "245 번째 loss, accuracy:  0.9159412576244772 0.675\n",
      "246 번째 loss, accuracy:  0.9151994658914046 0.675\n",
      "247 번째 loss, accuracy:  0.9141725291176478 0.675\n",
      "248 번째 loss, accuracy:  0.9130577549743085 0.675\n",
      "249 번째 loss, accuracy:  0.9122238829840297 0.675\n",
      "250 번째 loss, accuracy:  0.9112457156111315 0.675\n",
      "251 번째 loss, accuracy:  0.9104735715516744 0.675\n",
      "252 번째 loss, accuracy:  0.9096578071244836 0.675\n",
      "253 번째 loss, accuracy:  0.9088748184389009 0.675\n",
      "254 번째 loss, accuracy:  0.9077313920811778 0.675\n",
      "255 번째 loss, accuracy:  0.9066115664118163 0.675\n",
      "256 번째 loss, accuracy:  0.9056061854383198 0.675\n",
      "257 번째 loss, accuracy:  0.9048267255540964 0.675\n",
      "258 번째 loss, accuracy:  0.9040233466835591 0.675\n",
      "259 번째 loss, accuracy:  0.9028576422360148 0.675\n",
      "260 번째 loss, accuracy:  0.9015481809461485 0.675\n",
      "261 번째 loss, accuracy:  0.9005434075809604 0.675\n",
      "262 번째 loss, accuracy:  0.8994813171911289 0.675\n",
      "263 번째 loss, accuracy:  0.8983189142346507 0.675\n",
      "264 번째 loss, accuracy:  0.8974242651399887 0.675\n",
      "265 번째 loss, accuracy:  0.8965890471159906 0.675\n",
      "266 번째 loss, accuracy:  0.8954849585064358 0.675\n",
      "267 번째 loss, accuracy:  0.8945401783276703 0.675\n",
      "268 번째 loss, accuracy:  0.8935612030369954 0.675\n",
      "269 번째 loss, accuracy:  0.8926358358226724 0.675\n",
      "270 번째 loss, accuracy:  0.891451118144984 0.675\n",
      "271 번째 loss, accuracy:  0.8904528511939735 0.675\n",
      "272 번째 loss, accuracy:  0.8894566364941089 0.675\n",
      "273 번째 loss, accuracy:  0.8888054264208312 0.675\n",
      "274 번째 loss, accuracy:  0.8875900778475577 0.675\n",
      "275 번째 loss, accuracy:  0.886978238690094 0.675\n",
      "276 번째 loss, accuracy:  0.885948753939124 0.675\n",
      "277 번째 loss, accuracy:  0.8844532968955251 0.675\n",
      "278 번째 loss, accuracy:  0.8833963435568922 0.675\n",
      "279 번째 loss, accuracy:  0.8824259872260356 0.675\n",
      "280 번째 loss, accuracy:  0.8814870884149077 0.675\n",
      "281 번째 loss, accuracy:  0.8805016610588321 0.675\n",
      "282 번째 loss, accuracy:  0.8797727731284394 0.675\n",
      "283 번째 loss, accuracy:  0.8785680554996443 0.675\n",
      "284 번째 loss, accuracy:  0.8775945339575474 0.675\n",
      "285 번째 loss, accuracy:  0.8767667773835727 0.675\n",
      "286 번째 loss, accuracy:  0.8755291546773638 0.675\n",
      "287 번째 loss, accuracy:  0.8746861410600889 0.675\n",
      "288 번째 loss, accuracy:  0.8736203842913478 0.675\n",
      "289 번째 loss, accuracy:  0.8727904117137018 0.675\n",
      "290 번째 loss, accuracy:  0.8716167217537676 0.675\n",
      "291 번째 loss, accuracy:  0.8705004283917525 0.675\n",
      "292 번째 loss, accuracy:  0.8693146833398361 0.675\n",
      "293 번째 loss, accuracy:  0.8681180145766815 0.675\n",
      "294 번째 loss, accuracy:  0.8670155134481335 0.675\n",
      "295 번째 loss, accuracy:  0.8660633666720422 0.675\n",
      "296 번째 loss, accuracy:  0.864785026718392 0.675\n",
      "297 번째 loss, accuracy:  0.8638787154309334 0.675\n",
      "298 번째 loss, accuracy:  0.8628962790508429 0.675\n",
      "299 번째 loss, accuracy:  0.8614137607534815 0.675\n",
      "300 번째 loss, accuracy:  0.8603498793949492 0.675\n",
      "301 번째 loss, accuracy:  0.8591167887926672 0.675\n",
      "302 번째 loss, accuracy:  0.8580516836229187 0.675\n",
      "303 번째 loss, accuracy:  0.8569569688634431 0.675\n",
      "304 번째 loss, accuracy:  0.8560423865490737 0.675\n",
      "305 번째 loss, accuracy:  0.8550993704709481 0.675\n",
      "306 번째 loss, accuracy:  0.8539548736378222 0.675\n",
      "307 번째 loss, accuracy:  0.8528804601250949 0.675\n",
      "308 번째 loss, accuracy:  0.8517384621019263 0.675\n",
      "309 번째 loss, accuracy:  0.8506746425281061 0.675\n",
      "310 번째 loss, accuracy:  0.849221315603723 0.675\n",
      "311 번째 loss, accuracy:  0.8477632187755452 0.675\n",
      "312 번째 loss, accuracy:  0.8463616424706475 0.675\n",
      "313 번째 loss, accuracy:  0.8450485731782147 0.675\n",
      "314 번째 loss, accuracy:  0.843747596294643 0.675\n",
      "315 번째 loss, accuracy:  0.8429500220630275 0.675\n",
      "316 번째 loss, accuracy:  0.841725817803257 0.675\n",
      "317 번째 loss, accuracy:  0.840516332306382 0.675\n",
      "318 번째 loss, accuracy:  0.83970946600015 0.675\n",
      "319 번째 loss, accuracy:  0.8388784023911682 0.675\n",
      "320 번째 loss, accuracy:  0.8377674698887922 0.675\n",
      "321 번째 loss, accuracy:  0.8366774717763953 0.675\n",
      "322 번째 loss, accuracy:  0.8351417105070273 0.675\n",
      "323 번째 loss, accuracy:  0.8338944837253701 0.675\n",
      "324 번째 loss, accuracy:  0.8329744401344165 0.675\n",
      "325 번째 loss, accuracy:  0.8315068928244891 0.675\n",
      "326 번째 loss, accuracy:  0.830305732166502 0.675\n",
      "327 번째 loss, accuracy:  0.8292631859034725 0.675\n",
      "328 번째 loss, accuracy:  0.8277387515183351 0.675\n",
      "329 번째 loss, accuracy:  0.8263153225394397 0.675\n",
      "330 번째 loss, accuracy:  0.8250800514323141 0.675\n",
      "331 번째 loss, accuracy:  0.8238211391524286 0.675\n",
      "332 번째 loss, accuracy:  0.8226467145495892 0.675\n",
      "333 번째 loss, accuracy:  0.8209474271055391 0.675\n",
      "334 번째 loss, accuracy:  0.8196406142571877 0.675\n",
      "335 번째 loss, accuracy:  0.8184278518962597 0.675\n",
      "336 번째 loss, accuracy:  0.8170521533356182 0.675\n",
      "337 번째 loss, accuracy:  0.8154692713550039 0.675\n",
      "338 번째 loss, accuracy:  0.814306025455377 0.675\n",
      "339 번째 loss, accuracy:  0.8128358101781309 0.675\n",
      "340 번째 loss, accuracy:  0.8117332934008776 0.675\n",
      "341 번째 loss, accuracy:  0.8103391025589557 0.675\n",
      "342 번째 loss, accuracy:  0.8092193888202751 0.675\n",
      "343 번째 loss, accuracy:  0.8081067529417157 0.675\n",
      "344 번째 loss, accuracy:  0.8068835009657075 0.675\n",
      "345 번째 loss, accuracy:  0.8056713303576425 0.675\n",
      "346 번째 loss, accuracy:  0.8048511461812226 0.675\n",
      "347 번째 loss, accuracy:  0.8034375778522178 0.675\n",
      "348 번째 loss, accuracy:  0.8023694164195408 0.675\n",
      "349 번째 loss, accuracy:  0.8016033683379786 0.675\n",
      "350 번째 loss, accuracy:  0.8004034225234496 0.675\n",
      "351 번째 loss, accuracy:  0.7990690884165517 0.675\n",
      "352 번째 loss, accuracy:  0.7982062678656036 0.675\n",
      "353 번째 loss, accuracy:  0.7971695980786211 0.675\n",
      "354 번째 loss, accuracy:  0.796112919352696 0.675\n",
      "355 번째 loss, accuracy:  0.794891409071997 0.675\n",
      "356 번째 loss, accuracy:  0.7934830053583067 0.675\n",
      "357 번째 loss, accuracy:  0.7921816087528875 0.675\n",
      "358 번째 loss, accuracy:  0.790851886650609 0.675\n",
      "359 번째 loss, accuracy:  0.7898913151857861 0.675\n",
      "360 번째 loss, accuracy:  0.7887159357500688 0.675\n",
      "361 번째 loss, accuracy:  0.7874817043604281 0.675\n",
      "362 번째 loss, accuracy:  0.7865368059331423 0.675\n",
      "363 번째 loss, accuracy:  0.7855152032437931 0.675\n",
      "364 번째 loss, accuracy:  0.7843034338282648 0.675\n",
      "365 번째 loss, accuracy:  0.7832668240688224 0.675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366 번째 loss, accuracy:  0.7823056411276056 0.675\n",
      "367 번째 loss, accuracy:  0.7812208481057578 0.675\n",
      "368 번째 loss, accuracy:  0.7799767767634663 0.675\n",
      "369 번째 loss, accuracy:  0.7787282897675385 0.675\n",
      "370 번째 loss, accuracy:  0.777894295443263 0.675\n",
      "371 번째 loss, accuracy:  0.7772940739408821 0.675\n",
      "372 번째 loss, accuracy:  0.7761335975324227 0.675\n",
      "373 번째 loss, accuracy:  0.7751174897856553 0.675\n",
      "374 번째 loss, accuracy:  0.7739231444910086 0.675\n",
      "375 번째 loss, accuracy:  0.7731062716235464 0.675\n",
      "376 번째 loss, accuracy:  0.7720620769336528 0.675\n",
      "377 번째 loss, accuracy:  0.7713811524667974 0.675\n",
      "378 번째 loss, accuracy:  0.770416646625878 0.675\n",
      "379 번째 loss, accuracy:  0.7694673836539384 0.675\n",
      "380 번째 loss, accuracy:  0.7685340015046208 0.675\n",
      "381 번째 loss, accuracy:  0.7675026330773522 0.675\n",
      "382 번째 loss, accuracy:  0.7665293607722601 0.675\n",
      "383 번째 loss, accuracy:  0.7657424850075423 0.675\n",
      "384 번째 loss, accuracy:  0.7647423119696719 0.675\n",
      "385 번째 loss, accuracy:  0.763791160034193 0.675\n",
      "386 번째 loss, accuracy:  0.7630493507857127 0.675\n",
      "387 번째 loss, accuracy:  0.7619687638501785 0.675\n",
      "388 번째 loss, accuracy:  0.7609725616648331 0.675\n",
      "389 번째 loss, accuracy:  0.7598977362643434 0.675\n",
      "390 번째 loss, accuracy:  0.758857821808567 0.675\n",
      "391 번째 loss, accuracy:  0.7580443676668972 0.675\n",
      "392 번째 loss, accuracy:  0.757054392360904 0.675\n",
      "393 번째 loss, accuracy:  0.756265367822981 0.675\n",
      "394 번째 loss, accuracy:  0.7552343480381439 0.675\n",
      "395 번째 loss, accuracy:  0.7542667041025178 0.675\n",
      "396 번째 loss, accuracy:  0.7532791404522148 0.675\n",
      "397 번째 loss, accuracy:  0.7523692660462769 0.675\n",
      "398 번째 loss, accuracy:  0.7515342388427456 0.675\n",
      "399 번째 loss, accuracy:  0.750770695017155 0.675\n",
      "400 번째 loss, accuracy:  0.7498440116894588 0.675\n",
      "401 번째 loss, accuracy:  0.7491274675219032 0.675\n",
      "402 번째 loss, accuracy:  0.7481584839497469 0.675\n",
      "403 번째 loss, accuracy:  0.7472807188933107 0.675\n",
      "404 번째 loss, accuracy:  0.7464474189691804 0.675\n",
      "405 번째 loss, accuracy:  0.7455876223734054 0.675\n",
      "406 번째 loss, accuracy:  0.7445341968331043 0.675\n",
      "407 번째 loss, accuracy:  0.7439262768213456 0.675\n",
      "408 번째 loss, accuracy:  0.742927807626774 0.675\n",
      "409 번째 loss, accuracy:  0.7422318994205231 0.675\n",
      "410 번째 loss, accuracy:  0.7414226357244255 0.675\n",
      "411 번째 loss, accuracy:  0.7406637433655161 0.675\n",
      "412 번째 loss, accuracy:  0.7399325595589006 0.675\n",
      "413 번째 loss, accuracy:  0.7389738117842015 0.675\n",
      "414 번째 loss, accuracy:  0.7380060624076258 0.675\n",
      "415 번째 loss, accuracy:  0.7372969817114222 0.6833333333333333\n",
      "416 번째 loss, accuracy:  0.7366735122259546 0.6833333333333333\n",
      "417 번째 loss, accuracy:  0.7357140853788557 0.6833333333333333\n",
      "418 번째 loss, accuracy:  0.7348016948343551 0.6833333333333333\n",
      "419 번째 loss, accuracy:  0.7341643135080174 0.6833333333333333\n",
      "420 번째 loss, accuracy:  0.7334542676705273 0.6833333333333333\n",
      "421 번째 loss, accuracy:  0.7325710622931251 0.6833333333333333\n",
      "422 번째 loss, accuracy:  0.7317525997089743 0.6833333333333333\n",
      "423 번째 loss, accuracy:  0.730959239977615 0.6833333333333333\n",
      "424 번째 loss, accuracy:  0.7302368434634219 0.6833333333333333\n",
      "425 번째 loss, accuracy:  0.729562763314933 0.6833333333333333\n",
      "426 번째 loss, accuracy:  0.7285634434240708 0.6833333333333333\n",
      "427 번째 loss, accuracy:  0.7276967434460357 0.6833333333333333\n",
      "428 번째 loss, accuracy:  0.7268675824317593 0.6833333333333333\n",
      "429 번째 loss, accuracy:  0.7261841290694051 0.6833333333333333\n",
      "430 번째 loss, accuracy:  0.7254840297004591 0.6833333333333333\n",
      "431 번째 loss, accuracy:  0.7246978179992982 0.6833333333333333\n",
      "432 번째 loss, accuracy:  0.7238754345635918 0.6833333333333333\n",
      "433 번째 loss, accuracy:  0.7231480078554349 0.6833333333333333\n",
      "434 번째 loss, accuracy:  0.7225483880052477 0.6833333333333333\n",
      "435 번째 loss, accuracy:  0.7216908526391567 0.6833333333333333\n",
      "436 번째 loss, accuracy:  0.7209997163191822 0.6833333333333333\n",
      "437 번째 loss, accuracy:  0.7204047762490376 0.6833333333333333\n",
      "438 번째 loss, accuracy:  0.7197807648722848 0.6833333333333333\n",
      "439 번째 loss, accuracy:  0.7192612339097519 0.6833333333333333\n",
      "440 번째 loss, accuracy:  0.7185714512315313 0.6833333333333333\n",
      "441 번째 loss, accuracy:  0.7178188609247907 0.6833333333333333\n",
      "442 번째 loss, accuracy:  0.7172107924045725 0.6833333333333333\n",
      "443 번째 loss, accuracy:  0.716566767454575 0.6833333333333333\n",
      "444 번째 loss, accuracy:  0.7160185419149337 0.6833333333333333\n",
      "445 번째 loss, accuracy:  0.715399838846263 0.6833333333333333\n",
      "446 번째 loss, accuracy:  0.7147987641907193 0.6833333333333333\n",
      "447 번째 loss, accuracy:  0.7140983648245808 0.6833333333333333\n",
      "448 번째 loss, accuracy:  0.7135149305994778 0.6833333333333333\n",
      "449 번째 loss, accuracy:  0.7129674747107982 0.6833333333333333\n",
      "450 번째 loss, accuracy:  0.7121982469729706 0.6833333333333333\n",
      "451 번째 loss, accuracy:  0.7116262109490784 0.6833333333333333\n",
      "452 번째 loss, accuracy:  0.7110201882321202 0.675\n",
      "453 번째 loss, accuracy:  0.7104823300699471 0.675\n",
      "454 번째 loss, accuracy:  0.7096983555825083 0.675\n",
      "455 번째 loss, accuracy:  0.7093194781859334 0.675\n",
      "456 번째 loss, accuracy:  0.7087299940083297 0.675\n",
      "457 번째 loss, accuracy:  0.7081116538605073 0.675\n",
      "458 번째 loss, accuracy:  0.7072190418494614 0.675\n",
      "459 번째 loss, accuracy:  0.7065386292993185 0.675\n",
      "460 번째 loss, accuracy:  0.7058082326843894 0.6833333333333333\n",
      "461 번째 loss, accuracy:  0.7050227741231776 0.6833333333333333\n",
      "462 번째 loss, accuracy:  0.7043427772970429 0.6833333333333333\n",
      "463 번째 loss, accuracy:  0.7036503862005997 0.6833333333333333\n",
      "464 번째 loss, accuracy:  0.7031061432921438 0.6833333333333333\n",
      "465 번째 loss, accuracy:  0.7024195049121489 0.6833333333333333\n",
      "466 번째 loss, accuracy:  0.7018628917570169 0.6833333333333333\n",
      "467 번째 loss, accuracy:  0.7012989894610944 0.6833333333333333\n",
      "468 번째 loss, accuracy:  0.7006526271123664 0.6833333333333333\n",
      "469 번째 loss, accuracy:  0.7002298839960834 0.6833333333333333\n",
      "470 번째 loss, accuracy:  0.6996483155825993 0.6833333333333333\n",
      "471 번째 loss, accuracy:  0.6991526038683309 0.6833333333333333\n",
      "472 번째 loss, accuracy:  0.6985816961681789 0.6833333333333333\n",
      "473 번째 loss, accuracy:  0.6978590124742238 0.6833333333333333\n",
      "474 번째 loss, accuracy:  0.6971711549585446 0.6833333333333333\n",
      "475 번째 loss, accuracy:  0.6966768362210277 0.6833333333333333\n",
      "476 번째 loss, accuracy:  0.6960067680660781 0.6833333333333333\n",
      "477 번째 loss, accuracy:  0.6955502264713068 0.6833333333333333\n",
      "478 번째 loss, accuracy:  0.6949681853849817 0.6833333333333333\n",
      "479 번째 loss, accuracy:  0.6945067972269616 0.6833333333333333\n",
      "480 번째 loss, accuracy:  0.6939759578926523 0.6833333333333333\n",
      "481 번째 loss, accuracy:  0.6935872262433896 0.6833333333333333\n",
      "482 번째 loss, accuracy:  0.6928928893319013 0.6833333333333333\n",
      "483 번째 loss, accuracy:  0.692346914141659 0.6833333333333333\n",
      "484 번째 loss, accuracy:  0.6916498049967968 0.6833333333333333\n",
      "485 번째 loss, accuracy:  0.6909918483129291 0.6833333333333333\n",
      "486 번째 loss, accuracy:  0.690454872767487 0.6833333333333333\n",
      "487 번째 loss, accuracy:  0.6898349019732718 0.6833333333333333\n",
      "488 번째 loss, accuracy:  0.689237104129458 0.6833333333333333\n",
      "489 번째 loss, accuracy:  0.6886491156738297 0.6833333333333333\n",
      "490 번째 loss, accuracy:  0.6881510386817713 0.6833333333333333\n",
      "491 번째 loss, accuracy:  0.6876987866235483 0.6833333333333333\n",
      "492 번째 loss, accuracy:  0.6873213547720552 0.6833333333333333\n",
      "493 번째 loss, accuracy:  0.6868152855622561 0.6833333333333333\n",
      "494 번째 loss, accuracy:  0.6861549650552966 0.6833333333333333\n",
      "495 번째 loss, accuracy:  0.6855528670119174 0.6833333333333333\n",
      "496 번째 loss, accuracy:  0.6850766104256306 0.6833333333333333\n",
      "497 번째 loss, accuracy:  0.6843974467804838 0.6833333333333333\n",
      "498 번째 loss, accuracy:  0.6838873598272024 0.6833333333333333\n",
      "499 번째 loss, accuracy:  0.6834625981246637 0.6833333333333333\n",
      "500 번째 loss, accuracy:  0.682904550168095 0.6833333333333333\n",
      "501 번째 loss, accuracy:  0.6824207486941379 0.6833333333333333\n",
      "502 번째 loss, accuracy:  0.6818888450178275 0.6833333333333333\n",
      "503 번째 loss, accuracy:  0.681392922296763 0.6833333333333333\n",
      "504 번째 loss, accuracy:  0.6807537100629778 0.6833333333333333\n",
      "505 번째 loss, accuracy:  0.680254348221489 0.6833333333333333\n",
      "506 번째 loss, accuracy:  0.6797139402190205 0.6833333333333333\n",
      "507 번째 loss, accuracy:  0.6791342532796977 0.6833333333333333\n",
      "508 번째 loss, accuracy:  0.6784400545253846 0.6833333333333333\n",
      "509 번째 loss, accuracy:  0.6778778808953909 0.6916666666666667\n",
      "510 번째 loss, accuracy:  0.677345025638463 0.6916666666666667\n",
      "511 번째 loss, accuracy:  0.6768624193188237 0.6916666666666667\n",
      "512 번째 loss, accuracy:  0.6763849421026659 0.6916666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513 번째 loss, accuracy:  0.6758862412572191 0.6916666666666667\n",
      "514 번째 loss, accuracy:  0.6754608552298461 0.6916666666666667\n",
      "515 번째 loss, accuracy:  0.6749852843996479 0.6916666666666667\n",
      "516 번째 loss, accuracy:  0.6744660799836417 0.6916666666666667\n",
      "517 번째 loss, accuracy:  0.6739667789585586 0.6916666666666667\n",
      "518 번째 loss, accuracy:  0.6735246721969782 0.6916666666666667\n",
      "519 번째 loss, accuracy:  0.6731246354579521 0.6916666666666667\n",
      "520 번째 loss, accuracy:  0.6726315220966096 0.6916666666666667\n",
      "521 번째 loss, accuracy:  0.6721722455960228 0.6916666666666667\n",
      "522 번째 loss, accuracy:  0.6717433370720189 0.6916666666666667\n",
      "523 번째 loss, accuracy:  0.6712148837680054 0.6916666666666667\n",
      "524 번째 loss, accuracy:  0.6707566800306175 0.6916666666666667\n",
      "525 번째 loss, accuracy:  0.6702835606047695 0.6916666666666667\n",
      "526 번째 loss, accuracy:  0.6698335781540946 0.6916666666666667\n",
      "527 번째 loss, accuracy:  0.6693822672555171 0.6916666666666667\n",
      "528 번째 loss, accuracy:  0.6689019759983651 0.6916666666666667\n",
      "529 번째 loss, accuracy:  0.6684011775504501 0.6916666666666667\n",
      "530 번째 loss, accuracy:  0.6679376923349165 0.6916666666666667\n",
      "531 번째 loss, accuracy:  0.6674144912861284 0.6916666666666667\n",
      "532 번째 loss, accuracy:  0.6669868769586128 0.6916666666666667\n",
      "533 번째 loss, accuracy:  0.6666033937824648 0.6916666666666667\n",
      "534 번째 loss, accuracy:  0.6660972492303404 0.6916666666666667\n",
      "535 번째 loss, accuracy:  0.6655666927007566 0.6916666666666667\n",
      "536 번째 loss, accuracy:  0.6650557764066071 0.7\n",
      "537 번째 loss, accuracy:  0.6646340885366833 0.7\n",
      "538 번째 loss, accuracy:  0.6642077469277117 0.7\n",
      "539 번째 loss, accuracy:  0.6638472454179752 0.7\n",
      "540 번째 loss, accuracy:  0.663438027128289 0.6916666666666667\n",
      "541 번째 loss, accuracy:  0.6629622072067141 0.7\n",
      "542 번째 loss, accuracy:  0.6625382148725076 0.6916666666666667\n",
      "543 번째 loss, accuracy:  0.6620999243350937 0.7\n",
      "544 번째 loss, accuracy:  0.6617664214823891 0.7\n",
      "545 번째 loss, accuracy:  0.6612962531099238 0.7\n",
      "546 번째 loss, accuracy:  0.6608826540176589 0.7\n",
      "547 번째 loss, accuracy:  0.660519851399427 0.7\n",
      "548 번째 loss, accuracy:  0.6600800823761646 0.7\n",
      "549 번째 loss, accuracy:  0.6596193839931835 0.7\n",
      "550 번째 loss, accuracy:  0.6591875653999301 0.7\n",
      "551 번째 loss, accuracy:  0.6587805411323193 0.7\n",
      "552 번째 loss, accuracy:  0.6583266271129415 0.7\n",
      "553 번째 loss, accuracy:  0.6579208362187148 0.7\n",
      "554 번째 loss, accuracy:  0.657488406924104 0.7\n",
      "555 번째 loss, accuracy:  0.6571046371030161 0.7\n",
      "556 번째 loss, accuracy:  0.6566376341270188 0.7\n",
      "557 번째 loss, accuracy:  0.6562287446659183 0.7\n",
      "558 번째 loss, accuracy:  0.6557267544160704 0.7\n",
      "559 번째 loss, accuracy:  0.655307818030277 0.7\n",
      "560 번째 loss, accuracy:  0.6549048052560434 0.7\n",
      "561 번째 loss, accuracy:  0.6544589687759447 0.7\n",
      "562 번째 loss, accuracy:  0.6540277055535448 0.7\n",
      "563 번째 loss, accuracy:  0.6535919669596069 0.7\n",
      "564 번째 loss, accuracy:  0.653217327434285 0.7\n",
      "565 번째 loss, accuracy:  0.6527763469615178 0.7\n",
      "566 번째 loss, accuracy:  0.6523839355809958 0.7083333333333334\n",
      "567 번째 loss, accuracy:  0.6519464944065512 0.7\n",
      "568 번째 loss, accuracy:  0.6514839347458865 0.7\n",
      "569 번째 loss, accuracy:  0.6510492630320575 0.7\n",
      "570 번째 loss, accuracy:  0.6506661701768349 0.7\n",
      "571 번째 loss, accuracy:  0.650229283492964 0.7\n",
      "572 번째 loss, accuracy:  0.6498226879301988 0.7083333333333334\n",
      "573 번째 loss, accuracy:  0.6494564710022398 0.7083333333333334\n",
      "574 번째 loss, accuracy:  0.6490724442383483 0.7083333333333334\n",
      "575 번째 loss, accuracy:  0.648691823538191 0.7083333333333334\n",
      "576 번째 loss, accuracy:  0.648358033372629 0.7083333333333334\n",
      "577 번째 loss, accuracy:  0.647983190360321 0.7083333333333334\n",
      "578 번째 loss, accuracy:  0.6476237534139656 0.7083333333333334\n",
      "579 번째 loss, accuracy:  0.6472507776792852 0.7083333333333334\n",
      "580 번째 loss, accuracy:  0.6468717410691686 0.7166666666666667\n",
      "581 번째 loss, accuracy:  0.6465156318233167 0.7166666666666667\n",
      "582 번째 loss, accuracy:  0.6460771358185954 0.7166666666666667\n",
      "583 번째 loss, accuracy:  0.6456583392363505 0.7083333333333334\n",
      "584 번째 loss, accuracy:  0.6452841470573268 0.7166666666666667\n",
      "585 번째 loss, accuracy:  0.6448727940108537 0.7083333333333334\n",
      "586 번째 loss, accuracy:  0.6444785575508869 0.7083333333333334\n",
      "587 번째 loss, accuracy:  0.644053901171278 0.7083333333333334\n",
      "588 번째 loss, accuracy:  0.6437005546122937 0.7083333333333334\n",
      "589 번째 loss, accuracy:  0.643366513518035 0.7166666666666667\n",
      "590 번째 loss, accuracy:  0.6429933497907458 0.7083333333333334\n",
      "591 번째 loss, accuracy:  0.6426132588920381 0.7166666666666667\n",
      "592 번째 loss, accuracy:  0.6421891224868623 0.7166666666666667\n",
      "593 번째 loss, accuracy:  0.6418353129612091 0.7416666666666667\n",
      "594 번째 loss, accuracy:  0.6415126746598402 0.7583333333333333\n",
      "595 번째 loss, accuracy:  0.6411142617115364 0.775\n",
      "596 번째 loss, accuracy:  0.6407526507074988 0.775\n",
      "597 번째 loss, accuracy:  0.6403922799150777 0.8083333333333333\n",
      "598 번째 loss, accuracy:  0.6400484633001279 0.8083333333333333\n",
      "599 번째 loss, accuracy:  0.6396543295002066 0.7583333333333333\n",
      "600 번째 loss, accuracy:  0.6392627926585202 0.8\n",
      "601 번째 loss, accuracy:  0.6388810505739184 0.7916666666666666\n",
      "602 번째 loss, accuracy:  0.6385048347881678 0.7916666666666666\n",
      "603 번째 loss, accuracy:  0.638136913934705 0.7916666666666666\n",
      "604 번째 loss, accuracy:  0.6377554819222432 0.775\n",
      "605 번째 loss, accuracy:  0.637329665332261 0.7666666666666667\n",
      "606 번째 loss, accuracy:  0.6369391178076531 0.75\n",
      "607 번째 loss, accuracy:  0.6366067319841497 0.7333333333333333\n",
      "608 번째 loss, accuracy:  0.6362781093757265 0.75\n",
      "609 번째 loss, accuracy:  0.6359121878238023 0.7166666666666667\n",
      "610 번째 loss, accuracy:  0.6355148746765555 0.7166666666666667\n",
      "611 번째 loss, accuracy:  0.635211387517489 0.7416666666666667\n",
      "612 번째 loss, accuracy:  0.6348288187961045 0.7333333333333333\n",
      "613 번째 loss, accuracy:  0.6344825808494069 0.725\n",
      "614 번째 loss, accuracy:  0.6341177651542714 0.7166666666666667\n",
      "615 번째 loss, accuracy:  0.63380549410029 0.7166666666666667\n",
      "616 번째 loss, accuracy:  0.6334574291545434 0.7083333333333334\n",
      "617 번째 loss, accuracy:  0.6331205923830713 0.7166666666666667\n",
      "618 번째 loss, accuracy:  0.6328108532168859 0.7166666666666667\n",
      "619 번째 loss, accuracy:  0.6324662193244726 0.7333333333333333\n",
      "620 번째 loss, accuracy:  0.6321424601566072 0.7333333333333333\n",
      "621 번째 loss, accuracy:  0.631752019840652 0.7166666666666667\n",
      "622 번째 loss, accuracy:  0.6314198708356978 0.7166666666666667\n",
      "623 번째 loss, accuracy:  0.6311302429924411 0.7166666666666667\n",
      "624 번째 loss, accuracy:  0.6307946743902914 0.75\n",
      "625 번째 loss, accuracy:  0.6304351875084089 0.7416666666666667\n",
      "626 번째 loss, accuracy:  0.6300925642464413 0.7333333333333333\n",
      "627 번째 loss, accuracy:  0.6297523506224374 0.7583333333333333\n",
      "628 번째 loss, accuracy:  0.6294518092467157 0.7583333333333333\n",
      "629 번째 loss, accuracy:  0.6291645728149963 0.775\n",
      "630 번째 loss, accuracy:  0.6288127354031855 0.7916666666666666\n",
      "631 번째 loss, accuracy:  0.6284635124513543 0.7583333333333333\n",
      "632 번째 loss, accuracy:  0.6281125351666257 0.7583333333333333\n",
      "633 번째 loss, accuracy:  0.6277638049318969 0.75\n",
      "634 번째 loss, accuracy:  0.6274347938241316 0.775\n",
      "635 번째 loss, accuracy:  0.6271232452908244 0.775\n",
      "636 번째 loss, accuracy:  0.6268084901256636 0.8\n",
      "637 번째 loss, accuracy:  0.6264317899017052 0.75\n",
      "638 번째 loss, accuracy:  0.6261174240310259 0.7416666666666667\n",
      "639 번째 loss, accuracy:  0.6257876265619475 0.75\n",
      "640 번째 loss, accuracy:  0.6254581796862065 0.7583333333333333\n",
      "641 번째 loss, accuracy:  0.6251426605623966 0.775\n",
      "642 번째 loss, accuracy:  0.6248292178686508 0.775\n",
      "643 번째 loss, accuracy:  0.6245343814679991 0.775\n",
      "644 번째 loss, accuracy:  0.6242114803631333 0.8\n",
      "645 번째 loss, accuracy:  0.6239455068740443 0.8333333333333334\n",
      "646 번째 loss, accuracy:  0.6236405750921453 0.8333333333333334\n",
      "647 번째 loss, accuracy:  0.6233197806100965 0.8333333333333334\n",
      "648 번째 loss, accuracy:  0.623050991967286 0.8416666666666667\n",
      "649 번째 loss, accuracy:  0.6227581248480265 0.8416666666666667\n",
      "650 번째 loss, accuracy:  0.6224007996766959 0.8416666666666667\n",
      "651 번째 loss, accuracy:  0.6220456670363887 0.8333333333333334\n",
      "652 번째 loss, accuracy:  0.6217804452551725 0.8416666666666667\n",
      "653 번째 loss, accuracy:  0.6214302392447543 0.8333333333333334\n",
      "654 번째 loss, accuracy:  0.6211335161461918 0.8333333333333334\n",
      "655 번째 loss, accuracy:  0.6207495110220892 0.8\n",
      "656 번째 loss, accuracy:  0.6204494301793936 0.7583333333333333\n",
      "657 번째 loss, accuracy:  0.6201736405180486 0.8166666666666667\n",
      "658 번째 loss, accuracy:  0.6199089914116815 0.825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659 번째 loss, accuracy:  0.6196373515155617 0.8333333333333334\n",
      "660 번째 loss, accuracy:  0.619386275116664 0.8333333333333334\n",
      "661 번째 loss, accuracy:  0.6190305497421136 0.7833333333333333\n",
      "662 번째 loss, accuracy:  0.6186750095509849 0.7833333333333333\n",
      "663 번째 loss, accuracy:  0.618340210832327 0.775\n",
      "664 번째 loss, accuracy:  0.6180268186149245 0.7416666666666667\n",
      "665 번째 loss, accuracy:  0.617748616499744 0.7583333333333333\n",
      "666 번째 loss, accuracy:  0.6174393836702853 0.7416666666666667\n",
      "667 번째 loss, accuracy:  0.6171023830080531 0.7583333333333333\n",
      "668 번째 loss, accuracy:  0.6168100140767457 0.7583333333333333\n",
      "669 번째 loss, accuracy:  0.6164745020646988 0.7166666666666667\n",
      "670 번째 loss, accuracy:  0.6161885776920539 0.7166666666666667\n",
      "671 번째 loss, accuracy:  0.6158552737805738 0.7083333333333334\n",
      "672 번째 loss, accuracy:  0.6155659302030166 0.7083333333333334\n",
      "673 번째 loss, accuracy:  0.6152383106192854 0.7166666666666667\n",
      "674 번째 loss, accuracy:  0.6149538821468528 0.7083333333333334\n",
      "675 번째 loss, accuracy:  0.6146337346289548 0.7083333333333334\n",
      "676 번째 loss, accuracy:  0.6143552739019836 0.7083333333333334\n",
      "677 번째 loss, accuracy:  0.6140415290148119 0.7\n",
      "678 번째 loss, accuracy:  0.613775683789049 0.7083333333333334\n",
      "679 번째 loss, accuracy:  0.6134639202014767 0.7083333333333334\n",
      "680 번째 loss, accuracy:  0.6131510620499223 0.7083333333333334\n",
      "681 번째 loss, accuracy:  0.6128574323678413 0.7166666666666667\n",
      "682 번째 loss, accuracy:  0.6125844174512051 0.7166666666666667\n",
      "683 번째 loss, accuracy:  0.6123086054602805 0.725\n",
      "684 번째 loss, accuracy:  0.6120288126214376 0.75\n",
      "685 번째 loss, accuracy:  0.6117326885037584 0.7583333333333333\n",
      "686 번째 loss, accuracy:  0.6114336444606369 0.775\n",
      "687 번째 loss, accuracy:  0.6111542043451245 0.775\n",
      "688 번째 loss, accuracy:  0.6108350226858096 0.7416666666666667\n",
      "689 번째 loss, accuracy:  0.6105478133996679 0.7416666666666667\n",
      "690 번째 loss, accuracy:  0.610256553885305 0.75\n",
      "691 번째 loss, accuracy:  0.6099623893206496 0.725\n",
      "692 번째 loss, accuracy:  0.6096936309256777 0.75\n",
      "693 번째 loss, accuracy:  0.6094074634272535 0.7583333333333333\n",
      "694 번째 loss, accuracy:  0.6091361379010726 0.725\n",
      "695 번째 loss, accuracy:  0.6088344741467767 0.725\n",
      "696 번째 loss, accuracy:  0.6085490581989668 0.7416666666666667\n",
      "697 번째 loss, accuracy:  0.6082560609977766 0.7416666666666667\n",
      "698 번째 loss, accuracy:  0.6079805156005029 0.7166666666666667\n",
      "699 번째 loss, accuracy:  0.6077073915578228 0.7166666666666667\n",
      "700 번째 loss, accuracy:  0.6073997349925506 0.7083333333333334\n",
      "701 번째 loss, accuracy:  0.607128665507684 0.7083333333333334\n",
      "702 번째 loss, accuracy:  0.6068558670960739 0.7083333333333334\n",
      "703 번째 loss, accuracy:  0.6066233237769787 0.7083333333333334\n",
      "704 번째 loss, accuracy:  0.606327116299772 0.7083333333333334\n",
      "705 번째 loss, accuracy:  0.6060324809446006 0.7083333333333334\n",
      "706 번째 loss, accuracy:  0.6058066507276015 0.7083333333333334\n",
      "707 번째 loss, accuracy:  0.605555595723069 0.7083333333333334\n",
      "708 번째 loss, accuracy:  0.605346542489686 0.7\n",
      "709 번째 loss, accuracy:  0.6051172615510265 0.7\n",
      "710 번째 loss, accuracy:  0.6048808323368451 0.7\n",
      "711 번째 loss, accuracy:  0.6045719207473802 0.7\n",
      "712 번째 loss, accuracy:  0.6043120027068352 0.7\n",
      "713 번째 loss, accuracy:  0.6040369487517221 0.7\n",
      "714 번째 loss, accuracy:  0.6037784973432856 0.7\n",
      "715 번째 loss, accuracy:  0.6035151621887989 0.7\n",
      "716 번째 loss, accuracy:  0.6032579721757879 0.7\n",
      "717 번째 loss, accuracy:  0.6030101289720754 0.7\n",
      "718 번째 loss, accuracy:  0.6027411918158069 0.7\n",
      "719 번째 loss, accuracy:  0.602462752091568 0.7\n",
      "720 번째 loss, accuracy:  0.6021617130337517 0.7\n",
      "721 번째 loss, accuracy:  0.6019366680502157 0.7\n",
      "722 번째 loss, accuracy:  0.6016662799536215 0.7\n",
      "723 번째 loss, accuracy:  0.601448553810131 0.7\n",
      "724 번째 loss, accuracy:  0.6011810465968556 0.7\n",
      "725 번째 loss, accuracy:  0.6009543217019071 0.7\n",
      "726 번째 loss, accuracy:  0.6006391746620476 0.7\n",
      "727 번째 loss, accuracy:  0.6003539620277436 0.7\n",
      "728 번째 loss, accuracy:  0.6000836885152825 0.7\n",
      "729 번째 loss, accuracy:  0.5997846844127681 0.7083333333333334\n",
      "730 번째 loss, accuracy:  0.5995429649383752 0.7\n",
      "731 번째 loss, accuracy:  0.5992808728766855 0.7\n",
      "732 번째 loss, accuracy:  0.5990351241773797 0.7\n",
      "733 번째 loss, accuracy:  0.5988059948792376 0.7\n",
      "734 번째 loss, accuracy:  0.5986018454631217 0.7\n",
      "735 번째 loss, accuracy:  0.5983048900328719 0.7083333333333334\n",
      "736 번째 loss, accuracy:  0.5980489709002698 0.7083333333333334\n",
      "737 번째 loss, accuracy:  0.5978060186852562 0.7083333333333334\n",
      "738 번째 loss, accuracy:  0.5975408601307213 0.7083333333333334\n",
      "739 번째 loss, accuracy:  0.5972741747971928 0.7083333333333334\n",
      "740 번째 loss, accuracy:  0.5970157854173372 0.7083333333333334\n",
      "741 번째 loss, accuracy:  0.5967537421959401 0.7166666666666667\n",
      "742 번째 loss, accuracy:  0.5965030447272253 0.7166666666666667\n",
      "743 번째 loss, accuracy:  0.5962862607125072 0.7083333333333334\n",
      "744 번째 loss, accuracy:  0.5960328626635663 0.7083333333333334\n",
      "745 번째 loss, accuracy:  0.5957554862253323 0.7083333333333334\n",
      "746 번째 loss, accuracy:  0.5954963721795375 0.7166666666666667\n",
      "747 번째 loss, accuracy:  0.5952617412287022 0.7166666666666667\n",
      "748 번째 loss, accuracy:  0.59502758274844 0.7333333333333333\n",
      "749 번째 loss, accuracy:  0.5947587059914239 0.7333333333333333\n",
      "750 번째 loss, accuracy:  0.5945107800824446 0.7333333333333333\n",
      "751 번째 loss, accuracy:  0.5942343237284222 0.7583333333333333\n",
      "752 번째 loss, accuracy:  0.5939645721896105 0.7166666666666667\n",
      "753 번째 loss, accuracy:  0.5937119988476717 0.7166666666666667\n",
      "754 번째 loss, accuracy:  0.593482157748059 0.7083333333333334\n",
      "755 번째 loss, accuracy:  0.593276663635079 0.7083333333333334\n",
      "756 번째 loss, accuracy:  0.5930320665061056 0.7\n",
      "757 번째 loss, accuracy:  0.5927762660895801 0.7\n",
      "758 번째 loss, accuracy:  0.5925345093008212 0.7083333333333334\n",
      "759 번째 loss, accuracy:  0.5922870929702928 0.7083333333333334\n",
      "760 번째 loss, accuracy:  0.5920694220085617 0.7083333333333334\n",
      "761 번째 loss, accuracy:  0.5917980989866639 0.7083333333333334\n",
      "762 번째 loss, accuracy:  0.5915490182534587 0.7083333333333334\n",
      "763 번째 loss, accuracy:  0.5913039226452544 0.7083333333333334\n",
      "764 번째 loss, accuracy:  0.5910526177977712 0.7083333333333334\n",
      "765 번째 loss, accuracy:  0.5908014941301424 0.7083333333333334\n",
      "766 번째 loss, accuracy:  0.5905794238476694 0.7083333333333334\n",
      "767 번째 loss, accuracy:  0.5903065231489604 0.7083333333333334\n",
      "768 번째 loss, accuracy:  0.5900630511721509 0.7166666666666667\n",
      "769 번째 loss, accuracy:  0.5898179687243886 0.7166666666666667\n",
      "770 번째 loss, accuracy:  0.5895790411030729 0.7166666666666667\n",
      "771 번째 loss, accuracy:  0.5893376284761961 0.7333333333333333\n",
      "772 번째 loss, accuracy:  0.5890910546952104 0.7166666666666667\n",
      "773 번째 loss, accuracy:  0.5888402829283522 0.7333333333333333\n",
      "774 번째 loss, accuracy:  0.5886126521408983 0.7166666666666667\n",
      "775 번째 loss, accuracy:  0.5883944263961473 0.7166666666666667\n",
      "776 번째 loss, accuracy:  0.5881457858342467 0.7166666666666667\n",
      "777 번째 loss, accuracy:  0.5879135013816423 0.7166666666666667\n",
      "778 번째 loss, accuracy:  0.587683943368171 0.7166666666666667\n",
      "779 번째 loss, accuracy:  0.5874551203386278 0.7583333333333333\n",
      "780 번째 loss, accuracy:  0.5872246492115291 0.7833333333333333\n",
      "781 번째 loss, accuracy:  0.5869838391134652 0.7333333333333333\n",
      "782 번째 loss, accuracy:  0.586775843245044 0.7166666666666667\n",
      "783 번째 loss, accuracy:  0.5865712419227899 0.7166666666666667\n",
      "784 번째 loss, accuracy:  0.586323963502251 0.7166666666666667\n",
      "785 번째 loss, accuracy:  0.5861096372133354 0.7166666666666667\n",
      "786 번째 loss, accuracy:  0.5858948003575956 0.7166666666666667\n",
      "787 번째 loss, accuracy:  0.5856625623130521 0.7166666666666667\n",
      "788 번째 loss, accuracy:  0.5854278367302862 0.7166666666666667\n",
      "789 번째 loss, accuracy:  0.585207923160175 0.7333333333333333\n",
      "790 번째 loss, accuracy:  0.5849791421265207 0.75\n",
      "791 번째 loss, accuracy:  0.5847510709930468 0.7583333333333333\n",
      "792 번째 loss, accuracy:  0.5845283928204992 0.75\n",
      "793 번째 loss, accuracy:  0.5843201704299876 0.775\n",
      "794 번째 loss, accuracy:  0.5840863510394451 0.7583333333333333\n",
      "795 번째 loss, accuracy:  0.583880329242963 0.75\n",
      "796 번째 loss, accuracy:  0.5836627212454583 0.7583333333333333\n",
      "797 번째 loss, accuracy:  0.5834445568160395 0.7583333333333333\n",
      "798 번째 loss, accuracy:  0.5832358816357404 0.7416666666666667\n",
      "799 번째 loss, accuracy:  0.5830060553289559 0.75\n",
      "800 번째 loss, accuracy:  0.5827862166228883 0.75\n",
      "801 번째 loss, accuracy:  0.5825676597715362 0.7333333333333333\n",
      "802 번째 loss, accuracy:  0.58236293480219 0.7166666666666667\n",
      "803 번째 loss, accuracy:  0.5821489615070583 0.7166666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804 번째 loss, accuracy:  0.5819114356336402 0.7166666666666667\n",
      "805 번째 loss, accuracy:  0.5816940600026194 0.7333333333333333\n",
      "806 번째 loss, accuracy:  0.5814880598433803 0.7333333333333333\n",
      "807 번째 loss, accuracy:  0.5812639900682144 0.725\n",
      "808 번째 loss, accuracy:  0.581060992355992 0.7166666666666667\n",
      "809 번째 loss, accuracy:  0.5808481953170049 0.7333333333333333\n",
      "810 번째 loss, accuracy:  0.5806610749523453 0.7166666666666667\n",
      "811 번째 loss, accuracy:  0.5804429161655883 0.7416666666666667\n",
      "812 번째 loss, accuracy:  0.5802450498298677 0.7666666666666667\n",
      "813 번째 loss, accuracy:  0.5800396010384846 0.7333333333333333\n",
      "814 번째 loss, accuracy:  0.5798278202792351 0.7583333333333333\n",
      "815 번째 loss, accuracy:  0.5796104208153597 0.7583333333333333\n",
      "816 번째 loss, accuracy:  0.5793854396898408 0.775\n",
      "817 번째 loss, accuracy:  0.5791688582021841 0.775\n",
      "818 번째 loss, accuracy:  0.5789509940287002 0.7666666666666667\n",
      "819 번째 loss, accuracy:  0.5787409243059185 0.775\n",
      "820 번째 loss, accuracy:  0.5785228102862142 0.775\n",
      "821 번째 loss, accuracy:  0.578309852690213 0.775\n",
      "822 번째 loss, accuracy:  0.5781193533853841 0.7583333333333333\n",
      "823 번째 loss, accuracy:  0.5779136968234849 0.75\n",
      "824 번째 loss, accuracy:  0.5777082482733257 0.7333333333333333\n",
      "825 번째 loss, accuracy:  0.577517597795516 0.7166666666666667\n",
      "826 번째 loss, accuracy:  0.5773236420869272 0.7166666666666667\n",
      "827 번째 loss, accuracy:  0.577133879066863 0.7166666666666667\n",
      "828 번째 loss, accuracy:  0.5769230771278772 0.7166666666666667\n",
      "829 번째 loss, accuracy:  0.5767222672325937 0.7166666666666667\n",
      "830 번째 loss, accuracy:  0.5765059490538278 0.725\n",
      "831 번째 loss, accuracy:  0.5763119240011236 0.7333333333333333\n",
      "832 번째 loss, accuracy:  0.5761019904902078 0.7416666666666667\n",
      "833 번째 loss, accuracy:  0.5758901913802287 0.7416666666666667\n",
      "834 번째 loss, accuracy:  0.5757049037230428 0.725\n",
      "835 번째 loss, accuracy:  0.5754993179421355 0.7333333333333333\n",
      "836 번째 loss, accuracy:  0.5753001503391089 0.7416666666666667\n",
      "837 번째 loss, accuracy:  0.5751171630714104 0.7166666666666667\n",
      "838 번째 loss, accuracy:  0.5749502270497098 0.7166666666666667\n",
      "839 번째 loss, accuracy:  0.5747182088483128 0.7166666666666667\n",
      "840 번째 loss, accuracy:  0.5745355948440564 0.7166666666666667\n",
      "841 번째 loss, accuracy:  0.5743277899883498 0.7166666666666667\n",
      "842 번째 loss, accuracy:  0.5741241694028499 0.7166666666666667\n",
      "843 번째 loss, accuracy:  0.5739058223175497 0.725\n",
      "844 번째 loss, accuracy:  0.5737294894975526 0.7166666666666667\n",
      "845 번째 loss, accuracy:  0.5735334386907367 0.7333333333333333\n",
      "846 번째 loss, accuracy:  0.57334931963445 0.7166666666666667\n",
      "847 번째 loss, accuracy:  0.5731815187013792 0.7166666666666667\n",
      "848 번째 loss, accuracy:  0.5729795346403466 0.7166666666666667\n",
      "849 번째 loss, accuracy:  0.5727541543895527 0.7166666666666667\n",
      "850 번째 loss, accuracy:  0.5725575405722512 0.7166666666666667\n",
      "851 번째 loss, accuracy:  0.5723577968317208 0.7166666666666667\n",
      "852 번째 loss, accuracy:  0.572193564360871 0.7166666666666667\n",
      "853 번째 loss, accuracy:  0.5719910808821556 0.7166666666666667\n",
      "854 번째 loss, accuracy:  0.5717762327166682 0.7166666666666667\n",
      "855 번째 loss, accuracy:  0.571581616375794 0.7333333333333333\n",
      "856 번째 loss, accuracy:  0.5713900238251739 0.725\n",
      "857 번째 loss, accuracy:  0.5712030440598374 0.7166666666666667\n",
      "858 번째 loss, accuracy:  0.5710023935813252 0.7333333333333333\n",
      "859 번째 loss, accuracy:  0.570808289839244 0.7333333333333333\n",
      "860 번째 loss, accuracy:  0.570617915696629 0.725\n",
      "861 번째 loss, accuracy:  0.5704409518624177 0.7166666666666667\n",
      "862 번째 loss, accuracy:  0.5703077486111385 0.7083333333333334\n",
      "863 번째 loss, accuracy:  0.5701022703393815 0.7166666666666667\n",
      "864 번째 loss, accuracy:  0.569891956961754 0.7166666666666667\n",
      "865 번째 loss, accuracy:  0.5697085200890964 0.7166666666666667\n",
      "866 번째 loss, accuracy:  0.5695071879334178 0.7166666666666667\n",
      "867 번째 loss, accuracy:  0.5692844656265218 0.7166666666666667\n",
      "868 번째 loss, accuracy:  0.569098284141927 0.7166666666666667\n",
      "869 번째 loss, accuracy:  0.5688950307145949 0.7416666666666667\n",
      "870 번째 loss, accuracy:  0.5687173192547983 0.7416666666666667\n",
      "871 번째 loss, accuracy:  0.5685099088291665 0.7583333333333333\n",
      "872 번째 loss, accuracy:  0.5683165353881278 0.775\n",
      "873 번째 loss, accuracy:  0.5681427297452321 0.8\n",
      "874 번째 loss, accuracy:  0.5679720960478515 0.8333333333333334\n",
      "875 번째 loss, accuracy:  0.5677766727314096 0.8333333333333334\n",
      "876 번째 loss, accuracy:  0.5675897954820973 0.8333333333333334\n",
      "877 번째 loss, accuracy:  0.5674149748365646 0.8333333333333334\n",
      "878 번째 loss, accuracy:  0.5672099060767866 0.8333333333333334\n",
      "879 번째 loss, accuracy:  0.5670514414631511 0.8416666666666667\n",
      "880 번째 loss, accuracy:  0.5668767119815529 0.8416666666666667\n",
      "881 번째 loss, accuracy:  0.5666885151116765 0.8333333333333334\n",
      "882 번째 loss, accuracy:  0.5664909844015746 0.8166666666666667\n",
      "883 번째 loss, accuracy:  0.5663328476669004 0.8333333333333334\n",
      "884 번째 loss, accuracy:  0.5661643747765849 0.8166666666666667\n",
      "885 번째 loss, accuracy:  0.5659834789253134 0.825\n",
      "886 번째 loss, accuracy:  0.5658275362216645 0.825\n",
      "887 번째 loss, accuracy:  0.5656505123101018 0.8\n",
      "888 번째 loss, accuracy:  0.5654738008928227 0.8\n",
      "889 번째 loss, accuracy:  0.5652917849169842 0.775\n",
      "890 번째 loss, accuracy:  0.5651106963232074 0.775\n",
      "891 번째 loss, accuracy:  0.5649309052141233 0.8\n",
      "892 번째 loss, accuracy:  0.5647625221119452 0.8166666666666667\n",
      "893 번째 loss, accuracy:  0.5645891071112682 0.825\n",
      "894 번째 loss, accuracy:  0.5644127467550766 0.8333333333333334\n",
      "895 번째 loss, accuracy:  0.5642539706129467 0.8333333333333334\n",
      "896 번째 loss, accuracy:  0.5640761495896797 0.8416666666666667\n",
      "897 번째 loss, accuracy:  0.5638860601196714 0.8416666666666667\n",
      "898 번째 loss, accuracy:  0.563712870437448 0.8416666666666667\n",
      "899 번째 loss, accuracy:  0.5635521634325978 0.8416666666666667\n",
      "900 번째 loss, accuracy:  0.5633912204048716 0.85\n",
      "901 번째 loss, accuracy:  0.5632249099673196 0.8666666666666667\n",
      "902 번째 loss, accuracy:  0.5631032842465693 0.8833333333333333\n",
      "903 번째 loss, accuracy:  0.5629533072234778 0.8833333333333333\n",
      "904 번째 loss, accuracy:  0.562735845191474 0.8666666666666667\n",
      "905 번째 loss, accuracy:  0.5625625144343201 0.8583333333333333\n",
      "906 번째 loss, accuracy:  0.562418453982455 0.875\n",
      "907 번째 loss, accuracy:  0.5622650682346756 0.8833333333333333\n",
      "908 번째 loss, accuracy:  0.5620915321646726 0.8833333333333333\n",
      "909 번째 loss, accuracy:  0.5618946140584122 0.875\n",
      "910 번째 loss, accuracy:  0.5616990626360351 0.875\n",
      "911 번째 loss, accuracy:  0.5615789367900313 0.8833333333333333\n",
      "912 번째 loss, accuracy:  0.5614355902885396 0.9\n",
      "913 번째 loss, accuracy:  0.5612607780751695 0.9\n",
      "914 번째 loss, accuracy:  0.5610947397055346 0.9\n",
      "915 번째 loss, accuracy:  0.5609145316748569 0.9\n",
      "916 번째 loss, accuracy:  0.5607060441520443 0.8833333333333333\n",
      "917 번째 loss, accuracy:  0.5606048801410232 0.9083333333333333\n",
      "918 번째 loss, accuracy:  0.5604676115437836 0.925\n",
      "919 번째 loss, accuracy:  0.5602540200090135 0.9166666666666666\n",
      "920 번째 loss, accuracy:  0.5600939356145989 0.9166666666666666\n",
      "921 번째 loss, accuracy:  0.5599608354819244 0.925\n",
      "922 번째 loss, accuracy:  0.5597331775481611 0.9166666666666666\n",
      "923 번째 loss, accuracy:  0.5595915925131518 0.9166666666666666\n",
      "924 번째 loss, accuracy:  0.559458571058062 0.9333333333333333\n",
      "925 번째 loss, accuracy:  0.5592820436931727 0.925\n",
      "926 번째 loss, accuracy:  0.5590842598538249 0.9166666666666666\n",
      "927 번째 loss, accuracy:  0.5589367488322536 0.9166666666666666\n",
      "928 번째 loss, accuracy:  0.5588241239455497 0.9416666666666667\n",
      "929 번째 loss, accuracy:  0.5586053989857018 0.925\n",
      "930 번째 loss, accuracy:  0.5584590405519646 0.9333333333333333\n",
      "931 번째 loss, accuracy:  0.5582638630049993 0.9166666666666666\n",
      "932 번째 loss, accuracy:  0.5580411460526818 0.9\n",
      "933 번째 loss, accuracy:  0.5578841600720279 0.9\n",
      "934 번째 loss, accuracy:  0.5577295348374591 0.9\n",
      "935 번째 loss, accuracy:  0.5576124867954079 0.925\n",
      "936 번째 loss, accuracy:  0.5574636629679516 0.925\n",
      "937 번째 loss, accuracy:  0.5572777789529488 0.925\n",
      "938 번째 loss, accuracy:  0.5571256366161363 0.925\n",
      "939 번째 loss, accuracy:  0.5569010897699322 0.9\n",
      "940 번째 loss, accuracy:  0.5567186902633097 0.9\n",
      "941 번째 loss, accuracy:  0.5565242001188604 0.8833333333333333\n",
      "942 번째 loss, accuracy:  0.5563684511965515 0.9\n",
      "943 번째 loss, accuracy:  0.556226463788019 0.9166666666666666\n",
      "944 번째 loss, accuracy:  0.5560878088391035 0.925\n",
      "945 번째 loss, accuracy:  0.5559286897944731 0.925\n",
      "946 번째 loss, accuracy:  0.5557433335229838 0.9166666666666666\n",
      "947 번째 loss, accuracy:  0.5556402132733869 0.9416666666666667\n",
      "948 번째 loss, accuracy:  0.5554614337753843 0.9333333333333333\n",
      "949 번째 loss, accuracy:  0.5552855942477708 0.9333333333333333\n",
      "950 번째 loss, accuracy:  0.5550904144002209 0.9166666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951 번째 loss, accuracy:  0.5548889150171116 0.9\n",
      "952 번째 loss, accuracy:  0.5547015609040254 0.8833333333333333\n",
      "953 번째 loss, accuracy:  0.5545409500883236 0.8833333333333333\n",
      "954 번째 loss, accuracy:  0.5544268529269468 0.9083333333333333\n",
      "955 번째 loss, accuracy:  0.5542604496505412 0.9\n",
      "956 번째 loss, accuracy:  0.5541227979084782 0.9\n",
      "957 번째 loss, accuracy:  0.55404890605428 0.9416666666666667\n",
      "958 번째 loss, accuracy:  0.5538703391468706 0.9333333333333333\n",
      "959 번째 loss, accuracy:  0.5536882094465795 0.9166666666666666\n",
      "960 번째 loss, accuracy:  0.5535534965215132 0.9166666666666666\n",
      "961 번째 loss, accuracy:  0.5534068192102745 0.9166666666666666\n",
      "962 번째 loss, accuracy:  0.5532550217044938 0.925\n",
      "963 번째 loss, accuracy:  0.5531896939484032 0.95\n",
      "964 번째 loss, accuracy:  0.552985398412145 0.9416666666666667\n",
      "965 번째 loss, accuracy:  0.5528275666578284 0.9416666666666667\n",
      "966 번째 loss, accuracy:  0.5526962999591672 0.9416666666666667\n",
      "967 번째 loss, accuracy:  0.5525441159734721 0.9416666666666667\n",
      "968 번째 loss, accuracy:  0.5523307144325105 0.9166666666666666\n",
      "969 번째 loss, accuracy:  0.5522434772259089 0.9416666666666667\n",
      "970 번째 loss, accuracy:  0.5520382564702717 0.9166666666666666\n",
      "971 번째 loss, accuracy:  0.5518720673241292 0.9166666666666666\n",
      "972 번째 loss, accuracy:  0.5517254920681895 0.925\n",
      "973 번째 loss, accuracy:  0.5515294262586043 0.9\n",
      "974 번째 loss, accuracy:  0.5513342177233831 0.8833333333333333\n",
      "975 번째 loss, accuracy:  0.551156261614058 0.8833333333333333\n",
      "976 번째 loss, accuracy:  0.5510027884946034 0.8833333333333333\n",
      "977 번째 loss, accuracy:  0.5508607305384532 0.8916666666666667\n",
      "978 번째 loss, accuracy:  0.5507335255437785 0.9166666666666666\n",
      "979 번째 loss, accuracy:  0.5506063881980815 0.925\n",
      "980 번째 loss, accuracy:  0.5504579225249935 0.925\n",
      "981 번째 loss, accuracy:  0.5503039350203155 0.9166666666666666\n",
      "982 번째 loss, accuracy:  0.5501537869258066 0.9166666666666666\n",
      "983 번째 loss, accuracy:  0.5499571558332147 0.9\n",
      "984 번째 loss, accuracy:  0.5497848484876241 0.9\n",
      "985 번째 loss, accuracy:  0.5496643445626419 0.925\n",
      "986 번째 loss, accuracy:  0.5494492329338895 0.8916666666666667\n",
      "987 번째 loss, accuracy:  0.5492861172280986 0.8833333333333333\n",
      "988 번째 loss, accuracy:  0.5491194563145432 0.8833333333333333\n",
      "989 번째 loss, accuracy:  0.5489580784030131 0.8833333333333333\n",
      "990 번째 loss, accuracy:  0.5487912018584218 0.875\n",
      "991 번째 loss, accuracy:  0.548662203123501 0.8833333333333333\n",
      "992 번째 loss, accuracy:  0.5484891644798368 0.8833333333333333\n",
      "993 번째 loss, accuracy:  0.5483609946589068 0.8833333333333333\n",
      "994 번째 loss, accuracy:  0.5481865594636448 0.875\n",
      "995 번째 loss, accuracy:  0.5480580628294744 0.8833333333333333\n",
      "996 번째 loss, accuracy:  0.5478857706052279 0.875\n",
      "997 번째 loss, accuracy:  0.5477480558557437 0.875\n",
      "998 번째 loss, accuracy:  0.5475946964600558 0.875\n",
      "999 번째 loss, accuracy:  0.5474173221470323 0.85\n",
      "hidden layer의 Unit 수:  5\n",
      "learningRate:  0.02 , epoch:  1000 , batch_size:  40\n",
      "loss:  0.5472527663229487\n",
      "Train_Data accuracy:  0.8416666666666667\n",
      "Test_Data accuracy:  0.9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\"\"\" iris Data \"\"\"\n",
    "iris = load_iris()\n",
    "X = iris.data # iris data input\n",
    "y = iris.target # iris target (label)\n",
    "\n",
    "# 데이터 Split Use training : testing = 8 : 2 => 120 : 30\n",
    "suffle = np.random.choice(X.shape[0], X.shape[0], replace=False)\n",
    "for_train = suffle[:120]\n",
    "for_test = suffle[120:]\n",
    "\n",
    "# for training data (X, y)\n",
    "X_train = X[for_train]\n",
    "y_train = y[for_train]\n",
    "# for testing data (X, y)\n",
    "X_test = X[for_test]\n",
    "y_test = y[for_test]\n",
    "\n",
    "\"\"\" hidden layer의 Unit 수 = 5 \"\"\"\n",
    "tn2 = TwoLayerNeuralNetwork2(input_size = 4, hidden_size = 5, output_size = 3)\n",
    "tn2.init_data(X_train, y_train)\n",
    "\n",
    "\"\"\" hyperParameter 값 \"\"\"\n",
    "lr = 0.02\n",
    "epoch = 1000\n",
    "batch_size = 40\n",
    "batch = True # 배치 이용\n",
    "check = True # loss와 accuracy의 추이와 Plt 확인\n",
    "\n",
    "lossPlt = []\n",
    "accPlt = []\n",
    "lossPlt, accPlt = tn2.learn(lr, epoch, batch_size, batch, check) # epoch번 반복한 loss와 accuracy의 List를 return \n",
    "    \n",
    "print(\"hidden layer의 Unit 수: \", hidden_size)\n",
    "print(\"learningRate: \", lr, \", epoch: \", epoch, \", batch_size: \", batch_size)\n",
    "# loss, Train_Data accuracy와 Test_Data accuracy 출력\n",
    "print(\"loss: \", tn2.loss(X_train, y_train))\n",
    "print(\"Train_Data accuracy: \", tn2.accuracy(X_train, y_train))\n",
    "print(\"Test_Data accuracy: \", tn2.accuracy(X_test, y_test))\n",
    "\n",
    "# loss와 training accuracy를 Plot\n",
    "x = np.arange(epoch)\n",
    "plt.plot(x, lossPlt, x, accPlt)\n",
    "plt.legend([\"loss\", \"training accuracy\"]) # 각주\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden layer의 Unit 수:  3\n",
      "epoch:  1000  ,learningRate:  0.02  , batch_size:  40\n",
      "0 번째 loss:  1.0918593014913323 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "1 번째 loss:  0.5901418669668661 Train_Data accuracy:  0.8583333333333333 Test_Data accuracy:  0.7666666666666667\n",
      "2 번째 loss:  0.722055604250088 Train_Data accuracy:  0.6916666666666667 Test_Data accuracy:  0.6\n",
      "loss:  0.8013522575694288\n",
      "Train_Data accuracy:  0.6416666666666667\n",
      "Test_Data accuracy:  0.5111111111111111 \n",
      "\n",
      "epoch:  1000  ,learningRate:  0.02  , batch_size:  80\n",
      "0 번째 loss:  0.5197018309014388 Train_Data accuracy:  0.7833333333333333 Test_Data accuracy:  0.7\n",
      "1 번째 loss:  0.47029992925583664 Train_Data accuracy:  0.9333333333333333 Test_Data accuracy:  0.9333333333333333\n",
      "2 번째 loss:  1.091394112008174 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "loss:  0.6937986240551499\n",
      "Train_Data accuracy:  0.6972222222222223\n",
      "Test_Data accuracy:  0.6 \n",
      "\n",
      "epoch:  1000  ,learningRate:  0.02  , batch_size:  120\n",
      "0 번째 loss:  0.47752812202127104 Train_Data accuracy:  0.9 Test_Data accuracy:  0.8\n",
      "1 번째 loss:  0.6167885498820963 Train_Data accuracy:  0.6916666666666667 Test_Data accuracy:  0.5666666666666667\n",
      "2 번째 loss:  0.6115440747998954 Train_Data accuracy:  0.49166666666666664 Test_Data accuracy:  0.23333333333333334\n",
      "loss:  0.5686202489010875\n",
      "Train_Data accuracy:  0.6944444444444445\n",
      "Test_Data accuracy:  0.5333333333333333 \n",
      "\n",
      "epoch:  1000  ,learningRate:  0.01  , batch_size:  40\n",
      "0 번째 loss:  1.0947587308776074 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "1 번째 loss:  0.8861359936495351 Train_Data accuracy:  0.8083333333333333 Test_Data accuracy:  0.6\n",
      "2 번째 loss:  1.0998818307792624 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "loss:  1.0269255184354682\n",
      "Train_Data accuracy:  0.5194444444444445\n",
      "Test_Data accuracy:  0.31111111111111106 \n",
      "\n",
      "epoch:  1000  ,learningRate:  0.01  , batch_size:  80\n",
      "0 번째 loss:  1.0954021816288593 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "1 번째 loss:  1.0954907817021793 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "2 번째 loss:  1.0943400126893024 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "loss:  1.095077658673447\n",
      "Train_Data accuracy:  0.375\n",
      "Test_Data accuracy:  0.16666666666666666 \n",
      "\n",
      "epoch:  1000  ,learningRate:  0.01  , batch_size:  120\n",
      "0 번째 loss:  1.094970203726548 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "1 번째 loss:  0.5589568003759875 Train_Data accuracy:  0.5916666666666667 Test_Data accuracy:  0.43333333333333335\n",
      "2 번째 loss:  0.6379199727596496 Train_Data accuracy:  0.7583333333333333 Test_Data accuracy:  0.7\n",
      "loss:  0.7639489922873951\n",
      "Train_Data accuracy:  0.5750000000000001\n",
      "Test_Data accuracy:  0.4333333333333333 \n",
      "\n",
      "epoch:  1000  ,learningRate:  0.005  , batch_size:  40\n",
      "0 번째 loss:  1.0975788788238972 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "1 번째 loss:  1.0990896818354046 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "2 번째 loss:  0.6215622883052556 Train_Data accuracy:  0.6833333333333333 Test_Data accuracy:  0.6\n",
      "loss:  0.9394102829881859\n",
      "Train_Data accuracy:  0.4777777777777778\n",
      "Test_Data accuracy:  0.3111111111111111 \n",
      "\n",
      "epoch:  1000  ,learningRate:  0.005  , batch_size:  80\n",
      "0 번째 loss:  1.0696399442975892 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "1 번째 loss:  1.0978127401974382 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "2 번째 loss:  1.0238712520726112 Train_Data accuracy:  0.5 Test_Data accuracy:  0.2\n",
      "loss:  1.0637746455225463\n",
      "Train_Data accuracy:  0.4166666666666667\n",
      "Test_Data accuracy:  0.17777777777777778 \n",
      "\n",
      "epoch:  1000  ,learningRate:  0.005  , batch_size:  120\n",
      "0 번째 loss:  0.9831116750470656 Train_Data accuracy:  0.6666666666666666 Test_Data accuracy:  0.5333333333333333\n",
      "1 번째 loss:  1.0994909768362087 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "2 번째 loss:  0.8739490477877062 Train_Data accuracy:  0.675 Test_Data accuracy:  0.5666666666666667\n",
      "loss:  0.9855172332236601\n",
      "Train_Data accuracy:  0.5722222222222222\n",
      "Test_Data accuracy:  0.4222222222222222 \n",
      "\n",
      "epoch:  5000  ,learningRate:  0.02  , batch_size:  40\n",
      "0 번째 loss:  0.1659899997765488 Train_Data accuracy:  0.9666666666666667 Test_Data accuracy:  0.9666666666666667\n",
      "1 번째 loss:  1.0947502744242688 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "2 번째 loss:  0.4655366139164986 Train_Data accuracy:  0.6833333333333333 Test_Data accuracy:  0.6\n",
      "loss:  0.5754256293724387\n",
      "Train_Data accuracy:  0.6750000000000002\n",
      "Test_Data accuracy:  0.5777777777777778 \n",
      "\n",
      "epoch:  5000  ,learningRate:  0.02  , batch_size:  80\n",
      "0 번째 loss:  1.0943815459213753 Train_Data accuracy:  0.375 Test_Data accuracy:  0.16666666666666666\n",
      "1 번째 loss:  0.45251605147124735 Train_Data accuracy:  0.8666666666666667 Test_Data accuracy:  0.8333333333333334\n",
      "2 번째 loss:  0.1341423508333977 Train_Data accuracy:  0.9833333333333333 Test_Data accuracy:  0.9666666666666667\n",
      "loss:  0.5603466494086734\n",
      "Train_Data accuracy:  0.7416666666666667\n",
      "Test_Data accuracy:  0.6555555555555556 \n",
      "\n",
      "epoch:  5000  ,learningRate:  0.02  , batch_size:  120\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-862d85773738>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mlo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                     \u001b[0mtn2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                     \u001b[0mlo1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTe1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtn2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtn2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtn2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"번째 loss: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlo1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Train_Data accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTr1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Test_Data accuracy: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTe1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-f50ae02eb624>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, lr, epoch, batch_size, batch, check)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[1;31m# 훈련 데이터를 통해 기울기를 구한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[1;31m# 기울어진 방향으로 가중치의 값을 조정하고 learningRate를 곱함으로 overshooting을 막는다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-f50ae02eb624>\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# 새 딕셔너리 params에 각 layer의 Weight값과 bias값의 기울기를 저장한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, X)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;31m# print(\"in numerical gradient \", idx, x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumerical_gradient_no_batch_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AI\\gradient.py\u001b[0m in \u001b[0;36mnumerical_gradient_no_batch_\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m# f(x-h) 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mfxh2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfxh1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfxh2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-f50ae02eb624>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;34m\"\"\" 가중치 매개변수의 기울기를 구한다. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# lambda를 이용하여 loss(x, t)의 함수 f를 구한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# 새 딕셔너리 params에 각 layer의 Weight값과 bias값의 기울기를 저장한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-f50ae02eb624>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-f50ae02eb624>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# hidden layer --> output : softmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mz3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\"\"\" iris Data \"\"\"\n",
    "iris = load_iris()\n",
    "X = iris.data # iris data input\n",
    "y = iris.target # iris target (label)\n",
    "\n",
    "# 데이터 Split Use training : testing = 8 : 2 => 120 : 30\n",
    "suffle = np.random.choice(X.shape[0], X.shape[0], replace=False)\n",
    "for_train = suffle[:120]\n",
    "for_test = suffle[120:]\n",
    "\n",
    "# for training data (X, y)\n",
    "X_train = X[for_train]\n",
    "y_train = y[for_train]\n",
    "# for testing data (X, y)\n",
    "X_test = X[for_test]\n",
    "y_test = y[for_test]\n",
    "\n",
    "\"\"\" hidden layer의 Unit 수 = 3, 5, 7\"\"\"\n",
    "hidden_size = [3,5,7]\n",
    "input_size = 4\n",
    "output_size = 3\n",
    "for hid in hidden_size:\n",
    "    print(\"hidden layer의 Unit 수: \", hid)\n",
    "    tn2 = TwoLayerNeuralNetwork2(input_size, hid, output_size)\n",
    "    tn2.init_data(X_train, y_train) # Set Training Data\n",
    "\n",
    "    \"\"\" hyperParameter 값 별로 Test \"\"\"\n",
    "    lr = [0.02, 0.01, 0.005] # 0.01에서 *2, /2\n",
    "    epoch = [1000, 5000, 10000] # epoch 1000, 5000, 10000 각각 Test\n",
    "    batch_size = [40, 80, 120]\n",
    "    batch = True # 배치 이용\n",
    "    check = False # loss와 accuracy의 추이와 Plt 확인 안함\n",
    "\n",
    "    for e in epoch:\n",
    "        for l in lr:\n",
    "            for b in batch_size:\n",
    "                # 각 lr, epoch, batchsize에서의 loss, Train_Data accuracy, Test_Data accuracy 출력\n",
    "                print(\"epoch: \", e, \", learningRate: \", l, \", batch_size: \", b)\n",
    "                lo, Tr, Te = 0.0, 0.0, 0.0\n",
    "                for i in range(3):\n",
    "                    tn2.learn(l, e, b, batch, check)\n",
    "                    lo1, Tr1, Te1 = tn2.loss(X_train, y_train), tn2.accuracy(X_train, y_train), tn2.accuracy(X_test, y_test)\n",
    "                    tn2.reset() # parameter값들 Reset 해주어야 한다.\n",
    "                    print(i+1, \"번째 loss: \", lo1, \", Train_Data accuracy: \", Tr1,\", Test_Data accuracy: \", Te1)\n",
    "                    lo += lo1\n",
    "                    Tr += Tr1\n",
    "                    Te += Te1\n",
    "                print(\"평균 loss: \", lo / 3.0)\n",
    "                print(\"평균 Train_Data accuracy: \", Tr / 3.0)\n",
    "                print(\"평균 Test_Data accuracy: \", Te / 3.0, \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
